{
  "feed": {
    "title": "Artificial Intelligence",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
      "value": "Artificial Intelligence"
    },
    "links": [
      {
        "href": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "rel": "self",
        "type": "application/rss+xml"
      },
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://aws.amazon.com/blogs/machine-learning/"
      }
    ],
    "link": "https://aws.amazon.com/blogs/machine-learning/",
    "subtitle": "Official Machine Learning Blog of Amazon Web Services",
    "subtitle_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
      "value": "Official Machine Learning Blog of Amazon Web Services"
    },
    "updated": "Tue, 26 Aug 2025 18:37:21 +0000",
    "updated_parsed": [
      2025,
      8,
      26,
      18,
      37,
      21,
      1,
      238,
      0
    ],
    "language": "en-US",
    "sy_updateperiod": "hourly",
    "sy_updatefrequency": "1"
  },
  "entries": [
    {
      "title": "Learn how Amazon Health Services improved discovery in Amazon search using AWS ML and gen AI",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Learn how Amazon Health Services improved discovery in Amazon search using AWS ML and gen AI"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/learn-how-amazon-health-services-improved-discovery-in-amazon-search-using-aws-ml-and-gen-ai/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/learn-how-amazon-health-services-improved-discovery-in-amazon-search-using-aws-ml-and-gen-ai/",
      "authors": [
        {
          "name": "Faryab Haye"
        }
      ],
      "author": "Faryab Haye",
      "author_detail": {
        "name": "Faryab Haye"
      },
      "published": "Tue, 26 Aug 2025 18:37:21 +0000",
      "published_parsed": [
        2025,
        8,
        26,
        18,
        37,
        21,
        1,
        238,
        0
      ],
      "tags": [
        {
          "term": "Amazon Athena",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon EMR",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker",
          "scheme": null,
          "label": null
        },
        {
          "term": "AWS Health",
          "scheme": null,
          "label": null
        },
        {
          "term": "Customer Solutions",
          "scheme": null,
          "label": null
        },
        {
          "term": "Healthcare",
          "scheme": null,
          "label": null
        }
      ],
      "id": "e27705756c1e7e55bd0d8013da0fec052cd84872",
      "guidislink": false,
      "summary": "In this post, we show you how Amazon Health Services (AHS) solved discoverability challenges on Amazon.com search using AWS services such as Amazon SageMaker, Amazon Bedrock, and Amazon EMR. By combining machine learning (ML), natural language processing, and vector search capabilities, we improved our ability to connect customers with relevant healthcare offerings.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we show you how Amazon Health Services (AHS) solved discoverability challenges on Amazon.com search using AWS services such as Amazon SageMaker, Amazon Bedrock, and Amazon EMR. By combining machine learning (ML), natural language processing, and vector search capabilities, we improved our ability to connect customers with relevant healthcare offerings."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>Healthcare discovery on ecommerce domains presents unique challenges that traditional product search wasn’t designed to handle. Unlike searching for books or electronics, healthcare queries involve complex relationships between symptoms, conditions, treatments, and services, requiring sophisticated understanding of medical terminology and customer intent.</p> \n<p>This challenge became particularly relevant for Amazon as we expanded beyond traditional ecommerce into comprehensive healthcare services. Amazon now offers direct access to prescription medications through <a href=\"https://pharmacy.amazon.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Pharmacy</a>, primary care through <a href=\"https://health.amazon.com/prime\" rel=\"noopener noreferrer\" target=\"_blank\">One Medical</a>, and specialized care partnerships through <a href=\"https://health.amazon.com/health-condition-programs\" rel=\"noopener noreferrer\" target=\"_blank\">Health Benefits Connector</a>. These healthcare offerings represent a significant departure from traditional Amazon.com products, presenting both exciting opportunities and unique technical challenges.</p> \n<p>In this post, we show you how <a href=\"https://health.amazon.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Health Services</a> (AHS) solved discoverability challenges on Amazon.com search using AWS services such as <a href=\"https://aws.amazon.com/sagemaker/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker</a>, <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a>, and <a href=\"https://aws.amazon.com/emr/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon EMR</a>. By combining <a href=\"https://aws.amazon.com/ai/machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">machine learning</a> (ML), natural language processing, and vector search capabilities, we improved our ability to connect customers with relevant healthcare offerings. This solution is now used daily for health-related search queries, helping customers find everything from prescription medications to primary care services.</p> \n<p>At AHS, we’re on a mission to transform how people access healthcare. We strive to make healthcare more straightforward for customers to find, choose, afford, and engage with the services, products, and professionals they need to get and stay healthy.</p> \n<h2>Challenges</h2> \n<p>Integrating healthcare services into the ecommerce business of Amazon presented two unique opportunities to enhance search for customers on healthcare journeys: understanding health search intent in queries and matching up customer query intent with the most relevant healthcare products and services.</p> \n<p>The challenge in understanding health search intent lies in the relationships between symptoms (such as back pain or sore throat), conditions (such as a herniated disc or the common cold), treatments (such as physical therapy or medication), and the healthcare services Amazon offers. This requires sophisticated query understanding capabilities that can parse medical terminology and map it to common search terminology that a layperson outside of the medical field might use to search.</p> \n<p>AHS offerings also present unique challenges for search matching. For example, a customer searching for “back pain treatment” might be looking for a variety of solutions, from over-the-counter pain relievers like <a href=\"https://www.amazon.com/Tylenol-Arthritis-Extended-Release-Reliever/dp/B0077VYSYY\" rel=\"noopener noreferrer\" target=\"_blank\">Tylenol</a> or prescription medications such as <a href=\"https://pharmacy.amazon.com/Cyclobenzaprine-Generic-Flexeril-Oral-Tablet/dp/B084BWZF8T\" rel=\"noopener noreferrer\" target=\"_blank\">cyclobenzaprine</a> (a muscle relaxant), to scheduling a doctor’s appointment or accessing virtual physical therapy. Existing search algorithms optimized for physical products might not match these service-based health offerings, potentially missing relevant results such as One Medical’s primary care <a href=\"https://health.amazon.com/prime?ie=UTF8&amp;keywords=doctor&amp;ASIN=B0CZYRCB2B\" rel=\"noopener noreferrer\" target=\"_blank\">services</a> or <a href=\"https://health.amazon.com/health-condition-programs/dp/B0DLJ5PQ5K\" rel=\"noopener noreferrer\" target=\"_blank\">Hinge Health’s virtual physical therapy program</a> that helps reduce joint and muscle pain through personalized exercises and 1-on-1 support from dedicated therapists. This unique nature of healthcare offerings called for developing specialized approaches to connect customers with relevant services.</p> \n<h2>Solution overview</h2> \n<p>To address these challenges, we developed a comprehensive solution that combines ML for query understanding, vector search for product matching, and <a href=\"https://aws.amazon.com/what-is/large-language-model/\" rel=\"noopener noreferrer\" target=\"_blank\">large language models</a> (LLMs) for relevance optimization. The solution consists of three main components:</p> \n<ol> \n <li><strong>Query understanding pipeline</strong> – Uses ML models to identify and classify health-related searches, distinguishing between specific medication queries and broader health condition searches</li> \n <li><strong>Product knowledge base</strong> – Combines existing product metadata with LLM-enhanced health information to create comprehensive product embeddings for semantic search</li> \n <li><strong>Relevance optimization</strong> – Implements a hybrid approach using both human labeling and LLM-based classification to produce high-quality matches between searches and healthcare offerings</li> \n</ol> \n<p>The solution is built entirely on AWS services, with Amazon SageMaker powering our ML models, Amazon Bedrock providing LLM capabilities, and Amazon EMR and <a href=\"https://aws.amazon.com/athena/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Athena</a> handling our data processing needs.</p> \n<h2>Solution architecture</h2> \n<p>Now let’s examine the technical implementation details of our architecture, exploring how each component was engineered to address the unique challenges of healthcare search on Amazon.com.</p> \n<h3>Query understanding: Identification of health searches</h3> \n<p>We approached the customer search journey by recognizing its two distinct ends of the spectrum. On one end are what we call “spearfishing queries” or lower funnel searches, where customers have a clear product search intent with specific knowledge about attributes. For Amazon Health Services, these typically include searches for specific prescription medications with precise dosages and form factors, such as “atorvastatin 40 mg” or “lisinopril 20 mg.”</p> \n<p>On the other end are broad, upper funnel queries where customers seek inspiration, information, or recommendations with general product search intent that might encompass multiple product types. Examples include searches like “back pain relief,” “acne,” or “high blood pressure.” Building upon Amazon search capabilities, we developed additional query understanding models to serve the full spectrum of healthcare searches.</p> \n<p>For identifying spearfishing search intent, we analyzed anonymized customer search engagement data for Amazon products and trained a classification model to understand which search keywords exclusively lead to engagement with Amazon Pharmacy Amazon Standard Identification Numbers (ASINs). This process used PySpark on Amazon EMR and Athena to collect and process Amazon search data at scale. The following diagram shows this architecture.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-ap-classifier.png\"><img alt=\"Architecture diagram showing the pharmacy classifier training pipeline. Flow starts with Amazon search logs, which feed into a process using Amazon Athena and EMR to create training examples. Training data is stored in Amazon S3, then processed through AWS SageMaker Studio for model training.\" class=\"alignnone size-full wp-image-115078\" height=\"321\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-ap-classifier.png\" width=\"848\" /></a></p> \n<p>For identifying broad health search intent, we trained a named entity recognition (NER) model to annotate search keywords at a medical terminology level. To build this capability, we used a corpus of health ontology data sources to identify concepts such as health conditions, diseases, treatments, injuries, and medications. For health concepts where we did not have enough alternate terms in our knowledge base, we used LLMs to expand our knowledge base. For example, alternate terms for the condition “acid reflux” might be “heart burn”, “GERD”, “indigestion”, etc. We gated this NER model behind health-relevant product types predicted by <a href=\"https://arxiv.org/abs/2408.02215\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon search query-to-product-type models</a>. The following diagram shows the training process for the NER model.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-ner.png\"><img alt=\"Named Entity Recognition (NER) model training architecture. Shows two input sources: Health Concept Ontology database and Amazon Bedrock API for extracting similar terms. Both feed into a knowledge building process that stores training data in Amazon S3. The pipeline continues to AWS SageMaker Studio for training a base NER model, which is then fine-tuned to create the final NER model.\" class=\"alignnone size-full wp-image-115081\" height=\"321\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-ner.png\" width=\"1071\" /></a></p> \n<p>The following image is an example of a query identification task in practice. In the example on the left, the pharmacy classifier predicts that “atorvastatin 40 mg” is a query with intent for a prescription drug and triggers a custom search experience geared towards AHS products. In the example on the right, we detect the broad “high blood pressure” symptom but don’t know the customer’s intention. So, we trigger an experience that gives them multiple options to make the search more specific.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-cx-example.jpeg\"><img alt=\"Side-by-side mobile app screenshots demonstrating query classification in practice. Left screenshot shows a specific medication search for 'atorvastatin 40 mg' with pharmacy-exclusive classifier results displaying prescription medication options. Right screenshot shows a broad health query for 'high blood pressure' with NER model results offering multiple health-related options including medical care, prescription medication, monitors, and books to help customers refine their search intent.\" class=\"alignnone size-full wp-image-115079\" height=\"1127\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-cx-example.jpeg\" style=\"margin: 10px 0px 10px 0px;\" width=\"745\" /></a></p> \n<p>For those interested in implementing similar medical entity recognition capabilities, <a href=\"https://aws.amazon.com/comprehend/medical/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Comprehend Medical</a> offers powerful tools for detecting medical entities in text spans.</p> \n<h3>Building product knowledge</h3> \n<p>With our ability to identify health-related searches in place, we needed to build comprehensive knowledge bases for our healthcare products and services. We started with our existing offerings and collected all available product knowledge information that best described each product or service.</p> \n<p>To enhance this foundation, we used a <a href=\"https://aws.amazon.com/what-is/large-language-model/\" rel=\"noopener noreferrer\" target=\"_blank\">large language model</a> (LLM) with a fine-tuned prompt and few-shot examples to layer in additional relevant health conditions, symptoms, and treatment-related keywords for each product or service. We did this using the Amazon Bedrock <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html\" rel=\"noopener noreferrer\" target=\"_blank\">batch inference</a> capability. This approach meant that we significantly expanded our product knowledge with medically relevant information.</p> \n<p>The entire knowledge base was then converted into embeddings using <a href=\"https://github.com/facebookresearch/faiss\" rel=\"noopener noreferrer\" target=\"_blank\">Facebook AI Similarity Search</a> (FAISS), and we created an index file to enable efficient similarity searches. We maintained careful mappings from each embedding back to the original knowledge base items, making sure we could perform accurate reverse lookups when needed.</p> \n<p>This process used several AWS services, including <a href=\"https://aws.amazon.com/s3/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3) for storage of the knowledge base and the embeddings files. Note that <a href=\"https://aws.amazon.com/opensearch-service/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon OpenSearch Service</a> is also a viable option for vector database capabilities. Large-scale knowledge base embedding jobs were executed with scheduled SageMaker <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-auto-run.html\" rel=\"noopener noreferrer\" target=\"_blank\">Notebook Jobs</a>. Through the combination of these technologies, we built a robust foundation of healthcare product knowledge that could be efficiently searched and matched to customer queries.</p> \n<p>The following diagram illustrates how we built the product knowledge base using Amazon catalog data, and then used that to prepare a FAISS index file.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-kb-build.png\"><img alt=\"Product knowledge base building architecture diagram. Left side shows Knowledge Base Preparation with Amazon Catalog providing product data to KB Builder, and Amazon Bedrock API augmenting search terms. Combined knowledge base is stored in Amazon S3. Right side shows Index Building where the KB Builder processes the combined knowledge base to create embeddings and a similarity index using FAISS.\" class=\"alignnone size-full wp-image-115080\" height=\"341\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-kb-build.png\" width=\"1071\" /></a></p> \n<h3>Mapping health search intent to the most relevant products and services</h3> \n<p>A core component of our solution was implementing the <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\" rel=\"noopener noreferrer\" target=\"_blank\">Retrieval Augmented Generation</a> (RAG) design pattern. The first step in this pattern was to identify a set of known keywords and Amazon products, establishing the initial ground truth for our solution.</p> \n<p>With our product knowledge base built from Amazon catalog metadata and ASIN attributes, we were ready to support new queries from customers. When a customer search query arrived, we converted it to an embedding and used it as a search key for matching against our index. This similarity search used FAISS with matching criteria based on the threshold against the similarity score.</p> \n<p>To verify the quality of these query-product pairs identified for health search keywords, we needed to maintain the relevance of each pair. To achieve this, we implemented a two-pronged approach to relevance labeling. We used an established scheme to tag each offering as exact, substitute, complement, or irrelevant to the keyword. Referred to as the exact, substitute, complement, irrelevant (ESCI) framework established through academic research. For more information, refer to the <a href=\"https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search\" rel=\"noopener noreferrer\" target=\"_blank\">ESCI challenge</a> and <a href=\"https://github.com/amazon-science/esci-data?tab=readme-ov-file\" rel=\"noopener noreferrer\" target=\"_blank\">esci-data</a> GitHub repository.</p> \n<p>First, we worked with a human labeling team to establish ground truth on a substantial sample size, creating a reliable benchmark for our system’s performance using this scheme. The labeling team was given guidance based on the ESCI framework and tailored towards AHS products and services.</p> \n<p>Second, we implemented LLM-based labeling using Amazon Bedrock and batch jobs. After matches were found in the previous step, we retrieved the top products and used them as prompt context for our generative model. We included few-shot examples of ESCI guidance as part of the prompt. This way, we conducted large-scale inference across the top health searches, connecting them to the most relevant offerings using similarity search. We performed this at scale for the query-product pairs identified as relevant to AHS and stored the outputs in Amazon S3.</p> \n<p>The following diagram shows our query retrieval, re-ranking and ESCI labeling pipeline.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-retrieval-reranking.png\"><img alt=\"Query retrieval and re-ranking pipeline architecture. Left side shows Result retrieval and re-ranking process where input query flows through Search App and Model repos (bi-encoder, cross-encoder) to generate embeddings, reduce to top K results, and re-rank results. Right side shows Validation/QA process with ESCI Batch Processor connecting to Amazon Bedrock batch inference, plus human annotator input, producing ESCI labeled output.\" class=\"alignnone size-full wp-image-115082\" height=\"401\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/ML-18930-retrieval-reranking.png\" width=\"1121\" /></a></p> \n<p>Using a mix of high-confidence human and LLM-based labels, we established a true ground truth. Through this process, we successfully identified relevant product offerings for customers using only semantic data from aggregated search keywords and product metadata.</p> \n<h2>How did this help customers?</h2> \n<p>We’re on a mission to make it more straightforward for people to find, choose, afford, and engage with the services, products, and professionals they need to get and stay healthy. Today, customers searching for health solutions on Amazon—whether for acute conditions like acne, strep throat, and fever or chronic conditions such as arthritis, high blood pressure, and diabetes—will begin to see medically vetted and relevant offerings alongside other relevant products and services available on <a href=\"http://Amazon.com\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon.com</a>.</p> \n<p>Customers can now quickly find and choose to meet with doctors, get their prescription medications, and access other healthcare services through a familiar experience. By extending the powerful ecommerce search capabilities of Amazon to address healthcare-specific opportunities, we’ve created additional discovery pathways for relevant health services.</p> \n<p>We’ve used semantic understanding of health queries and comprehensive product knowledge to create connections that help customers find the right healthcare solutions at the right time.</p> \n<h2>Amazon Health Services Offerings</h2> \n<p>Here is a little more information about three healthcare services you can use directly through Amazon:</p> \n<ul> \n <li><a href=\"https://pharmacy.amazon.com/\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>Amazon Pharmacy (AP)</strong></a> provides a full-service, online pharmacy experience with transparent medication pricing, convenient home delivery at no additional cost, ongoing delivery updates, 24/7 pharmacist support, and insurance plan acceptance, which supports access and medication adherence. Prime members enjoy special savings with Prime Rx, RxPass, and automatic coupons, making medications more affordable.</li> \n <li><a href=\"https://health.amazon.com/onemedical/ppv?ref_=health_subnav_aom_flyout_desktop\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>One Medical Membership and Amazon One Medical Pay Per Visit</strong></a> offer flexible health solutions, from in-office and virtual primary care to condition-based telehealth. Membership offers convenient access to preventive, quality primary care and the option to connect with your care team virtually in the One Medical app. Pay-per-visit is a one-time virtual visit option to find treatment for more than 30 common conditions like acne, pink eye, and sinus infections.</li> \n <li><a href=\"https://health.amazon.com/health-condition-programs\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>Health Benefits Connector</strong></a> matches customers to digital health companies outside of Amazon that are covered by their employer. This program has been expanding over the past year, offering access to specialized care through partners like Hinge Health for musculoskeletal care, Rula and Talkspace for mental health support, and Omada for diabetes treatment.</li> \n</ul> \n<h2>Key takeaways</h2> \n<p>As we reflect on our journey to enhance healthcare discovery on Amazon, several key insights stand out that might be valuable for others working on similar challenges:</p> \n<ul> \n <li><strong>Using domain-specific ontology</strong> – We began by developing a deep understanding of customer health searches, specifically identifying what kinds of conditions, symptoms, and treatments customers were seeking. By using established health ontology datasets, we enriched a NER model to detect these entities in search queries, providing a foundation for better matching.</li> \n <li><strong>Similarity search on product knowledge</strong> – We used existing product knowledge along with LLM-augmented real-world knowledge to build a comprehensive corpus of data that could be mapped to our offerings. Through this approach, we created semantic connections between customer queries and relevant healthcare solutions without relying on individual customer data.</li> \n <li><a href=\"https://aws.amazon.com/generative-ai/\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>Generative AI</strong></a><strong> is more than just chatbots</strong> – Throughout this project, we relied on various AWS services that proved instrumental to our success. Amazon SageMaker provided the infrastructure for our ML models. However, using Amazon Bedrock batch inference was a key differentiator. It provided us with powerful LLMs for knowledge augmentation and relevance labeling, and services such as Amazon S3 and Amazon EMR supported our data storage and processing needs. Scaling this process manually would have required orders of magnitude more financial budget. Consider generative AI applications at scale beyond merely chat assistants.</li> \n</ul> \n<p>By combining these approaches, we’ve created a more intuitive and effective way for customers to discover healthcare offerings on Amazon.</p> \n<h2>Implementation considerations</h2> \n<p>If you’re looking to implement a similar solution for healthcare or search, consider the following:</p> \n<ul> \n <li><strong>Security and compliance</strong>: Make sure your solution adheres to healthcare data privacy regulations like <a href=\"https://aws.amazon.com/compliance/hipaa-compliance/\" rel=\"noopener noreferrer\" target=\"_blank\">Health Insurance Portability and Accountability Act</a> (HIPAA). Our approach doesn’t use individual customer data.</li> \n <li><strong>Cost optimization</strong>: \n  <ul> \n   <li>Use <a href=\"https://aws.amazon.com/ec2/spot/use-case/emr/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon EMR on EC2 Spot Instances</a> for batch processing jobs</li> \n   <li>Implement caching for frequently searched queries</li> \n   <li>Choose appropriate instance types for your workload</li> \n  </ul> </li> \n <li><strong>Scalability</strong>: \n  <ul> \n   <li>Design your vector search infrastructure to handle peak traffic</li> \n   <li>Use auto scaling for your inference endpoints</li> \n   <li>Implement proper monitoring and alerting</li> \n  </ul> </li> \n <li><strong>Maintenance</strong>: \n  <ul> \n   <li>Regularly update your health ontology datasets</li> \n   <li>Monitor model performance and retrain as needed</li> \n   <li>Keep your product knowledge base current</li> \n  </ul> </li> \n</ul> \n<h2>Conclusion</h2> \n<p>In this post, we demonstrated how Amazon Health Services used AWS ML and generative AI services to solve the unique challenges of healthcare discovery on Amazon.com, illustrating how you can build sophisticated domain-specific search experiences using Amazon SageMaker, Amazon Bedrock, and Amazon EMR. We showed how to create a query understanding pipeline to identify health-related searches, build comprehensive product knowledge bases enhanced with LLM capabilities, and implement semantic matching using vector search and the ESCI relevance framework to connect customers with relevant healthcare offerings.</p> \n<p>This scalable, AWS based approach demonstrates how ML and generative AI can transform specialized search experiences, advancing our mission to make healthcare more straightforward for customers to find, choose, afford, and engage with. We encourage you to explore how these AWS services can address similar challenges in your own healthcare or specialized search applications. For more information about implementing healthcare solutions on AWS, visit the <a href=\"https://aws.amazon.com/health/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS for Healthcare &amp; Life Sciences</a> page.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/faryab.png\"><img alt=\"Professional headshot of K. Faryab Haye, Applied Scientist II at Amazon Health Services, wearing a red and gray striped sweater.\" class=\"wp-image-115077 size-thumbnail alignleft\" height=\"117\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/faryab-100x117.png\" width=\"100\" /></a><strong>K. Faryab Haye</strong> is an Applied Scientist II at Amazon Health located in Seattle, WA, where he leads search and query understanding initiatives for healthcare AI. His work spans the complete ML lifecycle from large-scale data processing to deploying production systems that serve millions of customers. Faryab earned his MS in Computer Science with a Machine Learning specialization from the University of Michigan and co-founded the Applied Science Club at Amazon Health. When not building ML systems, he can be found hiking mountains, cycling, skiing, or playing volleyball.</p> \n<p style=\"clear: both;\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/vineeth.png\"><img alt=\"Professional headshot of Vineeth Harikumar, Principal Engineer at Amazon Health Services, wearing a dark gray hoodie.\" class=\"size-thumbnail wp-image-115083 alignleft\" height=\"128\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/22/vineeth-100x128.png\" width=\"100\" /></a><strong>Vineeth Harikumar</strong> is a Principal Engineer at Amazon Health Services working on growth and engagement tech initiatives for Amazon One Medical (primary care and telehealth services), Pharmacy prescription delivery, and Health condition programs. Prior to working in healthcare, he worked on building large-scale backend systems in Amazon’s global inventory, supply chain and fulfillment network, Kindle devices, and Digital commerce businesses (such as Prime Video, Music, and eBooks).</p>"
        }
      ]
    },
    {
      "title": "Enhance Geospatial Analysis and GIS Workflows with Amazon Bedrock Capabilities",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Enhance Geospatial Analysis and GIS Workflows with Amazon Bedrock Capabilities"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/enhance-geospatial-analysis-and-gis-workflows-with-amazon-bedrock-capabilities/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/enhance-geospatial-analysis-and-gis-workflows-with-amazon-bedrock-capabilities/",
      "authors": [
        {
          "name": "Dave Horne"
        }
      ],
      "author": "Dave Horne",
      "author_detail": {
        "name": "Dave Horne"
      },
      "published": "Fri, 22 Aug 2025 17:54:38 +0000",
      "published_parsed": [
        2025,
        8,
        22,
        17,
        54,
        38,
        4,
        234,
        0
      ],
      "tags": [
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Artificial Intelligence",
          "scheme": null,
          "label": null
        },
        {
          "term": "Intermediate (200)",
          "scheme": null,
          "label": null
        }
      ],
      "id": "95fdc04fd38d6606c5a3ffd169c2f1f3ce512211",
      "guidislink": false,
      "summary": "Applying emerging technologies to the geospatial domain offers a unique opportunity to create transformative user experiences and intuitive workstreams for users and organizations to deliver on their missions and responsibilities. In this post, we explore how you can integrate existing systems with Amazon Bedrock to create new workflows to unlock efficiencies insights. This integration can benefit technical, nontechnical, and leadership roles alike.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Applying emerging technologies to the geospatial domain offers a unique opportunity to create transformative user experiences and intuitive workstreams for users and organizations to deliver on their missions and responsibilities. In this post, we explore how you can integrate existing systems with Amazon Bedrock to create new workflows to unlock efficiencies insights. This integration can benefit technical, nontechnical, and leadership roles alike."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>As data becomes more abundant and information systems grow in complexity, stakeholders need solutions that reveal quality insights. Applying emerging technologies to the geospatial domain offers a unique opportunity to create transformative user experiences and intuitive workstreams for users and organizations to deliver on their missions and responsibilities.</p> \n<p>In this post, we explore how you can integrate existing systems with <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a> to create new workflows to unlock efficiencies insights. This integration can benefit technical, nontechnical, and leadership roles alike.</p> \n<h2>Introduction to geospatial data</h2> \n<p>Geospatial data is associated with a position relative to Earth (latitude, longitude, altitude). Numerical and structured geospatial data formats can be categorized as follows:</p> \n<ul> \n <li><strong>Vector data</strong> – Geographical features, such as roads, buildings, or city boundaries, represented as points, lines, or polygons</li> \n <li><strong>Raster data</strong> – Geographical information, such as satellite imagery, temperature, or elevation maps, represented as a grid of cells</li> \n <li><strong>Tabular data</strong> – Location-based data, such as descriptions and metrics (average rainfall, population, ownership), represented in a table of rows and columns</li> \n</ul> \n<p>Geospatial data sources might also contain natural language text elements for unstructured attributes and metadata for categorizing and describing the record in question. Geospatial Information Systems (GIS) provide a way to store, analyze, and display geospatial information. In GIS applications, this information is frequently presented with a map to visualize streets, buildings, and vegetation.</p> \n<h2>LLMs and Amazon Bedrock</h2> \n<p>Large language models (LLMs) are a subset of foundation models (FMs) that can transform input (usually text or image, depending on model modality) into outputs (generally text) through a process called <em>generation</em>. Amazon Bedrock is a comprehensive, secure, and flexible service for building <a href=\"https://aws.amazon.com/generative-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">generative AI</a> applications and agents.</p> \n<p>LLMs work in many generalized tasks involving natural language. Some common LLM use cases include:</p> \n<ul> \n <li><strong>Summarization</strong> – Use a model to summarize text or a document.</li> \n <li><strong>Q&amp;A</strong> – Use a model to answer questions about data or facts from context provided during training or inference using Retrieval Augmented Generation (RAG).</li> \n <li><strong>Reasoning</strong> – Use a model to provide chain of thought reasoning to assist a human with decision-making and hypothesis evaluation.</li> \n <li><strong>Data generation</strong> – Use a model to generate synthetic data for testing simulations or hypothetical scenarios.</li> \n <li><strong>Content generation</strong> – Use a model to draft a report from insights derived from an Amazon Bedrock knowledge base or a user’s prompt.</li> \n <li><strong>AI agent and tool orchestration</strong> – Use a model to plan the invocation of other systems and processes. After other systems are invoked by an agent, the agent’s output can then be used as context for further LLM generation.</li> \n</ul> \n<p>GIS can implement these capabilities to create value and improve user experiences. Benefits can include:</p> \n<ul> \n <li><strong>Live decision-making</strong> – Taking real-time insights to support immediate decision-making, such as emergency response coordination and traffic management</li> \n <li><strong>Research and analysis</strong> – In-depth analysis that humans or systems can identify, such as trend analysis, patterns and relationships, and environmental monitoring</li> \n <li><strong>Planning</strong> – Using research and analysis for informed long-term decision-making, such as infrastructure development, resource allocation, and environmental regulation</li> \n</ul> \n<p>Augmenting GIS and workflows with LLM capabilities leads to simpler analysis and exploration of data, discovery of new insights, and improved decision-making. Amazon Bedrock provides a way to host and invoke models as well as integrate the AI models with surrounding infrastructure, which we elaborate on in this post.</p> \n<h2>Combining GIS and AI through RAG and agentic workflows</h2> \n<p>LLMs are trained with large amounts of generalized information to discover patterns in how language is produced. To improve the performance of LLMs for specific use cases, approaches such as RAG and agentic workflows have been created. Retrieving policies and general knowledge for geospatial use cases can be accomplished with RAG, whereas calculating and analyzing GIS data would require an agentic workflow. In this section, we expand upon both RAG and agentic workflows in the context of geospatial use cases.</p> \n<h3>Retrieval Augmented Generation</h3> \n<p>With RAG, you can dynamically inject contextual information from a knowledge base during model invocation.</p> \n<p>RAG supplements a user-provided prompt with data sourced from a knowledge base (collection of documents). Amazon Bedrock offers managed knowledge bases to data sources, such as <a href=\"http://aws.amazon.com/s3\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3) and SharePoint, so you can provide supplemental information, such as city development plans, intelligence reports, or policies and regulations, when your AI assistant is generating a response for a user.</p> \n<p>Knowledge bases are ideal for unstructured documents with information stored in natural language. When your AI model responds to a user with information sourced from RAG, it can provide references and citations to its source material. The following diagram shows how the systems connect together.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-113887 size-full\" height=\"532\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/08/image-1-10.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"898\" /></p> \n<p>Because geospatial data is often structured and in a GIS, you can connect the GIS to the LLM using tools and agents instead of knowledge bases.</p> \n<h3>Tools and agents (to control a UI and a system)</h3> \n<p>Many LLMs, such as <a href=\"https://aws.amazon.com/bedrock/claude/\" rel=\"noopener noreferrer\" target=\"_blank\">Anthropic’s Claude</a> on Amazon Bedrock, make it possible to provide a description of tools available so your AI model can generate text to invoke external processes. These processes might retrieve live information, such as the current weather in a location or querying a structured data store, or might control external systems, such as starting a workflow or adding layers to a map. Some common geospatial functionality that you might want to integrate with your LLM using tools include:</p> \n<ul> \n <li>Performing mathematical calculations like the distance between coordinates, filtering datasets based on numeric values, or calculating derived fields</li> \n <li>Deriving information from predictive analysis models</li> \n <li>Looking up points of interest in structured data stores</li> \n <li>Searching content and metadata in unstructured data stores</li> \n <li>Retrieving real-time geospatial data, like traffic, directions, or estimated time to reach a destination</li> \n <li>Visualizing distances, points of interest, or paths</li> \n <li>Submitting work outputs such as analytic reports</li> \n <li>Starting workflows, like ordering supplies or adjusting supply chain</li> \n</ul> \n<p>Tools are often implemented in <a href=\"https://aws.amazon.com/lambda/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Lambda</a> functions. Lambda runs code without the complexity and overhead of running servers. It handles the infrastructure management, enabling faster development, improved performance, enhanced security, and cost-efficiency.</p> \n<p>Amazon Bedrock offers the feature <a href=\"https://aws.amazon.com/bedrock/agents/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Agents</a> to simplify the orchestration and integration with your geospatial tools. Amazon Bedrock agents follow instructions for LLM reasoning to break down a user prompt into smaller tasks and perform actions against identified tasks from action providers. The following diagram illustrates how Amazon Bedrock Agents works.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-113876 size-full\" height=\"912\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/08/image-2-5.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1622\" /></p> \n<p>The following diagram shows how Amazon Bedrock Agents can enhance GIS solutions.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-113877 size-full\" height=\"751\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/08/image-3-7.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1420\" /></p> \n<h2>Solution overview</h2> \n<p>The following demonstration applies the concepts we’ve discussed to an earthquake analysis agent as an example. This example deploys an Amazon Bedrock agent with a knowledge base based on <a href=\"http://aws.amazon.com/redshift\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Redshift</a>. The Redshift instance has two tables. One table is for earthquakes, which includes date, magnitude, latitude, and longitude. The second table holds the counites in California, described as polygon shapes. The geospatial capabilities of Amazon Redshift can relate these datasets to answer queries like which county had the most recent earthquake or which county has had the most earthquakes in the last 20 years. The Amazon Bedrock agent can generate these geospatially based queries based on natural language.</p> \n<p>This script creates an end-to-end pipeline that performs the following steps:</p> \n<ol> \n <li>Processes geospatial data.</li> \n <li>Sets up cloud infrastructure.</li> \n <li>Loads and configures the spatial database.</li> \n <li>Creates an AI agent for spatial analysis.</li> \n</ol> \n<p>In the following sections, we create this agent and test it out.</p> \n<h2>Prerequisites</h2> \n<p>To implement this approach, you must have an AWS account with the appropriate <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) permissions for Amazon Bedrock, Amazon Redshift, and Amazon S3.</p> \n<p>Additionally, complete the following steps to set up the <a href=\"http://aws.amazon.com/cli\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Command Line Interface</a> (AWS CLI):</p> \n<ol> \n <li>Confirm you have access to the latest version of the AWS CLI.</li> \n <li><a href=\"https://docs.aws.amazon.com/signin/latest/userguide/command-line-sign-in.html\" rel=\"noopener noreferrer\" target=\"_blank\">Sign in</a> to the AWS CLI with your credentials.</li> \n <li>Make sure ./jq is installed. If not, use the following command:</li> \n</ol> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">yum -y install jq</code></pre> \n</div> \n<h2>Set up error handling</h2> \n<p>Use the following code for the initial setup and error handling:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-shell\">#!/usr/bin/env bash\nset -ex\n\nLOG_FILE=\"deployment_$(date +%Y%m%d_%H%M%S).log\"\ntouch \"$LOG_FILE\"\n\nhandle_error() {\n&nbsp;&nbsp; &nbsp;local exit_code=$?\n&nbsp;&nbsp; &nbsp;local line_number=$1\n&nbsp;&nbsp; &nbsp;if [ $exit_code -ne 0 ]; then\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;log_error \"Failed at line $line_number with exit code $exit_code\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;exit $exit_code\n&nbsp;&nbsp; &nbsp;fi\n}\ntrap 'handle_error $LINENO' ERR</code></pre> \n</div> \n<p>This code performs the following functions:</p> \n<ul> \n <li>Creates a timestamped log file</li> \n <li>Sets up error trapping that captures line numbers</li> \n <li>Enables automatic script termination on errors</li> \n <li>Implements detailed logging of failures</li> \n</ul> \n<h2>Validate the AWS environment</h2> \n<p>Use the following code to validate the AWS environment:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-powershell\">AWS_VERSION=$(aws --version 2&gt;&amp;1)\nlog \"INFO\" \"AWS CLI version: $AWS_VERSION\"\n\nif ! aws sts get-caller-identity &amp;&gt;/dev/null; then\n&nbsp;&nbsp; &nbsp;log_error \"AWS CLI is not configured with valid credentials\"\n&nbsp;&nbsp; &nbsp;exit 1\nfi\n\nAWS_REGION=\"us-east-1\"\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)</code></pre> \n</div> \n<p>This code performs the essential AWS setup verification:</p> \n<ul> \n <li>Checks AWS CLI installation</li> \n <li>Validates AWS credentials</li> \n <li>Retrieves account ID for resource naming</li> \n</ul> \n<h2>Set up Amazon Redshift and Amazon Bedrock variables</h2> \n<p>Use the following code to create Amazon Redshift and Amazon Bedrock variables:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">REDSHIFT_CLUSTER_IDENTIFIER=\"geo-analysis-cluster\"\nREDSHIFT_DATABASE=\"geo_db\"\nREDSHIFT_MASTER_USER= [Create username]\nREDSHIFT_MASTER_PASSWORD= [Create Password]\nREDSHIFT_NODE_TYPE=\"dc2.large\"\nREDSHIFT_CLUSTER_TYPE=\"single-node\"\nBEDROCK_ROLE_NAME=\"BedrockGeospatialRole\"\n# Bedrock Configuration\nAGENT_NAME=\"GeoAgentRedshift\"\nKNOWLEDGE_BASE_NAME=\"GeospatialKB\"</code></pre> \n</div> \n<h2>Create IAM roles for Amazon Redshift and Amazon S3</h2> \n<p>Use the following code to set up IAM roles for Amazon S3 and Amazon Redshift:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">if aws iam get-role --role-name \"$REDSHIFT_ROLE_NAME\" &amp;&gt;/dev/null; then\n    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name \"$REDSHIFT_ROLE_NAME\" --query 'Role.Arn' --output text)\n    log \"INFO\" \"Using existing role ARN: $REDSHIFT_ROLE_ARN\"\nelse\n    # Create trust policy document\n    cat &gt; /tmp/trust-policy.json &lt;&lt; EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"redshift.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nEOF\n    # Create role\n    CREATE_ROLE_OUTPUT=$(aws iam create-role \\\n        --role-name \"$REDSHIFT_ROLE_NAME\" \\\n        --assume-role-policy-document \"file:///tmp/trust-policy.json\" \\\n        --description \"Role for Redshift to access S3\" 2&gt;&amp;1)\n    \n    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name \"$REDSHIFT_ROLE_NAME\" --query 'Role.Arn' --output text)\n    if [ $? -ne 0 ]; then\n        log_error \"Failed to create role:\"\n        exit 1\n    fi\n    REDSHIFT_ROLE_ARN=$(echo \"$CREATE_ROLE_OUTPUT\" | jq -r '.Role.Arn')\n    # Wait for role to be available\n    sleep 10\nfi\nATTACH_POLICY_OUTPUT=$(aws iam attach-role-policy \\\n    --role-name \"$REDSHIFT_ROLE_NAME\" \\\n    --policy-arn \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" 2&gt;&amp;1)\nif [ $? -ne 0 ]; then\n    if echo \"$ATTACH_POLICY_OUTPUT\" | grep -q \"EntityAlreadyExists\"; then\n    else\n        exit 1\n    fi\nfi</code></pre> \n</div> \n<h2>Prepare the data and Amazon S3</h2> \n<p>Use the following code to prepare the data and Amazon S3 storage:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">DATA_BUCKET=\"geospatial-bedrock-demo-data-${AWS_ACCOUNT_ID}\"\naws s3 mb s3://$DATA_BUCKET\n\n# Download source data\ncurl -o earthquakes.csv https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/earthquake-data/earthquakes.csv\ncurl -o california-counties.json https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/counties-data/california-counties.json</code></pre> \n</div> \n<p>This code sets up data storage and retrieval through the following steps:</p> \n<ul> \n <li>Creates a unique S3 bucket</li> \n <li>Downloads earthquake and county boundary data</li> \n <li>Prepares for data transformation</li> \n</ul> \n<h2>Transform geospatial data</h2> \n<p>Use the following code to transform the geospatial data:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">INPUT_FILE=\"california-counties.json\"\nOUTPUT_FILE=\"california-counties.csv\"\n\n# Create CSV header\necho \"OBJECTID,AREA,PERIMETER,CO06_D00_,CO06_D00_I,STATE,COUNTY,NAME,LSAD,LSAD_TRANS,Shape_Length,Shape_Area,WKT\" &gt; \"$OUTPUT_FILE\"\n\n# Function to convert ESRI rings to WKT POLYGON format\nesri_to_wkt() {\n    local rings=$1\n    \n    # Extract the first ring (exterior ring)\n    local exterior_ring=$(echo \"$rings\" | jq -c '.[0]')\n    \n    if [ \"$exterior_ring\" = \"null\" ] || [ -z \"$exterior_ring\" ]; then\n        echo \"POLYGON EMPTY\"\n        return\n    fi\n    \n    # Start building the WKT string\n    local wkt=\"POLYGON ((\"\n    \n    # Process each coordinate pair in the ring\n    local coords=$(echo \"$exterior_ring\" | jq -r '.[] | \"\\(.[0]) \\(.[1])\"')\n    local first_coord=\"\"\n    local result=\"\"\n    \n    while IFS= read -r coord; do\n        if [ -z \"$result\" ]; then\n            result=\"$coord\"\n            first_coord=\"$coord\"\n        else\n            result=\"$result, $coord\"\n        fi\n    done &lt;&lt;&lt; \"$coords\"\n    \n    # Close the ring by adding the first coordinate again if needed\n    if [ \"$first_coord\" != \"$(echo \"$coords\" | tail -1)\" ]; then\n        result=\"$result, $first_coord\"\n    fi\n    \n    wkt=\"${wkt}${result}))\"\n    echo \"$wkt\"\n}\n\n# Process each feature in the JSON file\njq -c '.features[]' \"$INPUT_FILE\" | while read -r feature; do\n    # Extract attributes\n    OBJECTID=$(echo \"$feature\" | jq -r '.attributes.OBJECTID // empty')\n    AREA=$(echo \"$feature\" | jq -r '.attributes.AREA // empty')\n    PERIMETER=$(echo \"$feature\" | jq -r '.attributes.PERIMETER // empty')\n    CO06_D00_=$(echo \"$feature\" | jq -r '.attributes.CO06_D00_ // empty')\n    CO06_D00_I=$(echo \"$feature\" | jq -r '.attributes.CO06_D00_I // empty')\n    STATE=$(echo \"$feature\" | jq -r '.attributes.STATE // empty')\n    COUNTY=$(echo \"$feature\" | jq -r '.attributes.COUNTY // empty')\n    NAME=$(echo \"$feature\" | jq -r '.attributes.NAME // empty')\n    LSAD=$(echo \"$feature\" | jq -r '.attributes.LSAD // empty')\n    LSAD_TRANS=$(echo \"$feature\" | jq -r '.attributes.LSAD_TRANS // empty')\n    Shape_Length=$(echo \"$feature\" | jq -r '.attributes.Shape_Length // empty')\n    Shape_Area=$(echo \"$feature\" | jq -r '.attributes.Shape_Area // empty')\n    \n    # Extract geometry and convert to WKT\n    if echo \"$feature\" | jq -e '.geometry.rings' &gt; /dev/null 2&gt;&amp;1; then\n        rings=$(echo \"$feature\" | jq -c '.geometry.rings')\n        WKT=$(esri_to_wkt \"$rings\")\n    else\n        WKT=\"POLYGON EMPTY\"\n    fi\n    \n    # Escape any commas in the fields\n    NAME=$(echo \"$NAME\" | sed 's/,/\\\\,/g')\n    LSAD=$(echo \"$LSAD\" | sed 's/,/\\\\,/g')\n    LSAD_TRANS=$(echo \"$LSAD_TRANS\" | sed 's/,/\\\\,/g')\n    \n     # Write to CSV - wrap WKT field in quotes\n    echo \"$OBJECTID,$AREA,$PERIMETER,$CO06_D00_,$CO06_D00_I,$STATE,$COUNTY,$NAME,$LSAD,$LSAD_TRANS,$Shape_Length,$Shape_Area,\\\"$WKT\\\"\" &gt;&gt; \"$OUTPUT_FILE\"\ndone\n\necho \"Conversion complete. Output saved to $OUTPUT_FILE\"\n\n# Upload data files to S3\naws s3 cp earthquakes.csv s3://$DATA_BUCKET/earthquakes/\naws s3 cp california-counties.csv s3://$DATA_BUCKET/counties/</code></pre> \n</div> \n<p>This code performs the following actions to convert the geospatial data formats:</p> \n<ul> \n <li>Transforms ESRI JSON to WKT format</li> \n <li>Processes county boundaries into CSV format</li> \n <li>Preserves spatial information for Amazon Redshift</li> \n</ul> \n<h2>Create a Redshift cluster</h2> \n<p>Use the following code to set up the Redshift cluster:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-typescript\"># Create Redshift cluster\naws redshift create-cluster \\\n&nbsp;&nbsp; &nbsp;--cluster-identifier \"$REDSHIFT_CLUSTER_IDENTIFIER\" \\\n&nbsp;&nbsp; &nbsp;--node-type \"$REDSHIFT_NODE_TYPE\" \\\n&nbsp;&nbsp; &nbsp;--cluster-type single-node \\\n&nbsp;&nbsp; &nbsp;--master-username \"$REDSHIFT_MASTER_USER\" \\\n&nbsp;&nbsp; &nbsp;--master-user-password \"$REDSHIFT_MASTER_PASSWORD\" \\\n&nbsp;&nbsp; &nbsp;--db-name \"$REDSHIFT_DATABASE\" \\\n&nbsp;&nbsp; &nbsp;--cluster-subnet-group-name \"$SUBNET_GROUP_NAME\" \\\n&nbsp;&nbsp; &nbsp;--vpc-security-group-ids \"$SG_ID\" \\\n&nbsp;&nbsp; &nbsp;--iam-roles \"$REDSHIFT_ROLE_ARN\"\n\n# Wait for cluster availability\nwhile true; do\n&nbsp;&nbsp; &nbsp;CLUSTER_STATUS=$(aws redshift describe-clusters \\\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--cluster-identifier \"$REDSHIFT_CLUSTER_IDENTIFIER\" \\\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--query 'Clusters[0].ClusterStatus' \\\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--output text)\n&nbsp;&nbsp; &nbsp;if [ \"$CLUSTER_STATUS\" = \"available\" ]; then\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;break\n&nbsp;&nbsp; &nbsp;fi\n&nbsp;&nbsp; &nbsp;sleep 30\ndone</code></pre> \n</div> \n<p>This code performs the following functions:</p> \n<ul> \n <li>Sets up a single-node cluster</li> \n <li>Configures networking and security</li> \n <li>Waits for cluster availability</li> \n</ul> \n<h2>Create a database schema</h2> \n<p>Use the following code to create the database schema:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-sql\">aws redshift-data execute-statement \\\n&nbsp;&nbsp; &nbsp;--cluster-identifier \"$REDSHIFT_CLUSTER_IDENTIFIER\" \\\n&nbsp;&nbsp; &nbsp;--database \"$REDSHIFT_DATABASE\" \\\n&nbsp;&nbsp; &nbsp;--sql \"\nCREATE TABLE IF NOT EXISTS counties (\n&nbsp;&nbsp; &nbsp;OBJECTID INTEGER PRIMARY KEY,\n&nbsp;&nbsp; &nbsp;AREA DOUBLE PRECISION,\n&nbsp;&nbsp; &nbsp;NAME VARCHAR(100),\n&nbsp;&nbsp; &nbsp;geom GEOMETRY\n);\n\nCREATE TABLE IF NOT EXISTS earthquakes (\n&nbsp;&nbsp; &nbsp;earthquake_date VARCHAR(50),\n&nbsp;&nbsp; &nbsp;latitude double precision,\n&nbsp;&nbsp; &nbsp;longitude double precision,\n&nbsp;&nbsp; &nbsp;magnitude double precision\n);\"</code></pre> \n</div> \n<p>This code performs the following functions:</p> \n<ul> \n <li>Creates a counties table with spatial data</li> \n <li>Creates an earthquakes table</li> \n <li>Configures appropriate data types</li> \n</ul> \n<h2>Create an Amazon Bedrock knowledge base</h2> \n<p>Use the following code to create a knowledge base:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\"># Create knowledge base\naws bedrock-agent create-knowledge-base \\\n&nbsp;&nbsp; &nbsp;--name \"$KNOWLEDGE_BASE_NAME\" \\\n&nbsp;&nbsp; &nbsp;--knowledge-base-configuration \"{\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\\\"type\\\": \\\"SQL\\\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\\\"sqlKnowledgeBaseConfiguration\\\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\\\"type\\\": \\\"REDSHIFT\\\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp;}\" \\\n&nbsp;&nbsp; &nbsp;--region \"$AWS_REGION\"\n\n# Create data source\naws bedrock-agent create-data-source \\\n&nbsp;&nbsp; &nbsp;--knowledge-base-id \"$KB_ID\" \\\n&nbsp;&nbsp; &nbsp;--name \"EarthquakeDataSource\" \\\n&nbsp;&nbsp; &nbsp;--data-source-configuration \"{\\\"type\\\": \\\"REDSHIFT_METADATA\\\"}\"</code></pre> \n</div> \n<p>This code performs the following functions:</p> \n<ul> \n <li>Creates an Amazon Bedrock knowledge base</li> \n <li>Sets up an Amazon Redshift data source</li> \n <li>Enables spatial queries</li> \n</ul> \n<h2>Create an Amazon Bedrock agent</h2> \n<p>Use the following code to create and configure an agent:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\"># Create agent\naws bedrock-agent create-agent \\\n&nbsp;&nbsp; &nbsp;--agent-name \"$AGENT_NAME\" \\\n&nbsp;&nbsp; &nbsp;--instruction \"You are a geospatial analysis assistant...\" \\\n&nbsp;&nbsp; &nbsp;--foundation-model \"anthropic.claude-3-sonnet-20240229-v1:0\"\n\n# Associate knowledge base\naws bedrock-agent associate-agent-knowledge-base \\\n&nbsp;&nbsp; &nbsp;--agent-id \"$AGENT_ID\" \\\n&nbsp;&nbsp; &nbsp;--knowledge-base-id \"$KB_ID\" \\\n&nbsp;&nbsp; &nbsp;--description \"Earthquake data knowledge base\" \\\n&nbsp;&nbsp; &nbsp;--agent-version \"DRAFT\"</code></pre> \n</div> \n<p>This code performs the following functions:</p> \n<ul> \n <li>Creates an Amazon Bedrock agent</li> \n <li>Associates the agent with the knowledge base</li> \n <li>Configures the AI model and instructions</li> \n</ul> \n<h2>Test the solution</h2> \n<p>Let’s observe the system behavior with the following natural language user inputs in the chat window.</p> \n<h3>Example 1: Summarization and Q&amp;A</h3> \n<p>For this example, we use the prompt “Summarize which zones allow for building of an apartment.”</p> \n<p>The LLM performs retrieval with a RAG approach, then uses the retrieved residential code documents as context to answer the user’s query in natural language.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-113878 size-full\" height=\"754\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/08/image-4-8.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1430\" /></p> \n<p>This example demonstrates the LLM capabilities for hallucination mitigation, RAG, and summarization.</p> \n<h3>Example 2: Generate a draft report</h3> \n<p>Next, we input the prompt “Write me a report on how various zones and related housing data can be utilized to plan new housing development to meet high demand.”</p> \n<p>The LLM retrieves relevant urban planning code documents, then summarizes the information into a standard reporting format as described in its system prompt.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-113879 size-full\" height=\"746\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/08/image-5-3.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1429\" /></p> \n<p>This example demonstrates the LLM capabilities for prompt templates, RAG, and summarization.</p> \n<h3>Example 3: Show places on the map</h3> \n<p>For this example, we use the prompt “Show me the low density properties on Abbeville street in Macgregor on the map with their address.”</p> \n<p>The LLM creates a chain of thought to look up which properties match the user’s query and then invokes the draw marker tool on the map. The LLM provides tool invocation parameters in its scratchpad, awaits the completion of these tool invocations, then responds in natural language with a bulleted list of markers placed on the map.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-113880 size-full\" height=\"705\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/08/image-6-5.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1432\" /></p> \n<p><img alt=\"\" class=\"alignnone wp-image-113881 size-full\" height=\"712\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/08/image-7-5.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1430\" /></p> \n<p>This example demonstrates the LLM capabilities for chain of thought reasoning, tool use, retrieval systems using agents, and UI control.</p> \n<h3>Example 4: Use the UI as context</h3> \n<p>For this example, we choose a marker on a map and input the prompt “Can I build an apartment here.”</p> \n<p>The “here” is not contextualized from conversation history but rather from the state of the map view. Having a state engine that can relay information from a frontend view to the LLM input adds a richer context.</p> \n<p>The LLM understands the context of “here” based on the selected marker, performs retrieval to see the land development policy, and responds to the user in simple natural language, “No, and here is why…”</p> \n<p><img alt=\"\" class=\"alignnone wp-image-113880 size-full\" height=\"705\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/08/image-6-5.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1432\" /></p> \n<p>This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, RAG, and tool use.</p> \n<h3>Example 5: UI context and UI control</h3> \n<p>Next, we choose a marker on the map and input the prompt “draw a .25 mile circle around here so I can visualize walking distance.”</p> \n<p>The LLM invokes the draw circle tool to create a layer on the map centered at the selected marker, contextualized by “here.”</p> \n<p><img alt=\"\" class=\"alignnone wp-image-113883 size-full\" height=\"832\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/08/image-9-4.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1430\" /></p> \n<p>This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, tool use, and UI control.</p> \n<h2>Clean up</h2> \n<p>To clean up your resources and prevent AWS charges from being incurred, complete the following steps:</p> \n<ol> \n <li>Delete the Amazon Bedrock knowledge base.</li> \n <li>Delete the Redshift cluster.</li> \n <li>Delete the S3 bucket.</li> \n</ol> \n<h2>Conclusion</h2> \n<p>The integration of LLMs with GIS creates intuitive systems that help users of different technical levels perform complex spatial analysis through natural language interactions. By using RAG and agent-based workflows, organizations can maintain data accuracy while seamlessly connecting AI models to their existing knowledge bases and structured data systems. Amazon Bedrock facilitates this convergence of AI and GIS technology by providing a robust platform for model invocation, knowledge retrieval, and system control, ultimately transforming how users visualize, analyze, and interact with geographical data.</p> \n<p>For further exploration, <a href=\"https://aws.amazon.com/earth/\" rel=\"noopener noreferrer\" target=\"_blank\">Earth on AWS</a> has videos and articles you can explore to understand how AWS is helping build GIS applications on the cloud.</p> \n<hr /> \n<h3>About the Authors</h3> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-114005 size-full alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/11/dave-horne.jpeg\" width=\"100\" /><strong>Dave Horne</strong>&nbsp;is a Sr. Solutions Architect supporting Federal System Integrators at AWS. He is based in Washington, DC, and has 15 years of experience building, modernizing, and integrating systems for public sector customers. Outside of work, Dave enjoys playing with his kids, hiking, and watching Penn State football!</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-114006 size-full alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/11/Kai-Jai.jpeg\" width=\"100\" /><strong>Kai-Jia Yue</strong>&nbsp;is a solutions architect on the Worldwide Public Sector Global Systems Integrator Architecture team at Amazon Web Services (AWS). She has a focus in data analytics and helping customer organizations make data-driven decisions. Outside of work, she loves spending time with friends and family and traveling.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-114161 alignleft\" height=\"135\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/IMG_5102.jpg\" width=\"100\" />Brian Smitches</strong> is the Head of Partner Deployed Engineering at Windsurf focusing on how partners can bring organizational value through the adoption of Agentic AI software development tools like Windsurf and Devin. Brian has a background in Cloud Solutions Architecture from his time at AWS, where he worked in the&nbsp;AWS Federal Partner ecosystem. In his personal time, Brian enjoys skiing, water sports, and traveling with friends and family.</p>"
        }
      ]
    },
    {
      "title": "Beyond the basics: A comprehensive foundation model selection framework for generative AI",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Beyond the basics: A comprehensive foundation model selection framework for generative AI"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/beyond-the-basics-a-comprehensive-foundation-model-selection-framework-for-generative-ai/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/beyond-the-basics-a-comprehensive-foundation-model-selection-framework-for-generative-ai/",
      "authors": [
        {
          "name": "Sandeep Singh"
        }
      ],
      "author": "Sandeep Singh",
      "author_detail": {
        "name": "Sandeep Singh"
      },
      "published": "Fri, 22 Aug 2025 17:31:28 +0000",
      "published_parsed": [
        2025,
        8,
        22,
        17,
        31,
        28,
        4,
        234,
        0
      ],
      "tags": [
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Artificial Intelligence",
          "scheme": null,
          "label": null
        },
        {
          "term": "Best Practices",
          "scheme": null,
          "label": null
        },
        {
          "term": "Foundation models",
          "scheme": null,
          "label": null
        },
        {
          "term": "Generative AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "Thought Leadership",
          "scheme": null,
          "label": null
        }
      ],
      "id": "306dc26fa76dd6c83e1333f4371ade5e7b8a2a82",
      "guidislink": false,
      "summary": "As the model landscape expands, organizations face complex scenarios when selecting the right foundation model for their applications. In this blog post we present a systematic evaluation methodology for Amazon Bedrock users, combining theoretical frameworks with practical implementation strategies that empower data scientists and machine learning (ML) engineers to make optimal model selections.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "As the model landscape expands, organizations face complex scenarios when selecting the right foundation model for their applications. In this blog post we present a systematic evaluation methodology for Amazon Bedrock users, combining theoretical frameworks with practical implementation strategies that empower data scientists and machine learning (ML) engineers to make optimal model selections."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>Most organizations evaluating foundation models limit their analysis to three primary dimensions: accuracy, latency, and cost. While these metrics provide a useful starting point, they represent an oversimplification of the complex interplay of factors that determine real-world model performance.</p> \n<p>Foundation models have revolutionized how enterprises develop generative AI applications, offering unprecedented capabilities in understanding and generating human-like content. However, as the model landscape expands, organizations face complex scenarios when selecting the right foundation model for their applications. In this blog post we present a systematic evaluation methodology for <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a> users, combining theoretical frameworks with practical implementation strategies that empower data scientists and machine learning (ML) engineers to make optimal model selections.</p> \n<h2>The challenge of foundation model selection</h2> \n<p><a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a> is a fully managed service that offers a choice of high-performing foundation models from leading AI companies such as&nbsp;<a href=\"https://aws.amazon.com/bedrock/ai21/\" rel=\"noopener noreferrer\" target=\"_blank\">AI21 Labs</a>,&nbsp;<a href=\"https://aws.amazon.com/bedrock/anthropic\" rel=\"noopener noreferrer\" target=\"_blank\">Anthropic</a>,&nbsp;<a href=\"https://aws.amazon.com/bedrock/cohere/\" rel=\"noopener noreferrer\" target=\"_blank\">Cohere</a>,&nbsp;<a href=\"https://aws.amazon.com/bedrock/deepseek\" rel=\"noopener noreferrer\" target=\"_blank\">DeepSeek</a>,&nbsp;<a href=\"https://aws.amazon.com/bedrock/luma-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Luma</a>,&nbsp;<a href=\"https://aws.amazon.com/bedrock/llama/\" rel=\"noopener noreferrer\" target=\"_blank\">Meta</a>,&nbsp;<a href=\"https://aws.amazon.com/bedrock/mistral/\" rel=\"noopener noreferrer\" target=\"_blank\">Mistral AI</a>,&nbsp;<a href=\"https://aws.amazon.com/bedrock/poolside/\" rel=\"noopener noreferrer\" target=\"_blank\">poolside&nbsp;</a>(coming soon),&nbsp;<a href=\"https://aws.amazon.com/bedrock/stability-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Stability AI</a>,&nbsp;<a href=\"https://aws.amazon.com/bedrock/twelvelabs/\" rel=\"noopener noreferrer\" target=\"_blank\">TwelveLabs</a>&nbsp;(coming soon),&nbsp;<a href=\"https://aws.amazon.com/bedrock/writer\" rel=\"noopener noreferrer\" target=\"_blank\">Writer</a>, and&nbsp;<a href=\"https://aws.amazon.com/ai/generative-ai/nova/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon&nbsp;</a>through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. The service’s API-driven approach allows seamless model interchangeability, but this flexibility introduces a critical challenge: which model will deliver optimal performance for a specific application while meeting operational constraints?</p> \n<p>Our research with enterprise customers reveals that many early generative AI projects select models based on either limited manual testing or reputation, rather than systematic evaluation against business requirements. This approach frequently results in:</p> \n<ul> \n <li>Over-provisioning computational resources to accommodate larger models than required</li> \n <li>Sub-optimal performance because of misalignment between model strengths and use case requirements</li> \n <li>Unnecessarily high operational costs because of inefficient token utilization</li> \n <li>Production performance issues discovered too late in the development lifecycle</li> \n</ul> \n<p>In this post, we outline a comprehensive evaluation methodology optimized for Amazon Bedrock implementations using <a href=\"https://aws.amazon.com/bedrock/evaluations/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Evaluations</a> while providing forward-compatible patterns as the foundation model landscape evolves. To read more about on how to evaluate large language model (LLM) performance, see <a href=\"https://aws.amazon.com/blogs/machine-learning/llm-as-a-judge-on-amazon-bedrock-model-evaluation/\" rel=\"noopener noreferrer\" target=\"_blank\">LLM-as-a-judge on Amazon Bedrock Model Evaluation.</a></p> \n<h2>A multidimensional evaluation framework—Foundation model capability matrix</h2> \n<p>Foundation models vary significantly across multiple dimensions, with performance characteristics that interact in complex ways. Our capability matrix provides a structured view of critical dimensions to consider when <a href=\"https://aws.amazon.com/blogs/aws/amazon-bedrock-model-evaluation-is-now-generally-available/\" rel=\"noopener noreferrer\" target=\"_blank\">evaluating models</a> in Amazon Bedrock. Below are four core dimensions (in no specific order) – Task performance, Architectural characteristics, Operational considerations, and Responsible AI attributes.</p> \n<h3><strong>Task performance</strong></h3> \n<p>Evaluating the models based on the task performance is crucial for achieving direct impact on business outcomes, ROI, user adoption and trust, and competitive advantage.</p> \n<ul> \n <li><strong>Task-specific accuracy</strong>: Evaluate models using benchmarks relevant to your use case (MMLU, HELM, or domain-specific benchmarks).</li> \n <li><strong>Few-shot learning capabilities</strong>: Strong few-shot performers require minimal examples to adapt to new tasks, leading to cost efficiency, faster time-to-market, resource optimization, and operational benefits.</li> \n <li><strong>Instruction following fidelity</strong>: For the applications that require precise adherence to commands and constraints, it is critical to evaluate model’s instruction following fidelity.</li> \n <li><strong>Output consistency</strong>: Reliability and reproducibility across multiple runs with identical prompts.</li> \n <li><strong>Domain-specific knowledge</strong>: Model performance varies dramatically across specialized fields based on training data. Evaluate the models base on your domain-specific use-case scenarios.</li> \n <li><strong>Reasoning capabilities: </strong>Evaluate the model’s ability to perform logical inference, causal reasoning, and multi-step problem-solving. This can include reasoning such as deductive and inductive, mathematical, chain-of-thought, and so on.</li> \n</ul> \n<h3><strong>Architectural characteristics</strong></h3> \n<p>Architectural characteristics for evaluating the models are important as they directly impact the model’s performance, efficiency, and suitability for specific tasks.</p> \n<ul> \n <li><strong>Parameter count (model size)</strong>: Larger models typically offer more capabilities but require greater computational resources and may have higher inference costs and latency.</li> \n <li><strong>Training data composition</strong>: Models trained on diverse, high-quality datasets tend to have better generalization abilities across different domains.</li> \n <li><strong>Model architecture</strong>: Decoder-only models excel at text generation, encoder-decoder architectures handle translation and summarization more effectively, while mixture of experts (MoE) architectures can be a powerful tool for improving the performance of both decoder-only and encoder-decoder models. Some specialized architectures focus on enhancing reasoning capabilities through techniques like chain-of-thought prompting or recursive reasoning.</li> \n <li><strong>Tokenization methodology</strong>: The way models process text affects performance on domain-specific tasks, particularly with specialized vocabulary.</li> \n <li><strong>Context window capabilities</strong>: Larger context windows enable processing more information at once, critical for document analysis and extended conversations.</li> \n <li><strong>Modality</strong>: Modality refers to type of data a model can process and generate, such as text, image, audio, or video. Consider the modality of the models depending on the use case, and choose the model optimized for that specific modality.</li> \n</ul> \n<h3><strong>Operational considerations</strong></h3> \n<p>Below listed operational considerations are critical for model selection as they directly impact the real-world feasibility, cost-effectiveness, and sustainability of AI deployments.</p> \n<ul> \n <li><strong>Throughput and latency profiles</strong>: Response speed impacts user experience and throughput determines scalability.</li> \n <li><strong>Cost structures</strong>: Input/output token pricing significantly affects economics at scale.</li> \n <li><strong>Scalability characteristics</strong>: Ability to handle concurrent requests and maintain performance during traffic spikes.</li> \n <li><strong>Customization options</strong>: Fine-tuning capabilities and adaptation methods for tailoring to specific use cases or domains.</li> \n <li><strong>Ease of integration</strong>: Ease of integration into existing systems and workflow is an important consideration.</li> \n <li><strong>Security</strong>: When dealing with sensitive data, model security—including data encryption, access control, and vulnerability management—is a crucial consideration.</li> \n</ul> \n<h3><strong>Responsible AI attributes</strong></h3> \n<p>As AI becomes increasingly embedded in business operations and daily lives, evaluating models on responsible AI attributes isn’t just a technical consideration—it’s a business imperative.</p> \n<ul> \n <li><strong>Hallucination propensity</strong>: Models vary in their tendency to generate plausible but incorrect information.</li> \n <li><strong>Bias measurements</strong>: Performance across different demographic groups affects fairness and equity.</li> \n <li><strong>Safety guardrail effectiveness</strong>: Resistance to generating harmful or inappropriate content.</li> \n <li><strong>Explainability and privacy</strong>: Transparency features and handling of sensitive information.</li> \n <li><strong>Legal Implications</strong>: Legal considerations should include data privacy, non-discrimination, intellectual property, and product liability.</li> \n</ul> \n<h2>Agentic AI considerations for model selection</h2> \n<p>The growing popularity of agentic AI applications introduces evaluation dimensions beyond traditional metrics. When assessing models for use in autonomous agents, consider these critical capabilities:</p> \n<p><strong>Agent-specific evaluation dimensions</strong></p> \n<ul> \n <li><strong>Planning and reasoning capabilities</strong>: Evaluate chain-of-thought consistency across complex multi-step tasks and self-correction mechanisms that allow agents to identify and fix their own reasoning errors.</li> \n <li><strong>Tool and API integration</strong>: Test function calling capabilities, parameter handling precision, and structured output consistency (JSON/XML) for seamless tool use.</li> \n <li><strong>Agent-to-agent communication</strong>: Assess protocol adherence to frameworks like A2A and efficient contextual memory management across extended multi-agent interactions.</li> \n</ul> \n<p><strong>Multi-agent collaboration testing</strong> for applications using multiple specialized agents</p> \n<ul> \n <li><strong>Role adherence</strong>: Measure how well models maintain distinct agent personas and responsibilities without role confusion.</li> \n <li><strong>Information sharing efficiency</strong>: Test how effectively information flows between agent instances without critical detail loss.</li> \n <li><strong>Collaborative intelligence</strong>: Verify whether multiple agents working together produce better outcomes than single-model approaches.</li> \n <li><strong>Error propagation resistance</strong>: Assess how robustly multi-agent systems contain and correct errors rather than amplifying them.</li> \n</ul> \n<h2>A four-phase evaluation methodology</h2> \n<p>Our recommended methodology progressively narrows model selection through increasingly sophisticated assessment techniques:</p> \n<h3>Phase 1: Requirements engineering</h3> \n<p>Begin with a precise specification of your application’s requirements:</p> \n<ul> \n <li><strong>Functional requirements</strong>: Define primary tasks, domain knowledge needs, language support, output formats, and reasoning complexity.</li> \n <li><strong>Non-functional requirements</strong>: Specify latency thresholds, throughput requirements, budget constraints, context window needs, and availability expectations.</li> \n <li><strong>Responsible AI requirements</strong>: Establish hallucination tolerance, bias mitigation needs, safety requirements, explainability level, and privacy constraints.</li> \n <li><strong>Agent-specific requirements</strong>: For agentic applications, define tool-use capabilities, protocol adherence standards, and collaboration requirements.</li> \n</ul> \n<p>Assign weights to each requirement based on business priorities to create your evaluation scorecard foundation.</p> \n<h3>Phase 2: Candidate model selection</h3> \n<p>Use the Amazon Bedrock model information <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html\" rel=\"noopener noreferrer\" target=\"_blank\">API</a> to filter models based on hard requirements. This typically reduces candidates from dozens to 3–7 models that are worth detailed evaluation.</p> \n<p>Filter options include but aren’t limited to the following:</p> \n<ul> \n <li>Filter by modality support, context length, and language capabilities</li> \n <li>Exclude models that don’t meet minimum performance thresholds</li> \n <li>Calculate theoretical costs at projected scale so that you can exclude options that exceed the available budget</li> \n <li>Filter for customization requirements such as fine-tuning capabilities</li> \n <li>For agentic applications, filter for function calling and multi-agent protocol support</li> \n</ul> \n<p>Although the Amazon Bedrock model information API might not provide the filters you need for candidate selection, you can use the Amazon Bedrock model catalog (shown in the following figure) to obtain additional information about these models.</p> \n<p><img alt=\"Bedrock model catalog\" class=\"alignnone size-full wp-image-114282\" height=\"1344\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ML-19016-Bedrock-model-catalog.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"3024\" /></p> \n<h3>Phase 3: Systematic performance evaluation</h3> \n<p>Implement structured evaluation using <a href=\"https://aws.amazon.com/bedrock/evaluations/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Evaluations</a>:</p> \n<ol> \n <li><strong>Prepare evaluation datasets</strong>: Create representative task examples, challenging edge cases, domain-specific content, and adversarial examples.</li> \n <li><strong>Design evaluation prompts</strong>: Standardize instruction format, maintain consistent examples, and mirror production usage patterns.</li> \n <li><strong>Configure metrics</strong>: Select appropriate metrics for subjective tasks (human evaluation and reference-free quality), objective tasks (precision, recall, and F1 score), and reasoning tasks (logical consistency and step validity).</li> \n <li><strong>For agentic applications</strong>: Add protocol conformance testing, multi-step planning assessment, and tool-use evaluation.</li> \n <li><strong>Execute evaluation jobs</strong>: Maintain consistent parameters across models and collect comprehensive performance data.</li> \n <li><strong>Measure operational performance</strong>: Capture throughput, latency distributions, error rates, and actual token consumption costs.</li> \n</ol> \n<h3>Phase 4: Decision analysis</h3> \n<p>Transform evaluation data into actionable insights:</p> \n<ol> \n <li><strong>Normalize metrics</strong>: Scale all metrics to comparable units using min-max normalization.</li> \n <li><strong>Apply weighted scoring</strong>: Calculate composite scores based on your prioritized requirements.</li> \n <li><strong>Perform sensitivity analysis</strong>: Test how robust your conclusions are against weight variations.</li> \n <li><strong>Visualize performance</strong>: Create radar charts, efficiency frontiers, and tradeoff curves for clear comparison.</li> \n <li><strong>Document findings</strong>: Detail each model’s strengths, limitations, and optimal use cases.</li> \n</ol> \n<h2>Advanced evaluation techniques</h2> \n<p>Beyond standard procedures, consider the following approaches for evaluating models.</p> \n<h3>A/B testing with production traffic</h3> \n<p>Implement comparative testing using <a href=\"https://aws.amazon.com/bedrock/intelligent-prompt-routing/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock’s routing</a> capabilities to gather real-world performance data from actual users.</p> \n<h3>Adversarial testing</h3> \n<p>Test model vulnerabilities through prompt injection attempts, challenging syntax, edge case handling, and domain-specific factual challenges.</p> \n<h3>Multi-model ensemble evaluation</h3> \n<p>Assess combinations such as sequential pipelines, voting ensembles, and cost-efficient routing based on task complexity.</p> \n<h3>Continuous evaluation architecture</h3> \n<p>Design systems to monitor production performance with:</p> \n<ul> \n <li>Stratified sampling of production traffic across task types and domains</li> \n <li>Regular evaluations and trigger-based reassessments when new models emerge</li> \n <li>Performance thresholds and alerts for quality degradation</li> \n <li>User feedback collection and failure case repositories for continuous improvement</li> \n</ul> \n<h3>Industry-specific considerations</h3> \n<p>Different sectors have unique requirements that influence model selection:</p> \n<ul> \n <li><strong>Financial services</strong>: Regulatory compliance, numerical precision, and personally identifiable information (PII) handling capabilities</li> \n <li><strong>Healthcare</strong>: Medical terminology understanding, HIPAA adherence, and clinical reasoning</li> \n <li><strong>Manufacturing</strong>: Technical specification comprehension, procedural knowledge, and spatial reasoning</li> \n <li><strong>Agentic systems</strong>: Autonomous reasoning, tool integration, and protocol conformance</li> \n</ul> \n<h2>Best practices for model selection</h2> \n<p>Through this comprehensive approach to model evaluation and selection, organizations can make informed decisions that balance performance, cost, and operational requirements while maintaining alignment with business objectives. The methodology makes sure that model selection isn’t a one-time exercise but an evolving process that adapts to changing needs and technological capabilities.</p> \n<ul> \n <li><strong>Assess your situation thoroughly</strong>: Understand your specific use case requirements and available resources</li> \n <li><strong>Select meaningful metrics</strong>: Focus on metrics that directly relate to your business objectives</li> \n <li><strong>Build for continuous evaluation</strong>: Design your evaluation process to be repeatable as new models are released</li> \n</ul> \n<h2>Looking forward: The future of model selection</h2> \n<p>As foundation models evolve, evaluation methodologies must keep pace. Below are further considerations (By no means this list of considerations is exhaustive and is subject to ongoing updates as technology evolves and best practices emerge), you should take into account while selecting the best model(s) for your use-case(s).</p> \n<ul> \n <li><strong>Multi-model architectures</strong>: Enterprises will increasingly deploy specialized models in concert rather than relying on single models for all tasks.</li> \n <li><strong>Agentic landscapes</strong>: Evaluation frameworks must assess how models perform as autonomous agents with tool-use capabilities and inter-agent collaboration.</li> \n <li><strong>Domain specialization</strong>: The growing landscape of domain-specific models will require more nuanced evaluation of specialized capabilities.</li> \n <li><strong>Alignment and control</strong>: As models become more capable, evaluation of controllability and alignment with human intent becomes increasingly important.</li> \n</ul> \n<h2>Conclusion</h2> \n<p>By implementing a comprehensive evaluation framework that extends beyond basic metrics, organizations can informed decisions about which foundation models will best serve their requirements. For agentic AI applications in particular, thorough evaluation of reasoning, planning, and collaboration capabilities is essential for success. By approaching model selection systematically, organizations can avoid the common pitfalls of over-provisioning, misalignment with use case needs, excessive operational costs, and late discovery of performance issues. The investment in thorough evaluation pays dividends through optimized costs, improved performance, and superior user experiences.</p> \n<hr /> \n<h3>About the author</h3> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-81283 alignleft\" height=\"116\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/07/23/snghigf-e1721342253292-258x300-1.jpg\" width=\"100\" />Sandeep Singh</strong> is a Senior Generative AI Data Scientist at Amazon Web Services, helping businesses innovate with generative AI. He specializes in generative AI, machine learning, and system design. He has successfully delivered state-of-the-art AI/ML-powered solutions to solve complex business problems for diverse industries, optimizing efficiency and scalability.</p>"
        }
      ]
    },
    {
      "title": "Accelerate intelligent document processing with generative AI on AWS",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Accelerate intelligent document processing with generative AI on AWS"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/accelerate-intelligent-document-processing-with-generative-ai-on-aws/"
        },
        {
          "length": "27253390",
          "type": "video/mp4",
          "href": "https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-18800/video_1755214496026.mp4",
          "rel": "enclosure"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/accelerate-intelligent-document-processing-with-generative-ai-on-aws/",
      "authors": [
        {
          "name": "Bob Strahan"
        }
      ],
      "author": "Bob Strahan",
      "author_detail": {
        "name": "Bob Strahan"
      },
      "published": "Fri, 22 Aug 2025 17:26:31 +0000",
      "published_parsed": [
        2025,
        8,
        22,
        17,
        26,
        31,
        4,
        234,
        0
      ],
      "tags": [
        {
          "term": "Amazon Bedrock Data Automation",
          "scheme": null,
          "label": null
        },
        {
          "term": "Generative AI",
          "scheme": null,
          "label": null
        }
      ],
      "id": "4974e83fa545d7ff5050e513c06b99a9a08ab9c3",
      "guidislink": false,
      "summary": "In this post, we introduce our open source GenAI IDP Accelerator—a tested solution that we use to help customers across industries address their document processing challenges. Automated document processing workflows accurately extract structured information from documents, reducing manual effort. We will show you how this ready-to-deploy solution can help you build those workflows with generative AI on AWS in days instead of months.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we introduce our open source GenAI IDP Accelerator—a tested solution that we use to help customers across industries address their document processing challenges. Automated document processing workflows accurately extract structured information from documents, reducing manual effort. We will show you how this ready-to-deploy solution can help you build those workflows with generative AI on AWS in days instead of months."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>Every day, organizations process millions of documents, including invoices, contracts, insurance claims, medical records, and financial statements. Despite the critical role these documents play, an <a href=\"https://mitsloan.mit.edu/ideas-made-to-matter/tapping-power-unstructured-data\" rel=\"noopener noreferrer\" target=\"_blank\">estimated 80–90%</a> of the data they contain is unstructured and largely untapped, hiding valuable insights that could transform business outcomes. Despite advances in technology, many organizations still rely on manual data entry, spending countless hours extracting information from PDFs, scanned images, and forms. This manual approach is time-consuming, error-prone, and prevents organizations from scaling their operations and responding quickly to business demands.</p> \n<p>Although generative AI has made it easier to build proof-of-concept document processing solutions, the journey from proof of concept to production remains fraught with challenges. Organizations often find themselves rebuilding from scratch when they discover their prototype can’t handle production volumes, lacks proper error handling, doesn’t scale cost-effectively, or fails to meet enterprise security and compliance requirements. What works in a demo with a handful of documents often breaks down when processing thousands of documents daily in a production environment.</p> \n<p>In this post, we introduce our open source <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/README.md\" rel=\"noopener noreferrer\" target=\"_blank\">GenAI IDP Accelerator</a>—a tested solution that we use to help customers across industries address their document processing challenges. Automated document processing workflows accurately extract structured information from documents, reducing manual effort. We will show you how this ready-to-deploy solution can help you build those workflows with generative AI on AWS in days instead of months.</p> \n<h2>Understanding intelligent document processing</h2> \n<p>Intelligent document processing (IDP) encompasses the technologies and techniques used to extract and process data from various document types. Common IDP tasks include:</p> \n<ul> \n <li><strong>OCR (Optical Character Recognition)</strong> – Converting scanned documents and images into machine-readable text</li> \n <li><strong>Document classification</strong> – Automatically identifying document types (such as invoices, contracts, or forms)</li> \n <li><strong>Data extraction</strong> – Pulling structured information from unstructured documents</li> \n <li><strong>Assessment</strong> – Evaluating the quality and confidence of extracted data</li> \n <li><strong>Summarization</strong> – Creating concise summaries of document content</li> \n <li><strong>Evaluation</strong> – Measuring accuracy and performance against expected outcomes</li> \n</ul> \n<p>These capabilities are critical across industries. In financial services, organizations use IDP to process loan applications, extract data from bank statements, and validate insurance claims. Healthcare providers rely on IDP to extract patient information from medical records, process insurance forms, and handle lab results efficiently. Manufacturing and logistics companies use IDP to process invoices and purchase orders, extract shipping information, and handle quality certificates. Government agencies use IDP to process citizen applications, extract data from tax forms, manage permits and licenses, and enforce regulatory compliance.</p> \n<h2>The generative AI revolution in IDP</h2> \n<p>Traditional IDP solutions relied on template-based extraction, regular expressions, and classical machine learning (ML) models. Though functional, these approaches required extensive setup, struggled with document variations, and achieved limited accuracy on complex documents.</p> \n<p>The emergence of large language models (LLMs) and generative AI has fundamentally transformed IDP capabilities. Modern AI models can understand document context, handle variations without templates, achieve near-human accuracy on complex extractions, and adapt to new document types with minimal examples. This shift from rule-based to intelligence-based processing means organizations can now process different document types with high accuracy, dramatically reducing the time and cost of implementation.</p> \n<h2>GenAI IDP Accelerator</h2> \n<p>We’re excited to share the <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/README.md\" rel=\"noopener noreferrer\" target=\"_blank\">GenAI IDP Accelerator</a>—an open source solution that transforms how organizations handle document processing by dramatically reducing manual effort and improving accuracy. This serverless foundation offers processing patterns which use <a href=\"https://aws.amazon.com/bedrock/bda/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Data Automation</a> for rich out-of-the-box document processing features, high accuracy, ease of use, and straightforward per-page pricing, <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a> state-of-the-art foundation models (FMs) for complex documents requiring custom logic, and other AWS AI services to provide a flexible, scalable starting point for enterprises to build document automation tailored to their specific needs.</p> \n<p>The following is a short demo of the solution in action, in this case showcasing the default Amazon Bedrock Data Automation processing pattern.</p> \n<div class=\"wp-video\" style=\"width: 640px;\">\n <video class=\"wp-video-shortcode\" controls=\"controls\" height=\"360\" id=\"video-114439-1\" preload=\"metadata\" width=\"640\">\n  <source src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-18800/video_1755214496026.mp4?_=1\" type=\"video/mp4\" />\n </video>\n</div> \n<h2>Real-world impact</h2> \n<p>The GenAI IDP Accelerator is already transforming document processing for organizations across industries.</p> \n<h3>Competiscan: Transforming marketing intelligence at scale</h3> \n<p>Competiscan, a leader in competitive marketing intelligence, faced a massive challenge: processing 35,000–45,000 marketing campaigns daily while maintaining a searchable archive of 45 million campaigns spanning 15 years.</p> \n<p>Using the GenAI IDP Accelerator, Competiscan achieved the following:</p> \n<ul> \n <li>85% classification and extraction accuracy across diverse marketing materials</li> \n <li>Increased scalability to handle 35,000–45,000 daily campaigns</li> \n <li>Removal of critical bottlenecks, facilitating business growth</li> \n <li>Production deployment in just 8 weeks from initial concept</li> \n</ul> \n<h3>Ricoh: Scaling document processing</h3> \n<p>Ricoh, a global leader in document management, implemented the GenAI IDP Accelerator to transform healthcare document processing for their clients. Processing over 10,000 healthcare documents monthly with potential to scale to 70,000, they needed a solution that could handle complex medical documentation with high accuracy.</p> \n<p>The results speak for themselves:</p> \n<ul> \n <li>Savings potential of over 1,900 person-hours annually through automation</li> \n <li>Achieved extraction accuracy to help minimize financial penalties from processing errors</li> \n <li>Automated classification of grievances vs. appeals</li> \n <li>Created a reusable framework deployable across multiple healthcare customers</li> \n <li>Integrated with human-in-the-loop review for cases requiring expert validation</li> \n <li>Leveraged modular architecture to integrate with existing systems, enabling custom document splitting and large-scale document processing</li> \n</ul> \n<h2>Solution overview</h2> \n<p>The GenAI IDP Accelerator is a modular, serverless solution that automatically converts unstructured documents into structured, actionable data. Built entirely on AWS services, it provides enterprise-grade scalability, security, and cost-effectiveness while requiring minimal setup and maintenance. Its configuration-driven design helps teams quickly adapt prompts, extraction templates, and validation rules for their specific document types without touching the underlying infrastructure.</p> \n<p>The solution follows a modular pipeline that enriches documents at each stage, from OCR to classification, to extraction, to assessment, to summarization, and ending with evaluation.</p> \n<p>You can deploy and customize each step independently, so you can optimize for your specific use cases while maintaining the benefits of the integrated workflow.</p> \n<p>The following diagram illustrates the solution architecture, showing the default Bedrock Data Automation workflow (Pattern-1).</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114443\" height=\"629\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-188002.png\" width=\"1163\" /></p> \n<p>Refer to the <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/architecture.md\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub repo</a> for additional details and processing patterns.</p> \n<p>Some of the key features of the solution include:</p> \n<ul> \n <li><strong>Serverless architecture</strong> – Built on <a href=\"https://aws.amazon.com/lambda/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Lambda</a>, <a href=\"https://aws.amazon.com/step-functions/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Step Functions</a>, and other serverless technologies for queueing, concurrency management, and retries to provide automatic scaling and pay-per-use pricing for production workloads of many sizes</li> \n <li><strong>Generative AI-powered document packet splitting and classification</strong> – Intelligent document <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/classification.md\" rel=\"noopener noreferrer\" target=\"_blank\">classification</a> using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMs, including support for multi-document packets and packet splitting</li> \n <li><strong>Advanced AI key information extraction</strong> – Key information <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/extraction.md\" rel=\"noopener noreferrer\" target=\"_blank\">extraction</a> using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMs</li> \n <li><strong>Multiple processing patterns</strong> – Choose from pre-built patterns optimized for different workloads with different configurability, cost, and accuracy requirements, or extend the solution with additional patterns: \n  <ul> \n   <li><a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/pattern-1.md\" rel=\"noopener noreferrer\" target=\"_blank\">Pattern 1</a> – Uses Amazon Bedrock Data Automation, a fully managed service that offers rich out-of-the-box features, ease of use, and straightforward per-page pricing. This pattern is recommended for most use cases.</li> \n   <li><a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/pattern-2.md\" rel=\"noopener noreferrer\" target=\"_blank\">Pattern 2</a> – Uses Amazon Textract and Amazon Bedrock with Amazon Nova, Anthropic’s Claude, or custom fine-tuned Amazon Nova models. This pattern is ideal for complex documents requiring custom logic.</li> \n   <li><a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/pattern-3.md\" rel=\"noopener noreferrer\" target=\"_blank\">Pattern 3</a> – Uses Amazon Textract, <a href=\"https://aws.amazon.com/sagemaker/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker</a> with a fine-tuned model for classification, and Amazon Bedrock for extraction. This pattern is ideal for documents requiring specialized classification.</li> \n  </ul> </li> \n</ul> \n<p>We expect to add more pattern options to handle additional real-world document processing needs, and to take advantage of ever-improving state-of-the-art capabilities:</p> \n<ul> \n <li><strong>Few-shot learning</strong> – Improve accuracy for classification and extraction by providing <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/few-shot-examples.md\" rel=\"noopener noreferrer\" target=\"_blank\">few-shot examples</a> to guide the AI models</li> \n <li><strong>Confidence assessment</strong> – AI-powered quality assurance that <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/assessment.md\" rel=\"noopener noreferrer\" target=\"_blank\">evaluates extraction field confidence</a>, used to indicate documents for human review</li> \n <li><strong>Human-in-the-loop (HITL) review</strong> – Integrated workflow for <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/pattern-1.md#human-in-the-loop-hitl\" rel=\"noopener noreferrer\" target=\"_blank\">human review</a> of low-confidence extractions using <a href=\"https://aws.amazon.com/augmented-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker Augmented AI</a> (Amazon A2I), currently available for Pattern 1, with support for Patterns 2 and 3 coming soon</li> \n <li><strong>Web user interface</strong> – <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/web-ui.md\" rel=\"noopener noreferrer\" target=\"_blank\">Responsive web UI</a> for monitoring document processing, viewing results, and managing configurations</li> \n <li><strong>Knowledge base integration</strong> – <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/knowledge-base.md\" rel=\"noopener noreferrer\" target=\"_blank\">Query processed documents</a> using natural language through <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Knowledge Bases</a></li> \n <li><strong>Built-in evaluation</strong> – Framework to <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/evaluation.md\" rel=\"noopener noreferrer\" target=\"_blank\">evaluate</a> and improve accuracy against baseline data</li> \n <li><strong>Analytics and reporting database</strong> – Centralized <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/reporting-database.md\" rel=\"noopener noreferrer\" target=\"_blank\">analytics database</a> for tracking processing metrics, accuracy trends, and cost optimization across document workflows, and for analyzing extracted document content using <a href=\"http://aws.amazon.com/athena\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Athena</a></li> \n <li><strong>No-code configuration</strong> – Customize document types, extraction fields, and processing logic through <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/configuration.md\" rel=\"noopener noreferrer\" target=\"_blank\">configuration</a>, editable in the web UI</li> \n <li><strong>Developer-friendly python package</strong> – For data science and engineering teams who want to experiment, optimize, or integrate the IDP capabilities directly into their workflows, the solution’s core logic is available through the <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/lib/idp_common_pkg/README.md\" rel=\"noopener noreferrer\" target=\"_blank\">idp_common Python package</a></li> \n</ul> \n<h2>Prerequisites</h2> \n<p>Before you deploy the solution, make sure you have an AWS account with administrator permissions and access to Amazon and Anthropic models on Amazon Bedrock. For more details, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\" rel=\"noopener noreferrer\" target=\"_blank\">Access Amazon Bedrock foundation models</a>.</p> \n<h2>Deploy the GenAI IDP Accelerator</h2> \n<p>To deploy the GenAI IDP Accelerator, you can use the provided <a href=\"http://aws.amazon.com/cloudformation\" rel=\"noopener noreferrer\" target=\"_blank\">AWS CloudFormation</a> template. For more details, see the <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/README.md#quick-start\" rel=\"noopener noreferrer\" target=\"_blank\">quick start option</a> on the GitHub repo. The high-level steps are as follows:</p> \n<ol> \n <li>Log in to your AWS account.</li> \n <li>Choose <strong>Launch Stack</strong> for your preferred AWS Region:</li> \n</ol> \n<table border=\"1px\" cellpadding=\"10px\" class=\"styled-table\"> \n <thead> \n  <tr> \n   <th style=\"padding: 10px;\"><strong>Region</strong></th> \n   <th style=\"padding: 10px;\"><strong>Launch Stack</strong></th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td style=\"padding: 10px;\">US East (N. Virginia)</td> \n   <td style=\"padding: 10px;\"><a href=\"https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/create/review?templateURL=https://s3.us-east-1.amazonaws.com/aws-ml-blog-us-east-1/artifacts/genai-idp/idp-main.yaml&amp;stackName=IDP\" rel=\"noopener\" target=\"_blank\"><img alt=\"\" class=\"alignnone wp-image-114448 size-thumbnail\" height=\"19\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-188003-100x19.png\" width=\"100\" /></a></td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\">US West (Oregon)</td> \n   <td style=\"padding: 10px;\"><a href=\"https://us-west-2.console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/create/review?templateURL=https://s3.us-west-2.amazonaws.com/aws-ml-blog-us-west-2/artifacts/genai-idp/idp-main.yaml&amp;stackName=IDP\" rel=\"noopener\" target=\"_blank\"><img alt=\"\" class=\"alignnone wp-image-114448 size-thumbnail\" height=\"19\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-188003-100x19.png\" width=\"100\" /></a></td> \n  </tr> \n </tbody> \n</table> \n<ol> \n <li>Enter your email address and choose your processing pattern (default is Pattern 1, using Amazon Bedrock Data Automation).</li> \n <li>Use defaults for all other configuration parameters.</li> \n <li>Deploy the stack.</li> \n</ol> \n<p>The stack takes approximately 15–20 minutes to deploy the resources. After deployment, you will receive an email with login credentials for the web interface.</p> \n<h2>Process documents</h2> \n<p>After you deploy the solution, you can start processing documents:</p> \n<ol> \n <li>Use the web interface to upload a sample document (you can use the provided sample: <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/samples/lending_package.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">lending_package.pdf</a>).</li> \n</ol> \n<p>In production, you typically automate loading your documents directly to the <a href=\"http://aws.amazon.com/s3\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3) input bucket, automatically triggering processing. To learn more, see <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/deployment.md#testing-without-the-ui\" rel=\"noopener noreferrer\" target=\"_blank\">Testing without the UI</a>.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114447\" height=\"632\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-188005.jpeg\" width=\"903\" /></p> \n<ol> \n <li>Select your document from the document list and choose <strong>View Processing Flow</strong> to watch as your document flows through the pipeline.</li> \n</ol> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114446\" height=\"878\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-188006.png\" width=\"1432\" /></p> \n<ol start=\"2\"> \n <li>Examine the extracted data with confidence scores.</li> \n</ol> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114445\" height=\"718\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-188007.png\" width=\"1196\" /></p> \n<ol start=\"3\"> \n <li>Use the knowledge base feature to ask questions about processed content.</li> \n</ol> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114444\" height=\"730\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-188008.png\" width=\"1430\" /></p> \n<h2>Alternative deployment methods</h2> \n<p>You can <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/deployment.md#option-2-build-deployment-assets-from-source-code\" rel=\"noopener noreferrer\" target=\"_blank\">build the solution from source code</a> if you need to deploy the solution to additional Regions or build and deploy code changes<strong>. </strong></p> \n<p>We hope to add support for <a href=\"https://aws.amazon.com/cdk/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Cloud Development Kit</a> (AWS CDK) and Terraform deployments. Follow the <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub repository</a> for updates, or contact <a href=\"https://aws.amazon.com/professional-services/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Professional Services</a> for implementation assistance.</p> \n<h2>Update an existing GenAI IDP Accelerator stack</h2> \n<p>You can update your existing GenAI IDP Accelerator stack to the latest release. For more details, see <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws/blob/main/docs/deployment.md#updating-an-existing-stack\" rel=\"noopener noreferrer\" target=\"_blank\">Updating an Existing Stack</a>.</p> \n<h2>Clean up</h2> \n<p>When you’re finished experimenting, clean up your resources by using the AWS CloudFormation console to delete the IDP stack that you deployed.</p> \n<h2>Conclusion</h2> \n<p>In this post, we discussed the GenAI IDP Accelerator, a new approach to document processing that combines the power of generative AI with the reliability and scale of AWS. You can process hundreds or even millions of documents to achieve better results faster and more cost-effectively than traditional approaches.</p> \n<p>Visit the <a href=\"https://github.com/aws-solutions-library-samples/accelerated-intelligent-document-processing-on-aws\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub repository</a> for detailed guides and examples and choose <strong>watch</strong> to stay informed on new releases and features. <a href=\"https://aws.amazon.com/professional-services/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Professional Services</a> and <a href=\"https://aws.amazon.com/partners/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Partners</a> are available to help with implementation. You can also join the GitHub community to contribute improvements and share your experiences.</p> \n<hr /> \n<h3>About the Authors</h3> \n<p style=\"clear: both;\"><strong><img alt=\"Bob Strahan\" class=\"size-full wp-image-21654 alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/02/10/Bob-Strahan-p.png\" width=\"100\" />Bob Strahan</strong> is a Principal Solutions Architect in the AWS Generative AI Innovation Center.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-89414 alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/10/15/ml-15690-kijosp.jpg\" width=\"100\" />Joe King</strong> is a Senior Data Scientist in the AWS Generative AI Innovation Center.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-92662 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/13/mofijul_islam.jpg\" width=\"100\" />Mofijul Islam</strong> is an Applied Scientist in the AWS Generative AI Innovation Center.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-113451 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/vincil.png\" width=\"100\" />Vincil Bishop </strong>is a Senior Deep Learning Architect in the AWS Generative AI Innovation Center.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-114452 alignleft\" height=\"113\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/kaleko.jpg\" width=\"100\" />David Kaleko </strong>is a Senior Applied Scientist in the AWS Generative AI Innovation Center.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-114453 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/rafal.png\" width=\"100\" />Rafal Pawlaszek </strong>is a Senior Cloud Application Architect in the AWS Generative AI Innovation Center.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-thumbnail wp-image-114440 alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-18379_sromo_llama-100x133.jpg\" width=\"100\" />Spencer Romo </strong>is a Senior Data Scientist in the AWS Generative AI Innovation Center.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-114454 alignleft\" height=\"134\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/gudivt.jpg\" width=\"100\" />Vamsi Thilak Gudi </strong>is a Solutions Architect in the AWS World Wide Public Sector team.</p> \n<p style=\"clear: both;\"></p> \n<hr /> \n<h2>Acknowledgments</h2> \n<p>We would like to thank&nbsp;Abhi Sharma, Akhil Nooney, Aleksei Iancheruk, Ava Kong, Boyi Xie, Diego Socolinsky, Guillermo Tantachuco, Ilya Marmur, Jared Kramer, Jason Zhang, Jordan Ratner, Mariano Bellagamba, Mark Aiyer, Niharika Jain, Nimish Radia, Shean Sager, Sirajus Salekin, Yingwei Yu, and many others in our expanding community, for their unwavering vision, passion, contributions, and guidance throughout.</p>"
        }
      ]
    },
    {
      "title": "Amazon SageMaker HyperPod enhances ML infrastructure with scalability and customizability",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Amazon SageMaker HyperPod enhances ML infrastructure with scalability and customizability"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-hyperpod-enhances-ml-infrastructure-with-scalability-and-customizability/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-hyperpod-enhances-ml-infrastructure-with-scalability-and-customizability/",
      "authors": [
        {
          "name": "Mark Vinciguerra"
        }
      ],
      "author": "Mark Vinciguerra",
      "author_detail": {
        "name": "Mark Vinciguerra"
      },
      "published": "Fri, 22 Aug 2025 17:14:39 +0000",
      "published_parsed": [
        2025,
        8,
        22,
        17,
        14,
        39,
        4,
        234,
        0
      ],
      "tags": [
        {
          "term": "Advanced (300)",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker HyperPod",
          "scheme": null,
          "label": null
        },
        {
          "term": "Compute",
          "scheme": null,
          "label": null
        },
        {
          "term": "Launch",
          "scheme": null,
          "label": null
        },
        {
          "term": "Technical How-to",
          "scheme": null,
          "label": null
        }
      ],
      "id": "4d40fb4928f1c5f2844cf8229eb7f6b998e00a92",
      "guidislink": false,
      "summary": "In this post, we introduced three features in SageMaker HyperPod that enhance scalability and customizability for ML infrastructure. Continuous provisioning offers flexible resource provisioning to help you start training and deploying your models faster and manage your cluster more efficiently. With custom AMIs, you can align your ML environments with organizational security standards and software requirements.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we introduced three features in SageMaker HyperPod that enhance scalability and customizability for ML infrastructure. Continuous provisioning offers flexible resource provisioning to help you start training and deploying your models faster and manage your cluster more efficiently. With custom AMIs, you can align your ML environments with organizational security standards and software requirements."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p><a href=\"https://aws.amazon.com/sagemaker-ai/hyperpod/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker HyperPod</a> is a purpose-built infrastructure for optimizing foundation model (FM) training and inference at scale. SageMaker HyperPod removes the undifferentiated heavy lifting involved in building and optimizing machine learning (ML) infrastructure for training FMs, reducing training time by up to 40%.</p> \n<p>SageMaker HyperPod offers persistent clusters with built-in resiliency, while also offering deep infrastructure control by allowing users to SSH into the underlying <a href=\"http://aws.amazon.com/ec2\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Elastic Compute Cloud</a> (Amazon EC2) instances. It helps efficiently scale model development and deployment tasks such as training, fine-tuning, or inference across a cluster of hundreds or thousands of AI accelerators, while reducing the operational heavy lifting involved in managing such clusters. As AI moves towards deployment adopting to a multitude of domains and use cases, the need for flexibility and control is becoming more pertinent. Large enterprises want to make sure the GPU clusters follow the organization-wide policies and security rules. Mission-critical AI/ML workloads often require specialized environments that align with the organization’s software stack and operational standards.</p> \n<p>SageMaker HyperPod supports <a href=\"https://aws.amazon.com/eks/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Elastic Kubernetes Service</a> (Amazon EKS) and offers two new features that enhance this control and flexibility to enable production deployment of large-scale ML workloads:</p> \n<ul> \n <li><strong>Continuous provisioning</strong> – SageMaker HyperPod now supports continuous provisioning, which enhances cluster scalability through features like partial provisioning, rolling updates, concurrent scaling operations, and continuous retries when launching and configuring your HyperPod cluster.</li> \n <li><strong>Custom AMIs </strong>– You can now use custom <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Machine Images</a> (AMIs), which enables the preconfiguration of software stacks, security agents, and proprietary dependencies that would otherwise require complex post-launch bootstrapping. Customers can create custom AMIs using the HyperPod public AMI as a base and install additional software required to meet their organization’s specific security and compliance requirements.</li> \n</ul> \n<p>In this post, we dive deeper into each of these features.</p> \n<h2>Continuous provisioning</h2> \n<p>The new continuous provisioning feature in SageMaker HyperPod represents a transformative advancement for organizations running intensive ML workloads, delivering unprecedented flexibility and operational efficiency that accelerates AI innovation. This feature provides the following benefits:</p> \n<ul> \n <li><strong>Partial provisioning</strong> – SageMaker HyperPod prioritizes delivering the maximum possible number of instances without failure. You can start running your workload while your cluster will attempt to provision the remaining instances.</li> \n <li><strong>Concurrent operations</strong> – SageMaker HyperPod supports simultaneous scaling and maintenance activities (such as scale up, scale down, and patching) on a single instance group waiting for previous operations to complete.</li> \n <li><strong>Continuous retries</strong> – SageMaker HyperPod persistently attempts to fulfill the user’s request until it encounters a <code>NonRecoverable</code> error from where recovery is not possible.</li> \n <li><strong>Increased customer visibility</strong> – SageMaker HyperPod maps customer-initiated and service-initiated operations to structured activity streams, providing real-time status updates and detailed progress tracking.</li> \n</ul> \n<p>For ML teams facing tight deadlines and resource constraints, this means dramatically reduced wait times and the ability to begin model training and deployment with whatever computing power is immediately available, while the system works diligently in the background to provision remaining requested resources.</p> \n<h3>Implement continuous provisioning in a SageMaker HyperPod cluster</h3> \n<p>The architecture introduces an intuitive yet powerful parameter that puts scaling strategy control directly in your hands: <code>--node-provisioning-mode</code>. Continuous provisioning maximizes resource utilization and operational agility.</p> \n<p>The following code creates a cluster with one instance group and continuous provisioning mode enabled using <code>--node-provisioning-mode</code>:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker&nbsp;create-cluster \\ \n--cluster-name $HP_CLUSTER_NAME \\\n--orchestrator 'Eks={ClusterArn='$EKS_CLUSTER_ARN'}' \\\n--vpc-config '{\n&nbsp;&nbsp; \"SecurityGroupIds\": [\"'$SECURITY_GROUP'\"],\n&nbsp;&nbsp; \"Subnets\": [\"'$SUBNET'\"]\n}' \\\n--instance-groups '{\n&nbsp;&nbsp; \"InstanceGroupName\": \"ig-1\",\n&nbsp;&nbsp; \"InstanceType\": \"ml.p6-b200.48xlarge\",\n&nbsp;&nbsp; \"InstanceCount\": 2,\n&nbsp;&nbsp; \"LifeCycleConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp;\"SourceS3Uri\": \"s3://'$BUCKET_NAME'\",\n&nbsp;&nbsp; &nbsp; &nbsp;\"OnCreate\": \"on_create.sh\"\n&nbsp;&nbsp; },\n&nbsp;&nbsp; \"ExecutionRole\": \"'$EXECUTION_ROLE'\",\n&nbsp;&nbsp; \"ThreadsPerCore\": 1\n}' \\\n--node-provisioning-mode Continuous\n{\n&nbsp;&nbsp; &nbsp;\"ClusterArn\": \"arn:aws:sagemaker:us-west-2:530295135845:cluster/pv09azbjo6hs\"\n}</code></pre> \n</div> \n<p>Additional features are released with continuous provisioning:</p> \n<ul> \n <li>Cron job scheduling for instance group software updates:</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker&nbsp;update-cluster --cluster-name $HP_CLUSTER_NAME \\\n--instance-groups '[{\n&nbsp;&nbsp; \"InstanceGroupName\": \"group2\",\n&nbsp;&nbsp; \"InstanceType\": \"ml.p6-b200.48xlarge\",\n&nbsp;&nbsp; \"InstanceCount\": 2,\n&nbsp;&nbsp; \"LifeCycleConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp;\"SourceS3Uri\": \"s3://'$BUCKET_NAME'\",\n&nbsp;&nbsp; &nbsp; &nbsp;\"OnCreate\": \"on_create.sh\"\n&nbsp;&nbsp; },\n&nbsp;&nbsp; \"ExecutionRole\": \"'$EXECUTION_ROLE'\",\n&nbsp;&nbsp; \"ThreadsPerCore\": 1,\n&nbsp;&nbsp; \"ScheduledUpdateConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp;\"ScheduleExpression\": \"cron(30 19 27 * ? *)\" # Cron job parameters:&nbsp;cron(Minutes Hours Day-of-month Month Day-of-week Year)\n&nbsp;&nbsp; }\n}]' \\</code></pre> \n</div> \n<ul> \n <li><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-rolling.html\">Rolling updates</a> with safety measures. With rolling deployment, HyperPod gradually shifts traffic from your old fleet to a new fleet. If there is an issue during deployment, it should not affect the whole cluster.</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker&nbsp;update-cluster --cluster-name $HP_CLUSTER_NAME \\\n--instance-groups '[{\n&nbsp;&nbsp; \"InstanceGroupName\": \"group4\",\n&nbsp;&nbsp; \"ScheduledUpdateConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp;\"ScheduleExpression\": \"cron(45 14 25 * ? *)\",\n&nbsp;&nbsp; &nbsp; &nbsp;\"DeploymentConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; \"AutoRollbackConfiguration\": [{\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"AlarmName\": \"RollbackPatchingAlarm\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; }],\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; \"RollingUpdatePolicy\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"MaximumBatchSize\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Type\": \"INSTANCE_COUNT\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \"Value\": 1\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; },\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; \"WaitIntervalInSeconds\": 15\n&nbsp;&nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; }\n}]'</code></pre> \n</div> \n<ul> \n <li>List cluster nodes API:</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker list-cluster-nodes --cluster-name $HP_CLUSTER_NAME</code></pre> \n</div> \n<ul> \n <li>Batch add nodes (add nodes to specific instance groups):</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker&nbsp;batch-add-cluster-nodes --cluster-name $HP_CLUSTER_NAME \\\n--nodes-to-add '[{\n&nbsp;&nbsp; \"InstanceGroupName\": \"group1\",\n&nbsp;&nbsp; \"IncrementTargetCountBy\": 5\n}]'</code></pre> \n</div> \n<ul> \n <li>Batch delete nodes (remove specific nodes by ID):</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker&nbsp;batch-delete-cluster-nodes --cluster-name $HP_CLUSTER_NAME \\\n--node-ids i-0b949a3867b2a963a</code></pre> \n</div> \n<ul> \n <li>Enable Training Plan capacity for instance provisioning by adding the <code>TrainingPlanArn</code> parameter during instance group creation:</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker&nbsp;update-cluster --cluster-name $HP_CLUSTER_NAME \\\n--instance-groups '[{\n&nbsp;&nbsp; \"InstanceGroupName\": \"training-group\",\n&nbsp;&nbsp; \"InstanceType\": \"ml.p6-b200.48xlarge\",\n&nbsp;&nbsp; \"InstanceCount\": 3,\n&nbsp;&nbsp; \"TrainingPlanArn\": \"YOUR_TRAINING_PLAN_ARN\"\n}]'</code></pre> \n</div> \n<ul> \n <li>Cluster event observability:</li> \n</ul> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemake list-cluster-events —cluster-name $HP_CLUSTER_NAME</code></pre> \n</div> \n<h2>Custom AMIs</h2> \n<p>To reduce operational overhead, nodes in a SageMaker HyperPod cluster are launched with the <a href=\"https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Deep Learning AMIs</a> (DLAMIs). AWS DLAMIs are pre-built AMIs that are optimized for running deep learning workloads on EC2 instances. They come pre-installed with popular deep learning frameworks, libraries, and tools to make it straightforward to get started with training and deploying deep learning models.</p> \n<p>The new custom AMI feature of SageMaker HyperPod unlocks even greater value for enterprise customers by delivering the granular control and operational excellence you need to accelerate AI initiatives while maintaining security standards. It seamlessly bridges high-performance computing requirements with enterprise-grade security and operational excellence.</p> \n<p>Organizations can now build customized AMIs using SageMaker HyperPod performance-tuned public AMIs as a foundation; teams can pre-install security agents, compliance tools, proprietary software, and specialized libraries directly into optimized images.</p> \n<p>This feature offers the following benefits:</p> \n<ul> \n <li>It accelerates time-to-value by minimizing runtime installation delays and reducing cluster initialization time through pre-built configurations.</li> \n <li>From a security standpoint, it enables enterprise-grade centralized control, so security teams can maintain complete oversight while meeting their compliance requirements.</li> \n <li>Operationally, the feature promotes excellence through standardized, reproducible environments using version-controlled AMIs, while providing seamless integration with existing workflows.</li> \n</ul> \n<p>The following sections outline a step-by-step approach to build your own AMI and use it on your SageMaker HyperPod cluster.</p> \n<h3>Select and obtain your SageMaker HyperPod base AMI</h3> \n<p>You can choose from two options to retrieve the SageMaker HyperPod base AMI. To use the Amazon EC2 console, complete the following steps:</p> \n<ol> \n <li>On the Amazon EC2 console, choose <strong>AMIs</strong> under <strong>Images</strong> in the navigation pane.</li> \n <li>Choose <strong>Public images</strong> as the image type and set the <strong>Owner alias</strong> filter to <strong>Amazon</strong>.</li> \n <li>Search for AMIs prefixed with <code>HyperPod EKS</code>.</li> \n <li>Choose the appropriate AMI (preferably the latest).</li> \n</ol> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/ML-19412-ami.jpeg\"><img alt=\"AMI-list\" class=\"alignnone wp-image-114505 size-full\" height=\"387\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/ML-19412-ami.jpeg\" style=\"margin: 10px 0px 10px 0px;\" width=\"1289\" /></a></p> \n<p>Alternatively, you can use the <a href=\"https://aws.amazon.com/cli/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Command Line Interface</a> (AWS CLI) with <a href=\"https://aws.amazon.com/systems-manager/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Systems Manager</a> to fetch the latest SageMaker HyperPod base AMI:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-php\">aws ssm get-parameter \\\n&nbsp;&nbsp;--name \"/aws/service/sagemaker-hyperpod/ami/x86_64/eks-1.31-amazon-linux-2/latest/ami-id\" \\\n&nbsp;&nbsp;--region us-west-2 \\\n&nbsp;&nbsp;--query \"Parameter.Value\" \\\n&nbsp;&nbsp;--output text\n\n// Replace the parameter name with corresponding kubernetes version as required.\n// For example, If you want to use kubernetes 1.30, use the following parameter</code></pre> \n</div> \n<h3>Build your custom AMI</h3> \n<p>After you select a SageMaker HyperPod public AMI, use that as the base AMI to <a href=\"https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html\" rel=\"noopener noreferrer\" target=\"_blank\">build your own custom AMI</a> using one of the following methods. This is not an exhaustive list for building AMIs; you can use your preferred method. SageMaker HyperPod does not have any strong recommendations.</p> \n<ul> \n <li><strong>Amazon EC2 console </strong>– Choose your customized EC2 instance, then choose <strong>Action</strong>, <strong>Image and Templates</strong>, <strong>Create Image</strong>.</li> \n <li><strong>AWS CLI</strong> – Use the <code>aws ec2</code> <a href=\"https://docs.aws.amazon.com/cli/latest/reference/ec2/create-image.html\" rel=\"noopener noreferrer\" target=\"_blank\">create-image</a> command.</li> \n <li><strong>HashiCorp Packer</strong> – <a href=\"https://developer.hashicorp.com/packer\" rel=\"noopener noreferrer\" target=\"_blank\">Packer</a> is an open source tool from HashiCorp that you can use to create identical machine images for multiple platforms from a single source configuration. It supports creating AMIs for AWS, as well as images for other cloud providers and virtualization platforms.</li> \n <li><strong>EC2 Image Builder </strong>– <a href=\"https://aws.amazon.com/image-builder/\" rel=\"noopener noreferrer\" target=\"_blank\">EC2 Image Builder</a> is a fully managed AWS service that makes it straightforward to automate the creation, maintenance, validation, sharing, and deployment of Linux or Windows Server images.</li> \n</ul> \n<h3>Set up the required permissions</h3> \n<p>Before you start using custom AMIs, confirm you have the required <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) policies configured. Make sure you add the following policies to your <code>ClusterAdmin</code> user permissions (IAM policy):</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\"># Minimum set of permissions for admin to run the HyperPod core APIs\n\"sagemaker:CreateCluster\",\n\"sagemaker:DeleteCluster\",\n\"sagemaker:DescribeCluster\",\n\"sagemaker:DescribeCluterNode\",\n\"sagemaker:ListClusterNodes\",\n\"sagemaker:ListClusters\",\n\"sagemaker:UpdateCluster\",\n\"sagemaker:UpdateClusterSoftware\",\n\"sagemaker:BatchDeleteClusterNodes\",\n\"eks:DescribeCluster\",\n\"eks:CreateAccessEntry\",\n\"eks:DescribeAccessEntry\",\n\"eks:DeleteAccessEntry\",\n\"eks:AssociateAccessPolicy\",\n\"iam:CreateServiceLinkedRole\",\n\n# Permissions required to manage HyperPod clusters with custom AMI\n\"ec2:DescribeImages\",\n\"ec2:ModifyImageAttribute\",\n\"ec2:modifySnapshotAttribute\",\n\"ec2:DescribeSnapshots\"</code></pre> \n</div> \n<h3>Run cluster management operations</h3> \n<p>To create a cluster with a custom AMI, use the <code>aws sagemaker create-cluster</code> command. Specify your custom AMI in the <code>ImageId</code> parameter, and include other required cluster configurations:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker&nbsp;create-cluster \\\n&nbsp;&nbsp; --cluster-name clusterNameHere \\\n&nbsp;&nbsp; --orchestrator 'Eks={ClusterArn='$EKS_CLUSTER_ARN'}' \\\n&nbsp;&nbsp; --node-provisioning-mode Continuous&nbsp;\\\n&nbsp;&nbsp; --instance-groups '{\n&nbsp;&nbsp; \"InstanceGroupName\": \"groupNameHere\",\n&nbsp;&nbsp; \"InstanceType\": \"ml.p6-b200.48xlarge\",\n&nbsp;&nbsp; \"InstanceCount\": 2,\n&nbsp;&nbsp; \"LifeCycleConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp;\"SourceS3Uri\": \"s3://'$BUCKET_NAME'\",\n&nbsp;&nbsp; &nbsp; &nbsp;\"OnCreate\": \"on_create.sh\"\n&nbsp;&nbsp; },\n&nbsp;&nbsp; \"ImageId: \"&lt;YOUR_CUSTOM_AMI&gt;,\n&nbsp;&nbsp; \"ExecutionRole\": \"'$EXECUTION_ROLE'\",\n&nbsp;&nbsp; \"ThreadsPerCore\": 1,\n&nbsp;&nbsp;&nbsp;\"InstanceStorageConfigs\": [\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"EbsVolumeConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"VolumeSizeInGB\": 500,\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; ]\n}' --vpc-config '{\n&nbsp;&nbsp; \"SecurityGroupIds\": [\"'$SECURITY_GROUP'\"],\n&nbsp;&nbsp; \"Subnets\": [\"'$SUBNET'\"]\n}'</code></pre> \n</div> \n<p>Scale up an instance group with the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker&nbsp;update-cluster \\\n&nbsp;&nbsp; &nbsp;--cluster-name $HP_CLUSTER_NAME --instance-groups '[{ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n&nbsp;&nbsp; &nbsp;\"InstanceGroupName\": \"groupNameHere\",\n&nbsp;&nbsp; \"InstanceType\": \"ml.p6-b200.48xlarge\",\n&nbsp;&nbsp; \"InstanceCount\": 10,\n&nbsp;&nbsp; \"LifeCycleConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp;\"SourceS3Uri\": \"s3://'$BUCKET_NAME'\",\n&nbsp;&nbsp; &nbsp; &nbsp;\"OnCreate\": \"on_create.sh\"\n&nbsp;&nbsp; },\n&nbsp;&nbsp; \"ExecutionRole\": \"'$EXECUTION_ROLE'\",\n&nbsp;&nbsp; \"ThreadsPerCore\": 1,\n&nbsp;&nbsp; \"ImageId: \"&lt;YOUR_CUSTOM_AMI&gt;,\n}]'</code></pre> \n</div> \n<p>Add an instance group with the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">aws sagemaker&nbsp;update-cluster \\\n&nbsp;&nbsp; --cluster-name \"clusterNameHere\" \\\n&nbsp;&nbsp; --instance-groups '{\n&nbsp;&nbsp; \"InstanceGroupName\": \"groupNameHere\",\n&nbsp;&nbsp; \"InstanceType\": \"ml.p6-b200.48xlarge\",\n&nbsp;&nbsp; \"InstanceCount\": 10,\n&nbsp;&nbsp; \"LifeCycleConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp;\"SourceS3Uri\": \"s3://'$BUCKET_NAME'\",\n&nbsp;&nbsp; &nbsp; &nbsp;\"OnCreate\": \"on_create.sh\"\n&nbsp;&nbsp; },\n&nbsp;&nbsp; \"ExecutionRole\": \"'$EXECUTION_ROLE'\",\n&nbsp;&nbsp; \"ThreadsPerCore\": 1,\n&nbsp;&nbsp; \"ImageId: \"&lt;YOUR_CUSTOM_AMI&gt;,\n}' '{\n&nbsp;&nbsp; \"InstanceGroupName\": \"groupNameHere2\",\n&nbsp;&nbsp; \"InstanceType\": \"ml.c5.2xlarge\",\n&nbsp;&nbsp; \"InstanceCount\": 1,\n&nbsp;&nbsp; \"LifeCycleConfig\": {\n&nbsp;&nbsp; &nbsp; &nbsp;\"SourceS3Uri\": \"s3://'$BUCKET_NAME'\",\n&nbsp;&nbsp; &nbsp; &nbsp;\"OnCreate\": \"on_create.sh\"\n&nbsp;&nbsp; },\n&nbsp;&nbsp; \"ExecutionRole\": \"'$EXECUTION_ROLE'\",\n&nbsp;&nbsp; \"ThreadsPerCore\": 1,\n&nbsp;&nbsp; \"ImageId: \"&lt;YOUR_CUSTOM_AMI_2&gt;,\n}'</code></pre> \n</div> \n<h3>Considerations</h3> \n<p>When using custom AMIs with your cluster, be aware of the following requirements and limitations:</p> \n<ul> \n <li><strong>Snapshot support</strong> – Custom AMIs must contain only the root snapshot. Additional snapshots are not supported and will cause cluster creation or update operations to fail with a validation exception if the AMI contains additional snapshots beyond the root volume.</li> \n <li><strong>Patching</strong> – <code>ImageId</code> in <code>update-cluster</code> is immutable. For patching existing instance groups, you must use <a href=\"https://docs.aws.amazon.com/cli/latest/reference/sagemaker/update-cluster-software.html\" rel=\"noopener noreferrer\" target=\"_blank\">UpdateClusterSoftware</a> with <code>ImageId</code>.</li> \n <li><strong>AMI versions and deprecation&nbsp;</strong>– The <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-release-public-ami.html\">public AMI releases page</a> talks about the public AMI versions and deprecation status. Customers are expected to monitor this page for AMI vulnerabilities and deprecation status and patch cluster with updated custom AMI.</li> \n</ul> \n<h2>Clean up</h2> \n<p>To clean up your resources to avoid incurring more charges, complete the following steps:</p> \n<ol> \n <li><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-cli-command-delete-cluster.html\" rel=\"noopener noreferrer\" target=\"_blank\">Delete your SageMaker HyperPod cluster</a>.</li> \n <li>If you created the networking stack from the <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US\" rel=\"noopener noreferrer\" target=\"_blank\">SageMaker HyperPod workshop</a>, delete the stack as well to clean up the virtual private cloud (VPC) resources and the FSx for Lustre volume.</li> \n</ol> \n<h2>Conclusion</h2> \n<p>In this post, we introduced three features in SageMaker HyperPod that enhance scalability and customizability for ML infrastructure. Continuous provisioning offers flexible resource provisioning to help you start training and deploying your models faster and manage your cluster more efficiently. With custom AMIs, you can align your ML environments with organizational security standards and software requirements. To learn more about these features, see:</p> \n<ul> \n <li><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-scaling-eks.html\">Continuous provisioning for enhanced cluster operations on Amazon EKS</a></li> \n <li><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/hyperpod-custom-ami-how-to.html\">Build a custom AMI</a></li> \n</ul> \n<p style=\"clear: both;\"></p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/mvincig.jpg\"><img alt=\"\" class=\"alignleft wp-image-114514 size-thumbnail\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/mvincig-100x133.jpg\" width=\"100\" /></a><strong>Mark Vinciguerra</strong> is an Associate Specialist Solutions Architect at Amazon Web Services (AWS) based in New York. He focuses on Generative AI training and inference, with the goal of helping customers architect, optimize, and scale their workloads across various AWS services. Prior to AWS, he went to Boston University and graduated with a degree in Computer Engineering. You can connect with him on <a href=\"https://www.linkedin.com/in/mark-vinciguerra/\">LinkedIn</a>.</p> \n<p style=\"clear: both;\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/anoopx.jpg\"><img alt=\"\" class=\"alignleft wp-image-114517 size-thumbnail\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/anoopx-100x133.jpg\" width=\"100\" /></a>Anoop Saha</strong> is a Sr GTM Specialist at Amazon Web Services (AWS) focusing on generative AI model training and inference. He partners with top frontier model builders, strategic customers, and AWS service teams to enable distributed training and inference at scale on AWS and lead joint GTM motions. Before AWS, Anoop held several leadership roles at startups and large corporations, primarily focusing on silicon and system architecture of AI infrastructure.</p> \n<p style=\"clear: both;\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/monidipa.jpg\"><img alt=\"\" class=\"alignleft size-thumbnail wp-image-114519\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/monidipa-100x133.jpg\" width=\"100\" /></a>Monidipa Chakraborty</strong> currently serves as a Senior Software Development Engineer at Amazon Web Services (AWS), specifically within the SageMaker HyperPod team. She is committed to assisting customers by designing and implementing robust and scalable systems that demonstrate operational excellence. Bringing nearly a decade of software development experience, Monidipa has contributed to various sectors within Amazon, including Video, Retail, Amazon Go, and AWS SageMaker.</p> \n<p style=\"clear: both;\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/nagpalar.jpg\"><img alt=\"\" class=\"alignleft size-thumbnail wp-image-114521\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/nagpalar-100x133.jpg\" width=\"100\" /></a>Arun Nagpal</strong> is a Sr Technical Account Manager &amp; Enterprise Support Lead at Amazon Web Services (AWS), specializing in driving generative AI and supporting startups through enterprise-wide cloud transformations. He focuses on adopting AI services within AWS and aligning technology strategies with business objectives to achieve impactful results.</p> \n<p style=\"clear: both;\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/ydaiming.jpg\"><img alt=\"\" class=\"alignleft size-thumbnail wp-image-114523\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/ydaiming-100x133.jpg\" width=\"100\" /></a>Daiming Yang</strong> is a technical leader at AWS, working on machine learning infrastructure that enables large-scale training and inference workloads. He has contributed to multiple AWS services and is proficient in various AWS technologies, with expertise in distributed systems, Kubernetes, and cloud-native architecture. Passionate about building reliable, customer-focused solutions, he specializes in transforming complex technical challenges into simple, robust systems that scale globally.</p> \n<p style=\"clear: both;\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/jhakunal.jpg\"><img alt=\"\" class=\"alignleft size-thumbnail wp-image-114518\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/jhakunal-100x133.jpg\" width=\"100\" /></a>Kunal Jha</strong> is a Principal Product Manager at AWS, where he focuses on building Amazon SageMaker HyperPod to enable scalable distributed training and fine-tuning of foundation models. In his spare time, Kunal enjoys skiing and exploring the Pacific Northwest. You can connect with him on <a href=\"https://www.linkedin.com/in/kunal-j/\">LinkedIn</a>.</p> \n<p style=\"clear: both;\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/saakla.jpg\"><img alt=\"\" class=\"alignleft size-thumbnail wp-image-114522\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/saakla-100x133.jpg\" width=\"100\" /></a>Sai Kiran Akula</strong> is an engineering leader at AWS, working on the HyperPod team focused on improving infrastructure for machine learning training/inference&nbsp;jobs. He has contributed to core AWS services like EC2, ECS, Fargate, and SageMaker partner AI apps. With a background in distributed systems, he focuses on building reliable and scalable solutions across teams.</p>"
        }
      ]
    },
    {
      "title": "Fine-tune OpenAI GPT-OSS models using Amazon SageMaker HyperPod recipes",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Fine-tune OpenAI GPT-OSS models using Amazon SageMaker HyperPod recipes"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/fine-tune-openai-gpt-oss-models-using-amazon-sagemaker-hyperpod-recipes/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/fine-tune-openai-gpt-oss-models-using-amazon-sagemaker-hyperpod-recipes/",
      "authors": [
        {
          "name": "Durga Sury"
        }
      ],
      "author": "Durga Sury",
      "author_detail": {
        "name": "Durga Sury"
      },
      "published": "Thu, 21 Aug 2025 21:35:59 +0000",
      "published_parsed": [
        2025,
        8,
        21,
        21,
        35,
        59,
        3,
        233,
        0
      ],
      "tags": [
        {
          "term": "Amazon SageMaker",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "Announcements",
          "scheme": null,
          "label": null
        },
        {
          "term": "Artificial Intelligence",
          "scheme": null,
          "label": null
        },
        {
          "term": "Foundation models",
          "scheme": null,
          "label": null
        }
      ],
      "id": "952309d6f2e51c14cfd4de4f1b721630f95ca3fc",
      "guidislink": false,
      "summary": "This post is the second part of the GPT-OSS series focusing on model customization with Amazon SageMaker AI. In Part 1, we demonstrated fine-tuning GPT-OSS models using open source Hugging Face libraries with SageMaker training jobs, which supports distributed multi-GPU and multi-node configurations, so you can spin up high-performance clusters on demand. In this post, […]",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "This post is the second part of the GPT-OSS series focusing on model customization with Amazon SageMaker AI. In Part 1, we demonstrated fine-tuning GPT-OSS models using open source Hugging Face libraries with SageMaker training jobs, which supports distributed multi-GPU and multi-node configurations, so you can spin up high-performance clusters on demand. In this post, […]"
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>This post is the second part of the GPT-OSS series focusing on model customization with <a href=\"https://aws.amazon.com/sagemaker/ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker AI</a>. In <a href=\"https://aws.amazon.com/blogs/machine-learning/fine-tune-openai-gpt-oss-models-on-amazon-sagemaker-ai-using-hugging-face-libraries/\" rel=\"noopener noreferrer\" target=\"_blank\">Part 1</a>, we demonstrated fine-tuning GPT-OSS models using open source Hugging Face libraries with SageMaker training jobs, which supports distributed multi-GPU and multi-node configurations, so you can spin up high-performance clusters on demand.</p> \n<p>In this post, we show how you can fine-tune GPT OSS models on using recipes on <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-recipes.html\" rel=\"noopener noreferrer\" target=\"_blank\">SageMaker HyperPod and Training Jobs</a>. SageMaker HyperPod recipes help you get started with training and fine-tuning popular publicly available foundation models (FMs) such as Meta’s Llama, Mistral, and DeepSeek in just minutes, using either SageMaker HyperPod or training jobs. The recipes provide pre-built, validated configurations that alleviate the complexity of setting up distributed training environments while maintaining enterprise-grade performance and scalability for models. We outline steps to fine-tune the GPT-OSS model on a multilingual reasoning dataset, <a href=\"https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking\" rel=\"noopener noreferrer\" target=\"_blank\">HuggingFaceH4/Multilingual-Thinking</a>, so GPT-OSS can handle structured, chain-of-thought (CoT) reasoning across multiple languages.</p> \n<h2>Solution overview</h2> \n<p>This solution uses SageMaker HyperPod recipes to run a fine-tuning job on HyperPod using <a href=\"https://aws.amazon.com/eks/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Elastic Kubernetes Service</a> (Amazon EKS) orchestration or training jobs. Recipes are processed through the SageMaker HyperPod recipe launcher, which serves as the orchestration layer responsible for launching a job on the corresponding architecture such as SageMaker HyperPod (Slurm or Amazon EKS) or training jobs. To learn more, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-recipes.html\" rel=\"noopener noreferrer\" target=\"_blank\">SageMaker HyperPod recipes</a>.</p> \n<p>For details on fine-tuning the GPT-OSS model, see <a href=\"https://aws.amazon.com/blogs/machine-learning/fine-tune-openai-gpt-oss-models-on-amazon-sagemaker-ai-using-hugging-face-libraries/\" rel=\"noopener noreferrer\" target=\"_blank\">Fine-tune OpenAI GPT-OSS models on Amazon SageMaker AI using Hugging Face libraries</a>.</p> \n<p>In the following sections, we discuss the prerequisites for both options, and then move on to the data preparation. The prepared data is saved to <a href=\"https://aws.amazon.com/fsx/lustre/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon FSx for Lustre</a>, which is used as the persistent file system for SageMaker HyperPod, or <a href=\"https://aws.amazon.com/s3/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3) for training jobs. We then use recipes to submit the fine-tuning job, and finally deploy the trained model to a SageMaker endpoint for testing and evaluating the model. The following diagram illustrates this architecture.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-115023\" height=\"441\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/21/ML-1609-1.jpg\" width=\"1155\" /></p> \n<h2>Prerequisites</h2> \n<p>To follow along, you must have the following prerequisites:</p> \n<ul> \n <li>A local development environment with AWS credentials configured for creating and accessing SageMaker resources, or a remote environment such as <a href=\"https://aws.amazon.com/sagemaker/ai/studio/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker Studio</a>.</li> \n <li>For SageMaker HyperPod fine-tuning, complete the following: \n  <ul> \n   <li>Make sure you have one ml.p5.48xlarge instance (with 8 x NVIDIA H100 GPUs) for cluster usage. If you don’t have sufficient limits, request the following SageMaker quotas on the <a href=\"https://docs.aws.amazon.com/servicequotas/latest/userguide/intro.html\" rel=\"noopener noreferrer\" target=\"_blank\">Service Quotas</a> console: P5 instance (ml.p5.48xlarge) for HyperPod clusters (ml.p5.48xlarge for cluster usage): 1.</li> \n   <li>Set up a SageMaker HyperPod cluster on Amazon EKS. For instructions, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks.html\" rel=\"noopener noreferrer\" target=\"_blank\">Orchestrating SageMaker HyperPod clusters with Amazon EKS</a>. Alternatively, you can use the <a href=\"https://aws.amazon.com/cloudformation/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS CloudFormation</a> template provided in the <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US/00-setup/00-workshop-infra-cfn\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon EKS Support in Amazon SageMaker HyperPod workshop</a> and follow the instructions to <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US/01-cluster\" rel=\"noopener noreferrer\" target=\"_blank\">set up a cluster</a> and a development environment to access and submit jobs to the cluster.</li> \n   <li>Set up an FSx for Lustre file system for saving and loading data and checkpoints. Refer to <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US/01-cluster/06-fsx-for-lustre\" rel=\"noopener noreferrer\" target=\"_blank\">Set Up an FSx for Lustre File System</a> to set up an FSx for Lustre volume and associate it with the cluster.</li> \n  </ul> </li> \n <li>For fine-tuning the model using SageMaker training jobs, you must have one ml.p5.48xlarge instance (with 8 x NVIDIA H100 GPUs) for training jobs usage. If you don’t have sufficient limits, request the following SageMaker quotas on the <a href=\"https://docs.aws.amazon.com/servicequotas/latest/userguide/intro.html\" rel=\"noopener noreferrer\" target=\"_blank\">Service Quotas</a> console: P5 instance (ml.p5.48xlarge) for training jobs (ml.p5.48xlarge for cluster usage): 1.</li> \n</ul> \n<p>It might take up to 24 hours for these limits to be approved. You can also use SageMaker training plans to reserve these instances for a specific timeframe and use case (cluster or training jobs usage). For more details, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/reserve-capacity-with-training-plans.html\" rel=\"noopener noreferrer\" target=\"_blank\">Reserve training plans for your training jobs or HyperPod clusters</a>.</p> \n<p>Next, use your preferred development environment to prepare the dataset for fine-tuning. You can find the full code in the <a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/tree/main/3_distributed_training/models/openai--gpt-oss\" rel=\"noopener noreferrer\" target=\"_blank\">Generative AI using Amazon SageMaker repository</a> on GitHub.</p> \n<h2>Data tokenization</h2> \n<p>We use the <a href=\"https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking\" rel=\"noopener noreferrer\" target=\"_blank\">Hugging FaceH4/Multilingual-Thinking</a> dataset, which is a multilingual reasoning dataset containing CoT examples translated into languages such as French, Spanish, and German. The recipe supports a sequence length of 4,000 tokens for the GPT-OSS 120B model. The following example code demonstrates how to tokenize the multilingual-thinking dataset. The recipe accepts data in Hugging Face format (arrow). After it’s tokenized, you can save the processed dataset to disk.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from datasets import load_dataset\n \nfrom transformers import AutoTokenizer\nimport numpy as np\n \ndataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n \ntokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-120b\")\nmessages = dataset[0][\"messages\"]\nconversation = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(conversation)\n \ndef preprocess_function(example):\n&nbsp;&nbsp;  return tokenizer.apply_chat_template(example['messages'], \n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return_dict=True, \n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;padding=\"max_length\", \n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_length=4096, \n&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;truncation=True)\n \ndef label(x):\n&nbsp;&nbsp;  x[\"labels\"]=np.array(x[\"input_ids\"])\n&nbsp;&nbsp;  x[\"labels\"][x[\"labels\"]==tokenizer.pad_token_id]=-100\n&nbsp;&nbsp;  x[\"labels\"]=x[\"labels\"].tolist()\n&nbsp;&nbsp;  return x\n&nbsp;\ndataset = dataset.map(preprocess_function, \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; remove_columns=['reasoning_language', \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'developer', \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'user', \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'analysis', \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'final',\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'messages'])\ndataset = dataset.map(label)\n\n# for HyperPod, save to mounted FSx volume\ndataset.save_to_disk(\"/fsx/multilingual_4096\")\n\n# for training jobs, save to S3\ndataset.save_to_disk(\"multilingual_4096\")\n\ndef upload_directory(local_dir, bucket_name, s3_prefix=''):\n&nbsp;&nbsp; &nbsp;s3_client = boto3.client('s3')\n&nbsp;&nbsp; &nbsp;\n&nbsp;&nbsp; &nbsp;for root, dirs, files in os.walk(local_dir):\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;for file in files:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;local_path = os.path.join(root, file)\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Calculate relative path for S3\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;relative_path = os.path.relpath(local_path, local_dir)\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;s3_path = os.path.join(s3_prefix, relative_path).replace(\"\\\\\", \"/\")\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(f\"Uploading {local_path} to {s3_path}\")\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;s3_client.upload_file(local_path, bucket_name, s3_path)\n\nupload_directory('./multilingual_4096/', &lt;your-bucket&gt;, 'multilingual_4096')</code></pre> \n</div> \n<p>Now that you have prepared and tokenized the dataset, you can fine-tune the GPT-OSS model on your dataset, using either SageMaker HyperPod or training jobs. SageMaker training jobs are ideal for one-off or periodic training workloads that need temporary compute resources, making it a fully managed, on-demand experience for your training needs. SageMaker HyperPod is optimal for continuous development and experimentation, providing a persistent, preconfigured, and failure-resilient cluster. Depending on your choice, skip to the appropriate section for next steps.</p> \n<h2>Fine-tune the model using SageMaker HyperPod</h2> \n<p>To fine-tune the model using HyperPod, start by setting up the virtual environment and installing the necessary dependencies to execute the training job on the EKS cluster. Make sure the cluster is <code>InService</code> before proceeding, and you’re using Python 3.9 or greater in your development environment.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">python3 -m venv ${PWD}/venv\nsource&nbsp;venv/bin/activate</code></pre> \n</div> \n<p>Next, download and set up the SageMaker HyperPod recipes repository:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">git&nbsp;clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git\ncd&nbsp;sagemaker-hyperpod-recipes\npip3 install&nbsp;-r requirements.txt&nbsp;</code></pre> \n</div> \n<p>You can now use the SageMaker HyperPod recipe launch scripts to submit your training job. Using the recipe involves updating the <code>k8s.yaml</code> configuration file and executing the launch script.</p> \n<p>In <code>recipes_collection/cluster/k8s.yaml</code>, update the <code>persistent_volume_claims</code> section. It mounts the FSx claim to the <code>/fsx</code> directory of each computing pod:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">- claimName: fsx-claim&nbsp;&nbsp; &nbsp;\n&nbsp; mountPath:&nbsp;fsx</code></pre> \n</div> \n<p>SageMaker HyperPod recipes provide a launch script for each recipe within the <code>launcher_scripts</code> directory. To fine-tune the GPT-OSS-120B model, update the launch scripts located at <code>launcher_scripts/gpt_oss/run_hf_gpt_oss_120b_seq4k_gpu_lora.sh</code> and update the <code>cluster_type</code> parameter.</p> \n<p>The updated launch script should look similar to the following code when running SageMaker HyperPod with Amazon EKS. Make sure that <code>cluster=k8s</code> and <code>cluster_type=k8s</code> are updated in the launch script:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-shell\">#!/bin/bash\n\n# Original Copyright (c), NVIDIA CORPORATION. Modifications © Amazon.com\n\n#Users should setup their cluster type in /recipes_collection/config.yaml\n\nSAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-\"$(pwd)\"}\n\nHF_MODEL_NAME_OR_PATH=\"openai/gpt-oss-120b\" # HuggingFace pretrained model name or path\n\nTRAIN_DIR=\"/fsx/multilingual_4096\" # Location of training dataset\nVAL_DIR=\"/fsx/multilingual_4096\" # Location of validation dataset\n\nEXP_DIR=\"/fsx/experiment\" # Location to save experiment info including logging, checkpoints, ect\nHF_ACCESS_TOKEN=\"hf_xxxxxxxx\" # Optional HuggingFace access token\n\nHYDRA_FULL_ERROR=1 python3 \"${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py\" \\\n&nbsp;&nbsp; &nbsp;recipes=fine-tuning/gpt_oss/hf_gpt_oss_120b_seq4k_gpu_lora \\\n&nbsp;&nbsp; &nbsp;container=\"658645717510.dkr.ecr.us-west-2.amazonaws.com/smdistributed-modelparallel:sm-pytorch_gpt_oss_patch_pt-2.7_cuda12.8\" \\\n&nbsp;&nbsp; &nbsp;base_results_dir=\"${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results\" \\\n&nbsp;&nbsp; &nbsp;recipes.run.name=\"hf-gpt-oss-120b-lora\" \\\n\t<strong>cluster=k8s \\&nbsp;# Imp: add cluster line when running on HP EKS</strong>\n\t<strong>cluster_type=k8s \\&nbsp;# Imp: add cluster_type line when running on HP EKS</strong>\n&nbsp;&nbsp; &nbsp;recipes.exp_manager.exp_dir=\"$EXP_DIR\" \\\n&nbsp;&nbsp; &nbsp;recipes.trainer.num_nodes=1 \\\n&nbsp;&nbsp; &nbsp;recipes.model.data.train_dir=\"$TRAIN_DIR\" \\\n&nbsp;&nbsp; &nbsp;recipes.model.data.val_dir=\"$VAL_DIR\" \\\n&nbsp;&nbsp; &nbsp;recipes.model.hf_model_name_or_path=\"$HF_MODEL_NAME_OR_PATH\" \\\n&nbsp;&nbsp; &nbsp;recipes.model.hf_access_token=\"$HF_ACCESS_TOKEN\" \\</code></pre> \n</div> \n<p>When the script is ready, you can launch fine-tuning of the GPT OSS 120B model using the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-shell\">chmod +x launcher_scripts/gpt_oss/run_hf_gpt_oss_120b_seq4k_gpu_lora.sh \nbash launcher_scripts/gpt_oss/run_hf_gpt_oss_120b_seq4k_gpu_lora.sh</code></pre> \n</div> \n<p>After submitting a job for fine-tuning, you can use the following command to verify successful submission. You should be able to see the pods running in your cluster:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">kubectl get pods\nNAME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY&nbsp; STATUS&nbsp; &nbsp;RESTARTS&nbsp; &nbsp;AGE\nhf-gpt-oss-120b-lora-h2cwd-worker-0 1/1&nbsp; &nbsp;&nbsp;Running&nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;14m</code></pre> \n</div> \n<p>To check logs for the job, you can use the <code>kubectl logs</code> command:</p> \n<p><code>kubectl logs -f hf-gpt-oss-120b-lora-h2cwd-worker-0</code></p> \n<p>You should be able to see the following logs when the training begins and completes. You will find the checkpoints written to the <code>/fsx/experiment/checkpoints</code> folder.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">warnings.warn(\n&nbsp;&nbsp; &nbsp;\nEpoch 0: &nbsp;40%|████ &nbsp; &nbsp; &nbsp;| 50/125 [08:47&lt;13:10, &nbsp;0.09it/s, Loss/train=0.254, Norms/grad_norm=0.128, LR/learning_rate=2.2e-6] [NeMo I 2025-08-18 17:49:48 nemo_logging:381] save SageMakerCheckpointType.PEFT_FULL checkpoint: /fsx/experiment/checkpoints/peft_full/steps_50\n[NeMo I 2025-08-18 17:49:48 nemo_logging:381] Saving PEFT checkpoint to /fsx/experiment/checkpoints/peft_full/steps_50\n[NeMo I 2025-08-18 17:49:49 nemo_logging:381] Loading Base model from : openai/gpt-oss-120b\nYou are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\nLoading checkpoint shards: 100%|██████████| 15/15 [01:49&lt;00:00, &nbsp;7.33s/it]\n[NeMo I 2025-08-18 17:51:39 nemo_logging:381] Merging the adapter, this might take a while......\nUnloading and merging model: 100%|██████████| 547/547 [00:07&lt;00:00, 71.27it/s]\n[NeMo I 2025-08-18 17:51:47 nemo_logging:381] Checkpointing to /fsx/experiment/checkpoints/peft_full/steps_50/final-model......\n[NeMo I 2025-08-18 18:00:14 nemo_logging:381] Successfully save the merged model checkpoint.\n`Trainer.fit` stopped: `max_steps=50` reached.\nEpoch 0: &nbsp;40%|████ &nbsp; &nbsp; &nbsp;| 50/125 [23:09&lt;34:43, &nbsp;0.04it/s, Loss/train=0.264, Norms/grad_norm=0.137, LR/learning_rate=2e-6] &nbsp;</code></pre> \n</div> \n<p>When the training is complete, the final merged model can be found in the <code>experiment</code> directory path you defined in the launcher script under <code>/fsx/experiment/checkpoints/peft_full/steps_50/final-model</code>.</p> \n<h2>Fine-tune using SageMaker training jobs</h2> \n<p>You can also use recipes directly with SageMaker training jobs using the SageMaker Python SDK. The training jobs automatically spin up the compute, load the input data, run the training script, save the model to your output location, and tear down the instances, for a smooth training experience.</p> \n<p>The following code snippet shows how to use recipes with the PyTorch estimator. You can use the <code>training_recipe</code> parameter to specify the training or fine-tuning recipe to be used, and <code>recipe_overrides</code> for any parameters that need replacement. For training jobs, update the <code>input</code>, <code>output</code>, and <code>results</code> directories to locations in <code>/opt/ml</code> as required by SageMaker training jobs.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">import&nbsp;os\nimport&nbsp;sagemaker,boto3\nfrom&nbsp;sagemaker.pytorch import&nbsp;PyTorch\nfrom sagemaker.inputs import FileSystemInput\n\nsagemaker_session =&nbsp;sagemaker.Session()\nrole =&nbsp;sagemaker.get_execution_role()\nbucket =&nbsp;sagemaker_session.default_bucket()\noutput =&nbsp;os.path.join(f\"s3://{bucket}\", \"output\")\n\nrecipe_overrides = {\n&nbsp;&nbsp; &nbsp;\"run\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"results_dir\": \"/opt/ml/model\",\n&nbsp;&nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp;\"exp_manager\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"exp_dir\": \"\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"explicit_log_dir\": \"/opt/ml/output/tensorboard\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"checkpoint_dir\": \"/opt/ml/checkpoints\",\n&nbsp;&nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp;\"model\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"data\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"train_dir\": \"/opt/ml/input/data/train\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"val_dir\": \"/opt/ml/input/data/val\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp;\"use_smp_model\": \"False\",\n}\n\n\n# create the estimator object\nestimator = PyTorch(\n&nbsp;&nbsp;output_path=output,\n&nbsp;&nbsp;base_job_name=f\"gpt-oss-recipe\",\n&nbsp;&nbsp;role=role,\n&nbsp;&nbsp;instance_type=\"ml.p5.48xlarge\",\n&nbsp; <strong>training_recipe=\"fine-tuning/gpt_oss/hf_gpt_oss_120b_seq4k_gpu_lora\"</strong>,\n&nbsp;&nbsp;recipe_overrides=recipe_overrides,\n&nbsp;&nbsp;sagemaker_session=sagemaker_session,\n&nbsp;&nbsp;image_uri=\"658645717510.dkr.ecr.us-west-2.amazonaws.com/smdistributed-modelparallel:sm-pytorch_gpt_oss_patch_pt-2.7_cuda12.8\",\n)\n\n# submit the training job\nestimator.fit(\ninputs={\n\"train\": f\"s3://{bucket}/datasets/multilingual_4096/\", \n\"val\": f\"s3://{bucket}/datasets/multilingual_4096/\"}, wait=True)</code></pre> \n</div> \n<p>After the job is submitted, you can monitor the status of your training job on the SageMaker console, by choosing <strong>Training jobs </strong>under <strong>Training </strong>in the navigation pane. Choose the training job that starts with <code>gpt-oss-recipe</code> to view its details and logs. When the training job is complete, the outputs will be saved to an S3 location. You can get the location of the output artifacts from the <strong>S3 model artifact </strong>section on the job details page.</p> \n<h2>Run inference</h2> \n<p>After you fine-tune your GPT-OSS model with SageMaker recipes on either SageMaker training jobs or SageMaker HyperPod, the output is a customized model artifact that merges the base model with the customized PEFT adapters. This final model is stored in Amazon S3 and can be deployed directly from Amazon S3 to SageMaker endpoints for real-time inference.</p> \n<p>To serve GPT-OSS models, you must have the latest vLLM containers (v0.10.1 or later). A full list of <code>vllm-openai</code> Docker image versions is available on <a href=\"https://hub.docker.com/r/vllm/vllm-openai/tags\" rel=\"noopener noreferrer\" target=\"_blank\">Docker hub</a>.</p> \n<p>The steps to deploy your fine-tuned GPT-OSS model are outlined in this section.</p> \n<h3>Build the latest GPT-OSS container for your SageMaker endpoint</h3> \n<p>If you’re deploying the model from SageMaker Studio using JupyterLab or the Code Editor, both environments come with Docker preinstalled. Make sure that you’re using the SageMaker Distribution image v3.0 or later for compatibility.You can build your deployment container by running the following commands:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">%%bash # &lt;- use this if you're running this inside JupterLab cell\n\n# navigate to deploy dir from the current workdir, to build container\ncd ./deploy \n\n# build a push container\nchmod +X build.sh\nbash build.sh\n\ncd ..&nbsp;</code></pre> \n</div> \n<p>If you’re running these commands from a local terminal or other environment, simply omit the <code>%%bash</code> line and run the commands as standard shell commands.</p> \n<p>The <code>build.sh</code> script is responsible for automatically building and pushing a <code>vllm-openai</code> container that is optimized for SageMaker endpoints. After it’s built, the custom SageMaker endpoint compatible <code>vllm</code> image is pushed to <a href=\"https://aws.amazon.com/ecr/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Elastic Container Registry</a> (Amazon ECR). SageMaker endpoints can then pull this image from Amazon ECR at runtime to spin up the container for inference.</p> \n<p>The following is an example of the <code>build.sh</code> script:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-shell\">export REGION={region}\nexport ACCOUNT_ID={account_id}\nexport REPOSITORY_NAME=vllm\nexport TAG=v0.10.1\n\nfull_name=\"${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${REPOSITORY_NAME}:${TAG}\"\n\necho \"building $full_name\"\n\nDOCKER_BUILDKIT=0 docker build . --network sagemaker --tag $full_name --file Dockerfile\n\naws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n\n# If the repository doesn't exist in ECR, create it.\naws ecr describe-repositories --region ${REGION} --repository-names \"${REPOSITIRY_NAME}\" &gt; /dev/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n&nbsp;&nbsp; &nbsp;aws ecr create-repository --region ${REGION} --repository-name \"${REPOSITORY_NAME}\" &gt; /dev/null\nfi\n\ndocker tag $REPOSITORY_NAME:$TAG ${full_name}\ndocker push ${full_name}</code></pre> \n</div> \n<p>The Dockerfile defines how we convert an open source vLLM Docker image into a SageMaker hosting-compatible image. This involves extending the base <code>vllm-openai</code> image, adding the <code>serve</code> entrypoint script, and making it executable. See the following example Dockerfile:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">FROM vllm/vllm-openai:v0.10.1\n\nCOPY serve /usr/bin/serve\nRUN chmod 777 /usr/bin/serve\n\nENTRYPOINT [ \"/usr/bin/serve\" ]</code></pre> \n</div> \n<p>The <code>serve</code> script acts as a translation layer between SageMaker hosting conventions and the vLLM runtime. You can maintain the same deployment workflow you’re familiar with when hosting models on SageMaker endpoints, while automatically converting SageMaker-specific configurations into the format expected by vLLM.</p> \n<p>Key points to note about this script:</p> \n<ul> \n <li>It enforces the use of port 8080, which SageMaker requires for inference containers</li> \n <li>It dynamically translates environment variables prefixed with <code>OPTION_</code> into CLI arguments for vLLM (for example, <code>OPTION_MAX_MODEL_LEN=4096</code> changes to <code>--max-model-len 4096</code>)</li> \n <li>It prints the final set of arguments for visibility</li> \n <li>It finally launches the vLLM API server with the translated arguments</li> \n</ul> \n<p>The following is an example <code>serve</code> script:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-shell\">#!/bin/bash\n\n# Define the prefix for environment variables to look for\nPREFIX=\"OPTION_\"\nARG_PREFIX=\"--\"\n\n# Initialize an array for storing the arguments\n# port 8080 required by sagemaker, https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-container-response\nARGS=(--port 8080)\n\n# Loop through all environment variables\nwhile IFS='=' read -r key value; do\n&nbsp;&nbsp; &nbsp;# Remove the prefix from the key, convert to lowercase, and replace underscores with dashes\n&nbsp;&nbsp; &nbsp;arg_name=$(echo \"${key#\"${PREFIX}\"}\" | tr '[:upper:]' '[:lower:]' | tr '_' '-')\n\n&nbsp;&nbsp; &nbsp;# Add the argument name and value to the ARGS array\n&nbsp;&nbsp; &nbsp;ARGS+=(\"${ARG_PREFIX}${arg_name}\")\n&nbsp;&nbsp; &nbsp;if [ -n \"$value\" ]; then\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ARGS+=(\"$value\")\n&nbsp;&nbsp; &nbsp;fi\ndone &lt; &lt;(env | grep \"^${PREFIX}\")\n\necho \"-------------------------------------------------------------------\"\necho \"vLLM engine args: [${ARGS[@]}]\"\necho \"-------------------------------------------------------------------\"\n\n# Pass the collected arguments to the main entrypoint\nexec python3 -m vllm.entrypoints.openai.api_server \"${ARGS[@]}\"</code></pre> \n</div> \n<h3>Host customized GPT-OSS as a SageMaker real-time endpoint</h3> \n<p>Now you can deploy your fine-tuned GPT-OSS model using the ECR image URI you built in the previous step. In this example, the model artifacts are stored securely in an S3 bucket, and SageMaker will download them into the container at runtime.Complete the following configurations:</p> \n<ul> \n <li>Set <code>model_data</code> to point to the S3 prefix where your model artifacts are located</li> \n <li>Set the <code>OPTION_MODEL</code> environment variable to <code>/opt/ml/model</code>, which is where SageMaker mounts the model inside the container</li> \n <li>(Optional) If you’re serving a model from Hugging Face Hub instead of Amazon S3, you can set <code>OPTION_MODEL</code> directly to the Hugging Face model ID instead</li> \n</ul> \n<p>The endpoint startup might take several minutes as the model artifacts are downloaded and the container is initialized.The following is an example deployment code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-javascript\">inference_image = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/vllm:v0.10.1\"\n\n...\n...\n\nlmi_model = sagemaker.Model(\n&nbsp;&nbsp; &nbsp;image_uri=inference_image,\n&nbsp;&nbsp; &nbsp;env={\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"OPTION_MODEL\": \"/opt/ml/model\",&nbsp;# set this to let SM endpoint read a model stored in s3, else set it to HF MODEL ID\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"OPTION_SERVED_MODEL_NAME\": \"model\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"OPTION_TENSOR_PARALLEL_SIZE\": json.dumps(num_gpus),\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"OPTION_DTYPE\": \"bfloat16\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;#\"VLLM_ATTENTION_BACKEND\": \"TRITON_ATTN_VLLM_V1\", # not required for vLLM 0.10.1 and above\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"OPTION_ASYNC_SCHEDULING\": \"true\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"OPTION_QUANTIZATION\": \"mxfp4\"\n&nbsp;&nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp;role=role,\n&nbsp;&nbsp; &nbsp;name=model_name,\n&nbsp;&nbsp; &nbsp;model_data={\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'S3DataSource': {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'S3Uri': \"s3://path/to/gpt-oss/model/artifacts\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'S3DataType': 'S3Prefix',\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'CompressionType': 'None'\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp;},\n)\n\n...\n\nlmi_model.deploy(\n&nbsp;&nbsp; &nbsp;initial_instance_count=1,\n&nbsp;&nbsp; &nbsp;instance_type=instance_type,\n&nbsp;&nbsp; &nbsp;container_startup_health_check_timeout=600,\n&nbsp;&nbsp; &nbsp;endpoint_name=endpoint_name,\n&nbsp;&nbsp; &nbsp;endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n&nbsp;&nbsp; &nbsp;inference_component_name=inference_component_name,\n&nbsp;&nbsp; &nbsp;resources=ResourceRequirements(requests={\"num_accelerators\": 1, \"memory\": 1024*3, \"copies\": 1,}),\n)</code></pre> \n</div> \n<h3>Sample inference</h3> \n<p>After your endpoint is deployed and in the <code>InService</code> state, you can invoke your fine-tuned GPT-OSS model using the SageMaker Python SDK.</p> \n<p>The following is an example predictor setup:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">pretrained_predictor = sagemaker.Predictor(\n&nbsp;&nbsp; &nbsp;endpoint_name=endpoint_name,\n&nbsp;&nbsp; &nbsp;sagemaker_session=sagemaker.Session(boto3.Session(region_name=boto3.Session().region_name)),\n&nbsp;&nbsp; &nbsp;serializer=serializers.JSONSerializer(),\n&nbsp;&nbsp; &nbsp;deserializer=deserializers.JSONDeserializer(),\n&nbsp;&nbsp; &nbsp;component_name=inference_component_name\n)</code></pre> \n</div> \n<p>The modified vLLM container is fully compatible with the OpenAI-style <code>messages</code> input format, making it straightforward to send chat-style requests:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">payload = {\n&nbsp;&nbsp; &nbsp;\"messages\": [{\"role\": \"user\", \"content\": \"Hello who are you?\"}],\n&nbsp;&nbsp; &nbsp;\"parameters\": {\"max_new_tokens\": 64, \"temperature\": 0.2}\n}\n\noutput = pretrained_predictor.predict(payload)</code></pre> \n</div> \n<p>You have successfully deployed and invoked your custom fine-tuned GPT-OSS model on SageMaker real-time endpoints, using the vLLM framework for optimized, low-latency inference. You can find more GPT-OSS hosting examples in the <a href=\"https://github.com/aws-samples/sagemaker-genai-hosting-examples/tree/main/OpenAI/gpt-oss\" rel=\"noopener noreferrer\" target=\"_blank\">OpenAI gpt-oss examples GitHub repo</a>.</p> \n<h2>Clean up</h2> \n<p>To avoid incurring additional charges, complete the following steps to clean up the resources used in this post:</p> \n<ol> \n <li>Delete the SageMaker endpoint:</li> \n</ol> \n<p><code>pretrained_predictor.delete_endpoint()</code></p> \n<ol start=\"2\"> \n <li>If you created a SageMaker HyperPod cluster for the purposes of this post, delete the cluster by following the instructions in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-cli-command-delete-cluster.html\" rel=\"noopener noreferrer\" target=\"_blank\">Deleting a SageMaker HyperPod cluster</a>.</li> \n <li>Clean up the FSx for Lustre volume if it’s no longer needed by following instructions in <a href=\"https://docs.aws.amazon.com/fsx/latest/LustreGuide/delete-file-system.html\" rel=\"noopener noreferrer\" target=\"_blank\">Deleting a file system</a>.</li> \n <li>If you used training jobs, the training instances are automatically deleted when the jobs are complete.</li> \n</ol> \n<h2>Conclusion</h2> \n<p>In this post, we showed how to fine-tune OpenAI’s GPT-OSS models (<code>gpt-oss-120b</code> and <code>gpt-oss-20b</code>) on SageMaker AI using SageMaker HyperPod recipes. We discussed how SageMaker HyperPod recipes provide a powerful yet accessible solution for organizations to scale their AI model training capabilities with <a href=\"https://aws.amazon.com/what-is/large-language-model/\" rel=\"noopener noreferrer\" target=\"_blank\">large language models</a> (LLMs) including GPT-OSS, using either a persistent cluster through SageMaker HyperPod, or an ephemeral cluster using SageMaker training jobs. The architecture streamlines complex distributed training workflows through its intuitive recipe-based approach, reducing setup time from weeks to minutes. We also showed how these fine-tuned models can be seamlessly deployed to production using SageMaker endpoints with vLLM optimization, providing enterprise-grade inference capabilities with OpenAI-compatible APIs. This end-to-end workflow, from training to deployment, helps organizations build and serve custom LLM solutions while using the scalable infrastructure of AWS and comprehensive ML platform capabilities of SageMaker.</p> \n<p>To begin using the SageMaker HyperPod recipes, visit the <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker HyperPod recipes GitHub repo</a> for comprehensive documentation and example implementations. If you’re interested in exploring the fine-tuning further, the <a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai\" rel=\"noopener noreferrer\" target=\"_blank\">Generative AI using Amazon SageMaker GitHub repo</a> has the necessary code and notebooks. Our team continues to expand the recipe ecosystem based on customer feedback and emerging ML trends, making sure that you have the tools needed for successful AI model training.</p> \n<p><em>Special thanks to everyone who contributed to the launch: Hengzhi Pei, Zach Kimberg, Andrew Tian, Leonard Lausen, Sanjay Dorairaj, Manish Agarwal, Sareeta Panda, Chang Ning Tsai, Maxwell Nuyens, Natasha Sivananjaiah, and Kanwaljit Khurmi.</em></p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><strong><img alt=\"author-surydurg\" class=\"alignleft size-thumbnail wp-image-109227\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/19/surydurg-100x100.jpg\" width=\"100\" />Durga Sury</strong>&nbsp;is a Senior Solutions Architect at Amazon SageMaker, where she helps enterprise customers build secure and scalable AI/ML systems. When she’s not architecting solutions, you can find her enjoying sunny walks with her dog, immersing herself in murder mystery books, or catching up on her favorite Netflix shows.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-full wp-image-115032\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/21/Pranav-Profile-100.jpeg\" width=\"100\" />Pranav Murthy</strong>&nbsp;is a Senior Generative AI Data Scientist at AWS, specializing in helping organizations innovate with Generative AI, Deep Learning, and Machine Learning on Amazon SageMaker AI. Over the past 10+ years, he has developed and scaled advanced computer vision (CV) and natural language processing (NLP) models to tackle high-impact problems—from optimizing global supply chains to enabling real-time video analytics and multilingual search. When he’s not building AI solutions, Pranav enjoys playing strategic games like chess, traveling to discover new cultures, and mentoring aspiring AI practitioners.&nbsp;You can find Pranav on&nbsp;<a href=\"https://www.linkedin.com/in/pranav-murthy-6bbb5773/\" rel=\"noopener\" target=\"_blank\">LinkedIn</a>.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-full wp-image-115031\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/21/Sumedha-Swamy.jpg\" width=\"100\" />Sumedha Swamy</strong>&nbsp;is a Senior Manager of Product Management at Amazon Web Services (AWS), where he leads several areas of the Amazon SageMaker, including SageMaker Studio – the industry-leading integrated development environment for machine learning, developer and administrator experiences, AI infrastructure, and SageMaker SDK.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-full wp-image-115029\" height=\"103\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/21/DmitrySoldatkin.jpg\" width=\"100\" />Dmitry Soldatkin</strong>&nbsp;is a Senior AI/ML Solutions Architect at Amazon Web Services (AWS), helping customers design and build AI/ML solutions. Dmitry’s work covers a wide range of ML use cases, with a primary interest in Generative AI, deep learning, and scaling ML across the enterprise. He has helped companies in many industries, including insurance, financial services, utilities, and telecommunications. You can connect with Dmitry on&nbsp;<a href=\"https://www.linkedin.com/in/dmitry-soldatkin/\" rel=\"noopener\" target=\"_blank\">LinkedIn</a>.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-full wp-image-115030\" height=\"108\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/21/arunkumar-Lokh.png\" width=\"100\" />Arun Kumar Lokanatha </strong>is a Senior ML Solutions Architect with the Amazon SageMaker team. He specializes in large language model training workloads, helping customers build LLM workloads using SageMaker HyperPod, SageMaker training jobs, and SageMaker distributed training. Outside of work, he enjoys running, hiking, and cooking.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-full wp-image-115028\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/21/aniviswa.jpg\" width=\"100\" />Anirudh Viswanathan</strong> is a Senior Product Manager, Technical, at AWS with the SageMaker team, where he focuses on Machine Learning. He holds a Master’s in Robotics from Carnegie Mellon University and an MBA from the Wharton School of Business. Anirudh is a named inventor on more than 50 AI/ML patents. He enjoys long-distance running, exploring art galleries, and attending Broadway shows.</p>"
        }
      ]
    },
    {
      "title": "Inline code nodes now supported in Amazon Bedrock Flows in public preview",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Inline code nodes now supported in Amazon Bedrock Flows in public preview"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/inline-code-nodes-now-supported-in-amazon-bedrock-flows-in-public-preview/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/inline-code-nodes-now-supported-in-amazon-bedrock-flows-in-public-preview/",
      "authors": [
        {
          "name": "Shubhankar Sumar"
        }
      ],
      "author": "Shubhankar Sumar",
      "author_detail": {
        "name": "Shubhankar Sumar"
      },
      "published": "Thu, 21 Aug 2025 20:36:40 +0000",
      "published_parsed": [
        2025,
        8,
        21,
        20,
        36,
        40,
        3,
        233,
        0
      ],
      "tags": [
        {
          "term": "Amazon Bedrock Prompt Flows",
          "scheme": null,
          "label": null
        },
        {
          "term": "Announcements",
          "scheme": null,
          "label": null
        }
      ],
      "id": "8f4a869aa6d3799ecde15169219601372e7bf153",
      "guidislink": false,
      "summary": "We are excited to announce the public preview of support for inline code nodes in Amazon Bedrock Flows. With this powerful new capability, you can write Python scripts directly within your workflow, alleviating the need for separate AWS Lambda functions for simple logic. This feature streamlines preprocessing and postprocessing tasks (like data normalization and response formatting), simplifying generative AI application development and making it more accessible across organizations.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "We are excited to announce the public preview of support for inline code nodes in Amazon Bedrock Flows. With this powerful new capability, you can write Python scripts directly within your workflow, alleviating the need for separate AWS Lambda functions for simple logic. This feature streamlines preprocessing and postprocessing tasks (like data normalization and response formatting), simplifying generative AI application development and making it more accessible across organizations."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>Today, we are excited to announce the public preview of support for <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/flows-nodes.html#flows-nodes-data\" rel=\"noopener noreferrer\" target=\"_blank\">inline code nodes</a> in <a href=\"https://aws.amazon.com/bedrock/flows/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Flows</a>. With this powerful new capability, you can write Python scripts directly within your workflow, alleviating the need for separate <a href=\"http://aws.amazon.com/lambda\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Lambda</a> functions for simple logic. This feature streamlines preprocessing and postprocessing tasks (like data normalization and response formatting), simplifying generative AI application development and making it more accessible across organizations. By removing adoption barriers and reducing maintenance overhead, the inline code feature accelerates enterprise adoption of generative AI solutions, resulting in faster iteration cycles and broader participation in AI application building.</p> \n<p>Organizations using Amazon Bedrock Flows now can use inline code nodes to design and deploy workflows for building more scalable and efficient generative AI applications fully within the Amazon Bedrock environment while achieving the following:</p> \n<ul> \n <li><strong>Preprocessing</strong> – Transforming input data before sending it to a large language model (LLM) without having to set up a separate Lambda function. For example, extracting specific fields from JSON, formatting text data, or normalizing values.</li> \n <li><strong>Postprocessing</strong> – Performing operations on model outputs directly within the flow. For example, extracting entities from responses, formatting JSON for downstream systems, or applying business rules to the results.</li> \n <li><strong>Complex use cases</strong> – Managing the execution of complex, multi-step generative AI workflows that can call popular packages like opencv, scipy, of pypdf.</li> \n <li><strong>Builder-friendly</strong> – Creating and managing inline code through both the <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock API</a> and the <a href=\"http://aws.amazon.com/console\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Management Console</a>.</li> \n <li><strong>Observability</strong> – Seamless user experience with the ability to trace the inputs and outputs from each node.</li> \n</ul> \n<p>In this post, we discuss the benefits of this new feature, and show how to use inline code nodes in Amazon Bedrock Flows.</p> \n<h2>Benefits of inline code in Amazon Bedrock Flows</h2> \n<p><a href=\"https://www.thomsonreuters.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Thomson Reuters</a>, a global information services company providing essential news, insights, and technology solutions to professionals across legal, tax, accounting, media, and corporate sectors, handles complex, multi-step generative AI use cases that require simple preprocessing and postprocessing as part of the workflow. With the inline code feature in Amazon Bedrock Flows, Thomson Reuters can now benefit from the following:</p> \n<ul> \n <li><strong>Simplified flow management</strong> – Alleviate the need to create and maintain individual Lambda functions for each custom code block, making it straightforward to manage thousands of workflows across a large user base (over 16,000 users and 6,000 chains) with less operational overhead.</li> \n <li><strong>Flexible data processing</strong> – Enable direct preprocessing of data before LLM calls and postprocessing of LLM responses, including the ability to interact with internal AWS services and third-party APIs through a single interface.</li> \n <li><strong>DIY flow creation</strong> – Help users build complex workflows with custom code blocks through a self-service interface, without exposing them to the underlying infrastructure complexities or requiring Lambda function management.</li> \n</ul> \n<h2>Solution overview</h2> \n<p>In the following sections, we show how to create a simple Amazon Bedrock flow and add inline code nodes. Our example showcases a practical application where we’ll construct a flow that processes user requests for music playlists, incorporating both preprocessing and postprocessing inline code nodes to handle data validation and response formatting.</p> \n<h2>Prerequisites</h2> \n<p>Before implementing the new capabilities, make sure you have the following:</p> \n<ul> \n <li>An AWS account</li> \n <li>Other Amazon Bedrock services in place: \n  <ul> \n   <li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/flows-create.html\" rel=\"noopener noreferrer\" target=\"_blank\">Create and test your base prompts</a> for customer service interactions in <a href=\"https://aws.amazon.com/es/bedrock/prompt-management/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Prompt Management</a></li> \n   <li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-components.html\" rel=\"noopener noreferrer\" target=\"_blank\">Create guardrails</a> with relevant rules using <a href=\"https://aws.amazon.com/bedrock/guardrails/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Guardrails</a></li> \n  </ul> </li> \n <li>Resources in auxiliary AWS services needed for your workflow, such as <a href=\"https://aws.amazon.com/dynamodb\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon DynamoDB</a>, <a href=\"https://docs.aws.amazon.com/s3\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3), and <a href=\"https://aws.amazon.com/sns/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Notification Service</a> (Amazon SNS)</li> \n <li>Required <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) permissions: \n  <ul> \n   <li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/flows-permissions.html\" rel=\"noopener noreferrer\" target=\"_blank\">Access to Amazon Bedrock Flows</a></li> \n   <li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-permissions.html\" rel=\"noopener noreferrer\" target=\"_blank\">Appropriate access to LLMs in Amazon Bedrock</a></li> \n  </ul> </li> \n</ul> \n<p>After these components are in place, you can proceed with using Amazon Bedrock Flows with inline code capabilities in your generative AI use case.</p> \n<h2>Create your flow using inline code nodes</h2> \n<p>Complete the following steps to create your flow:</p> \n<ol> \n <li>On the Amazon Bedrock console, choose <strong>Flows</strong> under <strong>Builder tools</strong> in the navigation pane.</li> \n <li>Create a new flow, for example, easy-inline-code-flow. For detailed instructions on creating a flow, see <a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-flows-is-now-generally-available-with-enhanced-safety-and-traceability/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Flows is now generally available with enhanced safety and traceability</a>.</li> \n <li>Add an inline code node. (For this example, we create two nodes for two separate prompts).</li> \n</ol> \n<p>Amazon Bedrock provides different <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/flows-nodes.html\" rel=\"noopener noreferrer\" target=\"_blank\">node types</a> to build your prompt flow. For this example, we use an inline code node instead of calling a Lambda function for custom code for a generative AI-powered application. There are two inline code nodes in the flow. We have extended the sample from the documentation <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/flows-ex-prompt.html\" rel=\"noopener noreferrer\" target=\"_blank\">Create a flow with a single prompt</a>. The new node type <strong>Inline Code</strong> is on the <strong>Nodes</strong> tab in the left pane.</p> \n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ml-19171-image-1.png\" style=\"margin: 10px 0px 10px 0px;\" /></p> \n<ol start=\"4\"> \n <li>Add some code to process in the <code>Preprocessing_InlineCode</code> node before sending it to the prompt node <code>prompt_1</code>. Python 3 is only supported at the time of writing. In this example, we check if the number of songs requested by the user is more than 10 and it’s set to 10.</li> \n</ol> \n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ml-19171-image-2.png\" style=\"margin: 10px 0px 10px 0px;\" /></p> \n<p>There is a Python code editor and sample code templates as well for writing the code.</p> \n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ml-19171-image-3.png\" style=\"margin: 10px 0px 10px 0px;\" /></p> \n<p>We use the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">import json\ndef __func():\n    try:\n        if userprompt['number'] &gt; 10:\n            userprompt['number']=10\n            return userprompt\n        else:\n            return userprompt\n            \n    except Exception as e:\n        return {\n            \"error\": \"Invalid input format\",\n            \"details\": str(e)\n        }\n__func()</code></pre> \n</div> \n<ol start=\"5\"> \n <li>In the Postprocessing_Inline Code node, we check the number of words in the response and feed the data to the next prompt node, <code>prompt_2</code>.</li> \n</ol> \n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ml-19171-image-4.png\" style=\"margin: 10px 0px 10px 0px;\" /></p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">def __func():\n    # Remove extra whitespace and count\n    cleaned_text = ' '.join(playlist.split())\n    word_count = len(cleaned_text.split())\n    return{\n        \"playlist\": playlist,     \"word_count\": word_count\n    }\n__func()</code></pre> \n</div> \n<ol start=\"6\"> \n <li>Test the flow with the following prompt:</li> \n</ol> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">Sample input for the Flow Input node \n{\n  \"genre\": \"pop\",\n    \"number\": 8\n  }</code></pre> \n</div> \n<p>Input to the inline code node (Python function) must be treated as untrusted user input, and appropriate parsing, validation, and data handling should be implemented.</p> \n<p>You can see the output as shown in the following screenshot. The system also provides access to node execution traces, offering detailed insights into each processing step, real-time performance metrics, and highlighting any issues that occurred during the flow’s execution. Traces can be enabled using an <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/flows-trace.html\" rel=\"noopener noreferrer\" target=\"_blank\">API</a> and sent to an <a href=\"http://aws.amazon.com/cloudwatch\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon CloudWatch</a> log. In the API, set the <code>enableTrace</code> field to true in an <code><a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_InvokeFlow.html\" rel=\"noopener noreferrer\" target=\"_blank\">InvokeFlow</a></code> request. Each <code>flowOutputEvent</code> in the response is returned alongside a <code>flowTraceEvent</code>.</p> \n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ml-19171-image-5.png\" style=\"margin: 10px 0px 10px 0px;\" /></p> \n<p>You have now successfully created and executed an Amazon Bedrock flows using inline code nodes. You can also use <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_StartFlowExecution.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock APIs</a> to programmatically execute this flow. For additional details on how to configure flows with enhanced safety and traceability, see <a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-flows-is-now-generally-available-with-enhanced-safety-and-traceability/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Flows is now generally available with enhanced safety and traceability</a>.</p> \n<h2>Considerations</h2> \n<p>When working with inline code nodes in Amazon Bedrock Flows, the following are the important things to note:</p> \n<ul> \n <li>Code is executed in an AWS managed, secured, sandbox environment that is not shared with anyone and doesn’t have internet access</li> \n <li>The feature supports Python 3.12 and above</li> \n <li>It efficiently handles code with binary size up to 4 MB, which is roughly 4 million characters</li> \n <li>It supports popular packages like opencv, scipy, and pypdf</li> \n <li>It supports 25 concurrent code execution sessions per AWS account</li> \n</ul> \n<h2>Conclusion</h2> \n<p>The integration of inline code nodes in Amazon Bedrock Flows marks a significant advancement in democratizing generative AI development, reducing the complexity of managing separate Lambda functions for basic processing tasks. This enhancement responds directly to enterprise customers’ needs for a more streamlined development experience, helping developers focus on building sophisticated AI workflows rather than managing infrastructure.</p> \n<p>Inline code in Amazon Bedrock Flows is now available in public preview in the following <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Regions</a>: US East (N. Virginia, Ohio), US West (Oregon) and Europe (Frankfurt). To get started, open the Amazon Bedrock console or Amazon Bedrock APIs to begin building flows with Amazon Bedrock Flows. To learn more, refer to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/flows-get-started.html\" rel=\"noopener noreferrer\" target=\"_blank\">Create your first flow in Amazon Bedrock</a> and <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/flows-trace.html\" rel=\"noopener noreferrer\" target=\"_blank\">Track each step in your flow by viewing its trace in Amazon Bedrock</a>.</p> \n<p>We’re excited to see the innovative applications you will build with these new capabilities. As always, we welcome your feedback through <a href=\"https://repost.aws/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS re:Post</a> for Amazon Bedrock or your usual AWS contacts. Join the generative AI builder community at <a href=\"https://community.aws/\" rel=\"noopener noreferrer\" target=\"_blank\">community.aws</a> to share your experiences and learn from others.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><img alt=\"Shubhankar Sumar\" class=\"wp-image-105760 size-full alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/05/02/sssumar-1.jpg\" title=\"Shubhankar Sumar\" width=\"100\" /><strong>Shubhankar Sumar</strong>&nbsp;is a Senior Solutions Architect at AWS, where he specializes in architecting generative AI-powered solutions for enterprise software and SaaS companies across the UK. With a strong background in software engineering, Shubhankar excels at designing secure, scalable, and cost-effective multi-tenant systems on the cloud. His expertise lies in seamlessly integrating cutting-edge generative AI capabilities into existing SaaS applications, helping customers stay at the forefront of technological innovation.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"alignleft wp-image-114386 size-thumbnail\" height=\"115\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/jrmander_Badgephoto-e1755166000536-100x115.jpeg\" title=\"Shubhankar Sumar\" width=\"100\" /><strong>Jesse Manders</strong> is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business.</p> \n<p style=\"clear: both;\"><img alt=\"Huong Nguyen\" class=\"wp-image-105760 size-full alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/19/Huuong.jpg\" width=\"100\" /><strong>Huong Nguyen</strong> is a Principal Product Manager at AWS. She is leading the Amazon Bedrock Flows, with 18 years of experience building customer-centric and data-driven products. She is passionate about democratizing responsible machine learning and generative AI to enable customer experience and business innovation. Outside of work, she enjoys spending time with family and friends, listening to audiobooks, traveling, and gardening.</p>"
        }
      ]
    },
    {
      "title": "Accelerate enterprise AI implementations with Amazon Q Business",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Accelerate enterprise AI implementations with Amazon Q Business"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/accelerate-enterprise-ai-implementations-with-amazon-q-business/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/accelerate-enterprise-ai-implementations-with-amazon-q-business/",
      "authors": [
        {
          "name": "Oliver Steffmann"
        }
      ],
      "author": "Oliver Steffmann",
      "author_detail": {
        "name": "Oliver Steffmann"
      },
      "published": "Thu, 21 Aug 2025 20:29:53 +0000",
      "published_parsed": [
        2025,
        8,
        21,
        20,
        29,
        53,
        3,
        233,
        0
      ],
      "tags": [
        {
          "term": "Amazon Q Business",
          "scheme": null,
          "label": null
        },
        {
          "term": "Artificial Intelligence",
          "scheme": null,
          "label": null
        },
        {
          "term": "Best Practices",
          "scheme": null,
          "label": null
        },
        {
          "term": "AI/ML",
          "scheme": null,
          "label": null
        },
        {
          "term": "Generative AI",
          "scheme": null,
          "label": null
        }
      ],
      "id": "c9b61ae9d4272f1788c2d447f6e4182c2fc59650",
      "guidislink": false,
      "summary": "Amazon Q Business offers AWS customers a scalable and comprehensive solution for enhancing business processes across their organization. By carefully evaluating your use cases, following implementation best practices, and using the architectural guidance provided in this post, you can deploy Amazon Q Business to transform your enterprise productivity. The key to success lies in starting small, proving value quickly, and scaling systematically across your organization.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Amazon Q Business offers AWS customers a scalable and comprehensive solution for enhancing business processes across their organization. By carefully evaluating your use cases, following implementation best practices, and using the architectural guidance provided in this post, you can deploy Amazon Q Business to transform your enterprise productivity. The key to success lies in starting small, proving value quickly, and scaling systematically across your organization."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>As an <a href=\"https://aws.amazon.com/\">Amazon Web Services (AWS)</a> enterprise customer, you’re probably exploring ways to use generative AI to enhance your business processes, improve customer experiences, and drive innovation.</p> \n<p>With a variety of options available—from <a href=\"https://aws.amazon.com/q/business/\">Amazon Q Business</a> to other AWS services or third-party offerings—choosing the right tool for your use case can be challenging. This post aims to guide you through the decision-making process and highlight the unique advantages of Amazon&nbsp;Q&nbsp;Business and how to build an AWS architecture to get started and onboard more use cases.</p> \n<p>Amazon Q Business is an AI-powered assistant that can help employees quickly find information, solve problems, and get work done across their company’s data and applications. With Amazon Q Business, employees can access information from various internal documents, websites, wikis, and other business resources through natural conversations, helping them to find exactly what they need without extensive searching. It can also be used to automate common workflows across enterprise systems. Amazon Q Business prioritizes security and privacy by operating within your organization’s existing permissions and access controls, helping to ensure that employees only see information that they’re authorized to access.</p> \n<h2>Understand your use case</h2> \n<p>The first step in selecting the right generative AI solution is to clearly define your use case. Are you looking to enhance a single system, or do you need a solution that spans multiple platforms? Single-system use cases might be well-served by specific generative AI solutions, while cross-system scenarios often benefit from a more unified approach.&nbsp;Organizations that benefit most from Amazon Q Business typically share several key characteristics:</p> \n<ul> \n <li><strong>Data complexity:</strong> Companies with large volumes of data spread across multiple repositories and formats (documents, images, audio, video)</li> \n <li><strong>Knowledge dependency:</strong> Organizations where employee productivity depends on accessing institutional knowledge quickly and accurately</li> \n <li><strong>Security requirements:</strong> Organizations with strict security and compliance needs requiring role-based permissions and access controls</li> \n <li><strong>Collaboration needs:</strong> Teams that need to share information and collaborate across departments and geographies</li> \n <li><strong>Process complexity:</strong> Organizations with complex workflows that could benefit from automation and streamlining</li> \n</ul> \n<h2>Key considerations for tool selection</h2> \n<p>When evaluating generative AI tools, there are several factors should you should consider to help ensure successful implementation and adoption:</p> \n<ul> \n <li><strong>Customization needs:</strong> Determine if you need custom AI behaviors or if out-of-the-box solutions suffice</li> \n <li><strong>Integration complexity:</strong> Assess the number of systems involved and the complexity of data flows between them</li> \n <li><strong>Future scalability:</strong> Think about your long-term needs and choose a solution that can grow with you</li> \n <li><strong>Data privacy and residency:</strong> Understand your data governance requirements and make sure that your chosen solution can meet them</li> \n <li><strong>Cost-effectiveness:</strong> Evaluate the total cost of ownership, including implementation, maintenance, and scaling costs</li> \n <li><strong>Time to market:</strong> Consider how quickly you need to implement your generative AI solution</li> \n <li><strong>Change management:</strong> As with any enterprise AI implementation, organizations must invest in proper training and change management strategies to help ensure adoption</li> \n</ul> \n<h2>The case for Amazon Q Business</h2> \n<p>Amazon Q Business offers unique advantages, especially for organizations that already have AWS services or that have complex, cross-system needs. For AWS enterprise customers that have the resources to build and operate their own solutions, an architecture that includes Amazon Q Business offers flexibility and cost advantages, including:</p> \n<ul> \n <li><strong>Unified experience:</strong> Amazon Q Business can provide a consistent AI experience across multiple systems, creating a seamless interface for users.</li> \n <li><strong>Architectural benefits:</strong> As a native AWS service, Amazon Q Business integrates seamlessly with your existing AWS architecture, reducing complexity and potential points of failure.</li> \n <li><strong>Flexibility:</strong> Amazon Q Business can connect to various enterprise systems, so that you can use it to create custom workflows that span multiple platforms.</li> \n <li><strong>Scalability:</strong> By using Amazon Q Business, you can take advantage of the proven scalability of AWS to handle growing workloads without worrying about infrastructure management.</li> \n <li><strong>Security and compliance:</strong> Use the robust security features and compliance certifications of AWS to help reduce your security and compliance burden.</li> \n <li><strong>Cost advantages:</strong> Amazon Q Business offers a pay-as-you-go model, so you can scale costs with the number of users and usage for knowledge bases. This can lead to significant cost savings (see <a href=\"https://aws.amazon.com/q/business/pricing/\">pricing details</a>).</li> \n</ul> \n<h2>Implement your generative&nbsp;AI use cases</h2> \n<p>After you’ve chosen your generative AI use cases, consider a phased implementation approach:</p> \n<ol> \n <li><strong>Start with pilot use cases to prove value quickly:</strong> Good pilot use cases include IT help desk or HR workflows. You can get started by taking advantage of AWS-provided example projects and open source samples.</li> \n <li><strong>Evaluate the next use cases:</strong> Prioritize you next use cases by business impact and feature coverage with existing Amazon Q Business connectors and plugins. Often AIOps use cases that include integrations or chat interfaces on top of ServiceNow, Confluence, Teams, or Slack are good examples.</li> \n <li><strong>Use existing data sources:</strong> Connect Amazon Q Business to enterprise systems with supported connectors first to maximize immediate value.</li> \n <li><strong>Implement accuracy testing using frameworks:</strong> Use tools such as the AWS evaluation framework for Amazon Q Business, which includes automated testing pipelines, ground truth datasets, and comprehensive metrics for measuring response quality, relevancy, truthfulness, and overall accuracy.</li> \n <li><strong>Iteratively scale successful implementations across your organization:</strong> Start your implementation with the teams that are most interested in the application and willing to provide feedback. Make changes based on the feedback as needed, then expand it across the organization.</li> \n <li><strong>Measure and track results:</strong> Establish clear KPIs before implementation to quantify business impact.</li> \n</ol> \n<p>Monitor usage and costs, implement feedback loops, and make sure to support security and compliance throughout your generative&nbsp;AI journey. Amazon Q Business can provide significant value when implemented in appropriate use cases with proper planning and governance. Success depends on careful evaluation of business needs, thorough implementation planning, and ongoing management of the solution.</p> \n<h2>Get started on AWS</h2> \n<p>When implementing your generative AI use cases, architectural decisions play a crucial role in achieving long-term success. Let’s explore some best practices for a typical AWS enterprise environment.</p> \n<ul> \n <li><strong><a href=\"https://aws.amazon.com/iam/\">AWS Identity and Access Management (IAM)</a>:</strong> Connecting your corporate source of identities to <a href=\"https://aws.amazon.com/iam/identity-center/\">AWS IAM Identity Center</a> provides better security and user experience, Amazon Q Business users authorize their Amazon Q session with their usual sign-in process, using their existing organizational credentials through the identity source already in place.</li> \n <li><strong>Account structure:</strong> Set up Amazon Q Business service, data sources, and plugins in a shared services account based on application group or business unit to help reduce the number of similar deployments across different AWS accounts.</li> \n <li><strong>Access channels:</strong> When rolling out new use cases, consider also enabling existing familiar enterprise channels such as collaboration tools (Teams or Slack) to provide a frictionless way to test and roll out new use cases.</li> \n <li><strong>Data sources:</strong> When adding data sources, estimate index storage needs and whether your use case requires crawling access control list (ACL) and identity information from the data source and if it is supported by the connector. To reduce initial complexity, focus on use cases that provide the same data to all users, then expand it in a second phase for use cases that rely on ACLs to control access.</li> \n <li><strong>Plugins:</strong> Use plugins to integrate external services as actions. For each use case, verify if a built-in plugin can provide this functionality, or if a custom plugin is needed. For custom plugins, plan an architecture that enables pointing to backend services using OpenAPI endpoints in other AWS accounts across the organization. This allows flexible integration of existing <a href=\"https://aws.amazon.com/lambda/\">AWS Lambda</a> functions or container-based functionality.</li> \n</ul> \n<p>By carefully considering these aspects, you can create a solid foundation for your generative&nbsp;AI implementation that aligns with your organization’s needs and future growth plans.</p> \n<h2>How to deploy Amazon Q Business in your organization</h2> \n<p>The following reference architecture illustrates the main components and flow of a typical Amazon Q Business implementation:</p> \n<p><img alt=\"\" class=\"alignnone wp-image-114264\" height=\"550\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/blog1.png\" width=\"1062\" /></p> \n<p>The workflow is as follows:</p> \n<ol> \n <li>A user interacts with an assistant through an enterprise collaboration system.</li> \n <li>Alternate: A user interacts with the built-in web interface provided by Amazon Q Business.</li> \n <li>The user is authenticated using IAM Identity Center and federated by a third-party identity provider (IdP).</li> \n <li>Data sources are configured for existing enterprise systems and data is crawled and indexed in Amazon Q Business. You can use custom connectors to integrate data sources that aren’t provided by Amazon Q Business.</li> \n <li>The user makes a request that requires action through a custom plugin. Use custom plugins to integrate third-party applications.</li> \n <li>The custom plugin calls an API endpoint that calls an <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> agent using Lambda or <a href=\"https://aws.amazon.com/eks/\">Amazon Elastic Kubernetes Service (Amazon EKS)</a> in another AWS account. The response is returned to Amazon Q Business and the user.</li> \n</ol> \n<h2>Use Amazon Q Business to improve enterprise productivity</h2> \n<p>Amazon Q Business, offers numerous practical applications across enterprise functions. Let’s explore some of the key use cases where Amazon Q Business can enhance organizational efficiency and productivity.</p> \n<ul> \n <li><strong>Knowledge management and support:</strong> Amazon Q Business can manage and retrieve information from documentation and repositories such as internal wikis, SharePoint, Confluence, and other knowledge bases. It provides contextual answers through natural language queries and helps maintain documentation quality by suggesting updates while connecting related information across different repositories. For examples, see <a href=\"https://aws.amazon.com/ai/generative-ai/customers/smartsheet/\">Smartsheet enhances productivity with Amazon Q Business</a>.</li> \n <li><strong>Employee onboarding and training:</strong> Improve your <a href=\"https://aws.amazon.com/blogs/machine-learning/build-a-generative-ai-assistant-to-enhance-employee-experience-using-amazon-q-business/\">employee onboarding experience</a> with automated, personalized learning journeys powered by intelligent support. From instant answers to common questions to guided system setup and interactive training content, this solution helps integrate new team members while supporting their continuous learning and development. To learn more, see <a href=\"https://aws.amazon.com/solutions/case-studies/deriv-case-study/\">Deriv Boosts Productivity and Reduces Onboarding Time by 45% with Amazon Q Business</a> and this <a href=\"https://aws.amazon.com/blogs/machine-learning/build-a-generative-ai-assistant-to-enhance-employee-experience-using-amazon-q-business/\">Amazon Machine Learning blog post</a>.</li> \n <li><strong>IT help desk support:</strong> Shorten IT response times by using AI-driven assistance that delivers round-the-clock support and intelligent troubleshooting guidance. By automating ticket management and using historical data for solution recommendations, this system dramatically reduces response times while easing the burden on your IT support teams.</li> \n <li><strong>Human resources:</strong> Support your HR operations and increase employee satisfaction with an AI-powered solution that provides quick answers to policy questions and streamlines benefits management. <a href=\"https://aws.amazon.com/solutions/guidance/ai-assistants-with-amazon-q-business/\">This intelligent assistant</a> guides employees through HR processes, simplifies leave management, and offers quick access to essential forms and documents, creating a more efficient and user-friendly HR experience.</li> \n <li><strong>Sales and marketing:</strong> Strengthen your sales and marketing efforts with an AI-powered platform that streamlines content creation, market analysis, and proposal development. From generating fresh content ideas to quickly providing product information and competitor insights, teams can use this solution to respond faster to customer needs while making data-driven decisions. See <a href=\"https://aws.amazon.com/blogs/machine-learning/how-aws-sales-uses-amazon-q-business-for-customer-engagement/\">How AWS sales uses Amazon Q Business for customer engagement</a>.</li> \n <li><strong>AI operations:</strong> Upgrade and improve your operational workflow with AI-driven monitoring and automation that transforms system management and incident response. From real-time performance tracking to automated routine tasks and intelligent root cause analysis, teams can use <a href=\"https://aws.amazon.com/blogs/machine-learning/building-an-aiops-chatbot-with-amazon-q-business-custom-plugins/\">this solution</a> to maintain operational efficiency and reduce manual intervention.</li> \n</ul> \n<h2>Customer case study</h2> \n<p>A leading enterprise organization transformed its operational efficiency by implementing Amazon Q Business to tackle widespread knowledge accessibility challenges. Prior to implementation, the company struggled with fragmented institutional knowledge scattered across multiple systems, causing significant productivity losses as employees—from systems analysts to executives—spent hours daily searching through documentation, legacy code, and reports.</p> \n<p>By deploying Amazon Q Business, the organization centralized its scattered information from various sources including <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> buckets, Jira, SharePoint, and other content management systems into a single, intelligent interface. The solution dramatically streamlined access to critical information across their complex ecosystem of enterprise resource planning (ERP) systems, databases, sales platforms, and e-commerce integrations.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-114267\" height=\"543\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/blog2.png\" width=\"954\" /></p> \n<p>With approximately 300 employees each saving two hours daily on routine information retrieval tasks, the company achieved remarkable productivity and efficiency gains. Beyond the gains, Amazon Q Business fostered smarter collaboration, reduced subject-matter expert (SME) dependencies, and accelerated decision-making processes, effectively redefining how enterprise knowledge is accessed and used across the organization.</p> \n<h2>Conclusion</h2> \n<p>Amazon Q Business offers AWS customers a scalable and comprehensive solution for enhancing business processes across their organization. By carefully evaluating your use cases, following implementation best practices, and using the architectural guidance provided in this post, you can deploy Amazon Q Business to transform your enterprise productivity. The key to success lies in starting small, proving value quickly, and scaling systematically across your organization.</p> \n<p>For more information on Amazon Q Business, including detailed documentation and getting started guides, visit:</p> \n<ul> \n <li>Explore the&nbsp;<a href=\"https://docs.aws.amazon.com/amazonq/\">Amazon Q documentation</a> to understand more about building custom plugins.</li> \n <li>Check out these related resources: \n  <ul> \n   <li><a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/getting-started.html\">Getting Started with Amazon Q Business</a></li> \n   <li><a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/plugins.html\">Plugins for Amazon Q Business</a></li> \n   <li><a href=\"https://aws.amazon.com/q/business/faqs/\">Amazon Q Business FAQs</a></li> \n  </ul> </li> \n</ul> \n<p>For questions and feedback, visit the&nbsp;<a href=\"https://repost.aws/tags/TALmcXzmfeRaKOzrBowJ9cJQ/amazon-q\">AWS re:Post</a>&nbsp;or contact&nbsp;<a href=\"https://docs.aws.amazon.com/awssupport/latest/user/getting-started.html\">AWS Support</a>.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-thumbnail wp-image-114344 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/Oliver-Steffmann_bio-2-100x100.png\" width=\"100\" /><strong>Oliver Steffmann</strong> is a Principal Solutions Architect at AWS based in New York and is passionate about GenAI and public blockchain use cases. He has over 20 years of experience working with financial institutions and helps his customers get their cloud transformation off the ground. Outside of work he enjoys spending time with his family and training for the next Ironman.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-thumbnail wp-image-114349 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/KP-1-100x100.jpg\" width=\"100\" /><strong>Krishna Pramod</strong>&nbsp;is a Senior Solutions Architect at AWS. He works as a trusted advisor for customers, guiding them through innovation with modern technologies and development of well-architected applications in the AWS cloud. Outside of work, Krishna enjoys reading, music and exploring new destinations.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-thumbnail wp-image-114345 alignleft\" height=\"125\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/Mo-RIV-photo-100x125.jpg\" width=\"100\" /><strong>Mo Naqvi</strong> is a Generative AI Specialist at AWS on the Amazon Q Business team, where he helps enterprise customers leverage generative AI to transform workplace productivity and unlock business intelligence. With expertise in AI-powered search, deep research capabilities, and agentic workflows, he enables organizations to break down data silos and derive actionable insights from their enterprise information.</p>"
        }
      ]
    },
    {
      "title": "Speed up delivery of ML workloads using Code Editor in Amazon SageMaker Unified Studio",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Speed up delivery of ML workloads using Code Editor in Amazon SageMaker Unified Studio"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/speed-up-delivery-of-ml-workloads-using-code-editor-in-amazon-sagemaker-unified-studio/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/speed-up-delivery-of-ml-workloads-using-code-editor-in-amazon-sagemaker-unified-studio/",
      "authors": [
        {
          "name": "Paul Hargis"
        }
      ],
      "author": "Paul Hargis",
      "author_detail": {
        "name": "Paul Hargis"
      },
      "published": "Thu, 21 Aug 2025 20:24:35 +0000",
      "published_parsed": [
        2025,
        8,
        21,
        20,
        24,
        35,
        3,
        233,
        0
      ],
      "tags": [
        {
          "term": "Advanced (300)",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker Studio",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker Unified Studio",
          "scheme": null,
          "label": null
        },
        {
          "term": "Announcements",
          "scheme": null,
          "label": null
        },
        {
          "term": "Technical How-to",
          "scheme": null,
          "label": null
        },
        {
          "term": "AI/ML",
          "scheme": null,
          "label": null
        }
      ],
      "id": "06a60de1ba8424a8a5cc3acaf101286ab6b94e06",
      "guidislink": false,
      "summary": "In this post, we walk through how you can use the new Code Editor and multiple spaces support in SageMaker Unified Studio. The sample solution shows how to develop an ML pipeline that automates the typical end-to-end ML activities to build, train, evaluate, and (optionally) deploy an ML model.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we walk through how you can use the new Code Editor and multiple spaces support in SageMaker Unified Studio. The sample solution shows how to develop an ML pipeline that automates the typical end-to-end ML activities to build, train, evaluate, and (optionally) deploy an ML model."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p><a href=\"https://aws.amazon.com/sagemaker/unified-studio/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker Unified Studio</a> is a single integrated development environment (IDE) that brings together your data tools for analytics and AI. As part of the next generation of <a href=\"https://aws.amazon.com/sagemaker/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker</a>, it contains integrated tooling for building data pipelines, sharing datasets, monitoring data governance, running SQL analytics, building artificial intelligence and machine learning (AI/ML) models, and creating generative AI applications. Recently, AWS announced two additional options that enhance the development experience for analytics, ML, and generative AI teams: <a href=\"https://aws.amazon.com/about-aws/whats-new/2025/05/code-editor-vs-code-open-source-sagemaker-unified-studio/\" rel=\"noopener noreferrer\" target=\"_blank\">Code Editor and multiple spaces</a>. These new IDE options can help developers and data scientists speed up delivery of ML workloads by offering familiar IDE layouts, using popular extensions to enhance development, and using critical debug and test options, all within a unified environment.</p> \n<p>Code Editor, based on Code-OSS (Visual Studio Code – Open Source), provides a lightweight and powerful IDE with familiar shortcuts and terminal access, along with advanced debugging capabilities and refactoring tools. The VSCode IDE, and Code-OSS variants like Code Editor, remain the most <a href=\"https://visualstudiomagazine.com/articles/2023/06/28/so-2023.aspx\" rel=\"noopener noreferrer\" target=\"_blank\">popular</a> development tool in recent years. Teams can boost their productivity by accessing thousands of Code Editor-compatible extensions from the <a href=\"https://open-vsx.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Open VSX extension</a> gallery. The Code Editor IDE within SageMaker Unified Studio supports version control and cross-team collaboration through GitHub, GitLab, or Bitbucket repositories, while offering preconfigured SageMaker distribution for popular ML frameworks.</p> \n<p>Within SageMaker Unified Studio, a <em>space</em> is a work environment that runs a particular IDE. To maximize the benefits of Code Editor alongside other coding interfaces in SageMaker Unified Studio, including JupyterLab, SageMaker now supports multiple spaces per user per project. With multiple spaces, users can manage parallel workstreams with different computational needs. Each space maintains a 1-to-1 relationship with an application instance, so users can efficiently organize their storage and resource requirements. This enhancement provides the flexibility to access multiple applications and instances simultaneously, improving workflow management and productivity.</p> \n<p>In this post, we walk through how you can use the new Code Editor and multiple spaces support in SageMaker Unified Studio. The sample solution shows how to develop an ML pipeline that automates the typical end-to-end ML activities to build, train, evaluate, and (optionally) deploy an ML model.</p> \n<h2>Features of Code Editor in SageMaker Unified Studio</h2> \n<p>Code Editor offers a unique set of features to increase the productivity of your ML team:</p> \n<ul> \n <li><strong>Fully managed infrastructure</strong> – The Code Editor IDE runs on fully managed infrastructure. SageMaker takes care of keeping the instances up-to-date with the latest security patches and upgrades.</li> \n <li><strong>Dial resources up and down</strong> – With Code Editor, you can seamlessly change the underlying resources (such as instance type or EBS volume size) on which Code Editor is running. This is beneficial for developers who want to run workloads with changing compute, memory, and storage needs.</li> \n <li><strong>SageMaker provided images</strong> – Code Editor is preconfigured with <a href=\"https://github.com/aws/sagemaker-distribution\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker Distribution</a> as the default image. This container image has the most popular ML frameworks supported by SageMaker, along with the <a href=\"https://pypi.org/project/sagemaker-studio/\" rel=\"noopener noreferrer\" target=\"_blank\">SageMaker Studio SDK</a>, <a href=\"https://sagemaker.readthedocs.io/en/stable/\" rel=\"noopener noreferrer\" target=\"_blank\">SageMaker Python SDK</a>, <a href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\" rel=\"noopener noreferrer\" target=\"_blank\">Boto3</a>, and other AWS and data science specific libraries installed. This significantly reduces the time you spend setting up your environment and decreases the complexity of managing package dependencies in your ML project.</li> \n <li><strong>Amazon Q Developer</strong> – Code Editor also comes with generative AI capabilities powered by <a href=\"https://aws.amazon.com/q/developer/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Q Developer</a>. You can boost your productivity by generating inline code suggestions within the IDE. In addition, you can use Amazon Q chat to ask questions about building at AWS and for assistance with software development. Amazon Q can explain coding concepts and code snippets, generate code and unit tests, and improve code, including debugging or refactoring.</li> \n <li><strong>Extensions and configuration settings </strong>– Code Editor also includes persistence of installed extensions and configuration settings.</li> \n</ul> \n<p>When you open Code Editor, you will notice that the space has been bootstrapped with the current state of your project’s repository. Navigate to the file explorer, and you will find a <code>getting_started.ipynb</code> Jupyter notebook, as shown in the following screenshot.</p> \n<p><img alt=\"\" class=\"aligncenter wp-image-114203 size-full\" height=\"753\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-smus-nb-1.jpeg\" width=\"1287\" /></p> \n<p>You can choose <strong>Run All</strong> to execute this notebook. Select <strong>Python Environments</strong> when prompted to select the kernel and then choose the recommended Python environment named <code>base</code>. Now the <code>getting_started</code> notebook will be executed, and you can explore the output of the various cells.</p> \n<h2>Architecture of Code Editor in SageMaker Unified Studio</h2> \n<p>When you open Code Editor in SageMaker Unified Studio, it creates an application container that runs on an <a href=\"http://aws.amazon.com/ec2\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Elastic Compute Cloud</a> (Amazon EC2) instance. This instance type matches your selection during Code Editor space configuration. The underlying infrastructure management happens automatically in a service-managed account controlled by SageMaker Unified Studio. The following diagram shows the infrastructure as it relates to end-users and how instances are provisioned. User A has configured two spaces, and User B is using a single space. Both users have the option to create additional spaces as needed. Currently, these spaces are isolated private environments, with shared space functionality planned for a future release.</p> \n<p>SageMaker Unified Studio lets you create multiple spaces with Code Editor or JupyterLab as the IDE, each configurable with different ML instance types, including those with accelerated computing capabilities. For each space, you must specify three core elements: the EBS volume size, your chosen instance type, and the application type you want to run (such as Code Editor or JupyterLab). When you initiate a space, SageMaker Unified Studio automatically provisions a compute instance and launches a SageMaker Unified Studio Code Editor application using your specified container image. The storage system is designed for continuity: your EBS volume persists across sessions, even when you stop and restart the IDE. This means that when you stop the Code Editor application to save on computing costs, although the compute resources shut down, your EBS volume is preserved. Upon restart, the system automatically reattaches this volume, so your work remains intact.</p> \n<p><img alt=\"\" class=\"aligncenter wp-image-114196 size-full\" height=\"1698\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-arch-diag.jpg\" width=\"1420\" /></p> \n<h2>Solution overview</h2> \n<p>In the following sections, we show how to develop an ML project with Code Editor on SageMaker Unified Studio. For this example, we run through a Jupyter notebook that creates an ML pipeline using <a href=\"https://aws.amazon.com/sagemaker/pipelines/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker Pipelines</a>, which automates the usual tasks of building, training, and (optionally) deploying a model.</p> \n<p>In this scenario, Code Editor can be used by an ML engineering team who needs advanced IDE features to test and debug their code, create and execute a pipeline, and monitor the status in SageMaker Unified Studio.</p> \n<h2>Prerequisites</h2> \n<p>To prepare your organization to use the new Code Editor IDE and multiple spaces support in SageMaker Unified Studio, complete the following prerequisite steps:</p> \n<ol> \n <li>Create an AWS account.</li> \n <li>Configure <a href=\"https://aws.amazon.com/iam/identity-center/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS IAM Identity Center</a> accordingly.</li> \n</ol> \n<p>By default, authentication and authorization for a SageMaker Unified Studio domain is controlled through IAM Identity Center, which can only be configured in a single AWS Region that must be the same Region as your SageMaker domain. See <a href=\"https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/setting-up.html\" rel=\"noopener noreferrer\" target=\"_blank\">Setting up Amazon SageMaker Unified Studio</a> for additional information.</p> \n<ol start=\"3\"> \n <li>Create a SageMaker Unified Studio domain using the <a href=\"https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/create-domain-sagemaker-unified-studio-quick.html\" rel=\"noopener noreferrer\" target=\"_blank\">quick setup</a>. A virtual private cloud (VPC) is required; one will be created for you (if needed) during setup.</li> \n <li>After you create the domain, you can enable access to SageMaker Unified Studio for users with single sign-on (SSO) credentials through IAM Identity Center by choosing <strong>Configure</strong> next to <strong>Configure SSO user access </strong>in the <strong>Next steps for your domain </strong>section.</li> \n</ol> \n<p><img alt=\"\" class=\"aligncenter wp-image-114201 size-full\" height=\"588\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-smus-domain-3.gif\" width=\"1662\" /></p> \n<ol start=\"5\"> \n <li>After you configure user access for your newly created domain, navigate to the SageMaker Unified Studio URL and log in using SSO.</li> \n</ol> \n<p>You can find the URL on the SageMaker console, as shown in the following screenshot.</p> \n<p><img alt=\"\" class=\"aligncenter wp-image-114202 size-full\" height=\"254\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-smus-domain-4.png\" width=\"1283\" /></p> \n<p>By default, IAM Identity Center requires multi-factor authentication on user accounts, and you might be prompted to configure this upon first login to SageMaker Unified Studio, as shown in the following screenshot. For more details about this requirement, refer to <a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/user-device-registration.html?icmpid=docs_sso_user_portal\" rel=\"noopener noreferrer\" target=\"_blank\">Registering your device for MFA</a>.</p> \n<p><img alt=\"\" class=\"aligncenter wp-image-114199 size-full\" height=\"906\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-idc-mfa-5.png\" width=\"800\" /></p> \n<ol start=\"6\"> \n <li>After you log in, choose <strong>Create Project</strong> and follow the prompts to create your first SageMaker Unified Studio project. We choose the <strong>All Capabilities</strong> project profile during setup.</li> \n</ol> \n<p>We abstract away some of the concepts around project profiles in this post for simplicity. For more information, refer to <a href=\"https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/project-profiles.html\" rel=\"noopener noreferrer\" target=\"_blank\">Project profiles in Amazon SageMaker Unified Studio</a>.</p> \n<p><img alt=\"\" class=\"aligncenter wp-image-114204 size-full\" height=\"914\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-smus-proj-6.gif\" width=\"1812\" /></p> \n<p>After you create a project, you can create your space (an IDE) in which Code Editor will be provisioned.</p> \n<ol start=\"7\"> \n <li>On the <strong>Compute</strong> tab of the project, choose <strong>Create Space</strong>, then enter a name and choose <strong>Code Editor</strong>.</li> \n</ol> \n<p><img alt=\"\" class=\"aligncenter wp-image-114205 size-full\" height=\"914\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-smus-proj-7.gif\" width=\"1812\" /></p> \n<ol start=\"8\"> \n <li>When the <strong>Status</strong> column indicates the space is <strong>Running</strong>, open the space to be redirected to Code Editor.</li> \n</ol> \n<p><img alt=\"\" class=\"aligncenter wp-image-114197 size-full\" height=\"648\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-codeeditor-8.jpeg\" width=\"1289\" /></p> \n<h2>Interacting with AWS services directly from your IDE</h2> \n<p>Out of the box, Code Editor comes with the <a href=\"https://aws.amazon.com/visualstudiocode/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Toolkit for Visual Studio Code</a> to provide you with an integrated experience to other AWS services during your project, such as viewing data within your <a href=\"http://aws.amazon.com/s3\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3) buckets, finding container images in <a href=\"http://aws.amazon.com/ecr/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Container Registry</a> (Amazon ECR), or visualizing <a href=\"http://aws.amazon.com/cloudwatch\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon CloudWatch</a> logs for your SageMaker environment.</p> \n<p><img alt=\"\" class=\"aligncenter wp-image-114198 size-full\" height=\"946\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-codeeditor-9.gif\" width=\"1614\" /></p> \n<p>The AWS Toolkit for Visual Studio Code uses the permissions of the <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) role assigned to the project. You can find the Amazon Resource Name (ARN) of the project role on the project details page, as shown in the following screenshot.</p> \n<p><img alt=\"\" class=\"aligncenter wp-image-114206 size-full\" height=\"622\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-smus-proj-10.png\" width=\"1287\" /></p> \n<h2>Use Code Editor to create and execute an ML pipeline in SageMaker</h2> \n<p>In this section, we upload and execute a Jupyter notebook that creates and starts a machine learning operations (MLOps) pipeline orchestrated with SageMaker Pipelines. The pipeline we create follows a typical ML application pattern of data preprocessing, training, evaluation, model creation, transformation, and model registration, as illustrated in the following diagram.</p> \n<p><img alt=\"\" class=\"aligncenter wp-image-114200 size-full\" height=\"283\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-sm-pipeline-11.jpg\" width=\"1313\" /></p> \n<p>Begin by uploading the sample notebook directly into Code Editor. You can drag and drop the notebook, or right-click and choose <strong>Upload </strong>in the file explorer pane.</p> \n<p><img alt=\"\" class=\"aligncenter wp-image-114207 size-full\" height=\"612\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ML-18917-smus-upload-12.jpg\" width=\"1056\" /></p> \n<p>You can download and run sample notebooks using standard <code>Git clone</code> commands from the GitHub repository where these notebooks are located. Running the Full Pipeline notebook sample requires a few extra IAM role permissions other than the defaults assigned when the SageMaker Unified Studio project is created. The Quick Pipeline can be run as-is with the default IAM permissions.</p> \n<ul> \n <li><a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/2_end_to_end_genai_on_sagemaker/4_mlops/smus_pipelines_preprocess_train_evaluate_model.ipynb\" rel=\"noopener noreferrer\" target=\"_blank\">Quick Pipeline sample notebook</a></li> \n <li><a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/2_end_to_end_genai_on_sagemaker/4_mlops/smus_pipelines_preprocess_train_evaluate_batch_transform.ipynb\" rel=\"noopener noreferrer\" target=\"_blank\">Full Pipeline sample notebook</a></li> \n</ul> \n<h2>Region availability, cost, and limitations</h2> \n<p>Code Editor and multiple spaces support are available in supported SageMaker Unified Studio domains. For more information about Regions where these features are available, see <a href=\"https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/supported-regions.html\" rel=\"noopener noreferrer\" target=\"_blank\">Regions where Amazon SageMaker Unified Studio is supported</a>. Code Editor will be provisioned within a SageMaker space and run on a user-selectable instance type, anywhere from ultra low-cost instances (ml.t3.medium) up to highly performant GPU-based instances (G6 instance family).</p> \n<p>The primary cost associated with running a Code Editor space is tied directly to the underlying compute instance type. The hourly costs for ML instance types can found on the <a href=\"https://aws.amazon.com/sagemaker-ai/pricing/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker AI pricing page</a> on the <strong>Instance details</strong> tab. To prevent unnecessary charges, the space will be automatically shut down after a configurable timeout when the space is idle (see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_SpaceIdleSettings.html\" rel=\"noopener noreferrer\" target=\"_blank\">SpaceIdleSettings</a>). There will also be minimal charges tied to storage for the EBS volume that is attached to the Code Editor space.</p> \n<p>At launch, Code Editor spaces can be configured to use a particular SageMaker Distribution image, either version 2.6 or 3.1. Additional major and minor releases of the SageMaker Distribution will be added over time.</p> \n<h2>Clean up</h2> \n<p>To avoid incurring additional charges, delete the resources created from following this post. This includes any development environments created, such as Code Editor or JupyterLab spaces, which you can delete by navigating to the <strong>Project Compute</strong> navigation pane, choosing the <strong>Spaces</strong> tab, choosing the options menu (three vertical dots) aligned with the space, and choosing <strong>Delete</strong>. You can remove project resources by <a href=\"https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/delete-project.html\" rel=\"noopener noreferrer\" target=\"_blank\">deleting the project</a>, which can be done from the SageMaker Unified Studio console. There is no charge for a SageMaker Unified Studio domain, but you can optionally delete this from the SageMaker AI console. If you created IAM Identity Center users that you no longer need, delete the users from the IAM Identity Center console.</p> \n<h2>Conclusion</h2> \n<p>The addition of the new Code Editor IDE to SageMaker Unified Studio provides a familiar working environment to thousands of data scientists and developers. With this powerful IDE, data scientists can more quickly build, train, tune, and deploy their ML models and push them into production where they can get measurable ROI. With thousands of pre-tested extensions through the VSX Registry, developers will have improved usability and productivity as they build and deploy their generative AI applications.</p> \n<p>In addition, SageMaker Unified Studio now supports multiple spaces per user per project. These new environment options can help MLOps personas segregate workloads, isolate compute resources, and increase productivity through parallelized workstreams. Together, these enhancements help data science teams work more efficiently in bringing ML and generative AI solutions into production, where they can begin to reap the benefits of their work.</p> \n<p>To get started using SageMaker Unified Studio, refer to the <a href=\"https://catalog.us-east-1.prod.workshops.aws/workshops/06dbe60c-3a94-463e-8ac2-18c7f85788d4/en-US\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker Workshop</a>. This workshop provides complete step-by-step instructions, plus sample datasets, source code, and Jupyter notebooks for gaining hands-on experience with the tooling.</p> \n<p>To learn more about Code Editor, see <a href=\"https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/code-editor.html\" rel=\"noopener noreferrer\" target=\"_blank\">Using the Code Editor IDE in Amazon SageMaker Unified Studio</a>.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-thumbnail wp-image-114690\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/19/phhargis-pic-100x100.png\" width=\"100\" />Paul Hargis</strong> has focused his efforts on machine learning at several companies, including AWS, Amazon, and Hortonworks. He enjoys building technology solutions and teaching people how to leverage them. Paul likes to help customers expand their machine learning initiatives to solve real-world problems. Prior to his role at AWS, he was lead architect for Amazon Exports and Expansions, helping amazon.com improve the experience for international shoppers.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-thumbnail wp-image-114691\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/19/hqudah-pic-100x100.png\" width=\"100\" />Hazim Qudah</strong> is an AI/ML Specialist Solutions Architect at Amazon Web Services. He enjoys helping customers build and adopt AI/ML solutions using AWS technologies and best practices. Prior to his role at AWS, he spent many years in technology consulting with customers across many industries and geographies. In his free time, he enjoys running and playing with his dogs!</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-thumbnail wp-image-114693\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/19/jayan-pic-100x100.png\" width=\"100\" />Jayan Kuttagupthan</strong> is a Senior Software Engineer at Amazon with over 15 years of experience in backend development and design. He is currently working on improving Seller Partner Support Experience at Amazon. As a technical leader, Jayan has successfully built and mentored engineering teams across organizations, while also contributing to the broader tech community through speaking engagements such as SRECon Asia.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"alignleft size-thumbnail wp-image-114697\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/19/majipar-100x133.png\" width=\"100\" />Majisha Namath Parambath</strong> is a Senior Software Engineer at Amazon SageMaker with 9+ years at Amazon. She’s provided technical leadership on SageMaker Studio (Classic and V2) and Studio Lab, and now leads key initiatives for the next-generation Amazon SageMaker Unified Studio, delivering an end-to-end data analytics and interactive machine learning experience. Her work spans system design and architecture, and cross-team execution, with a focus on security, performance, and reliability at scale. Outside of work, she enjoys reading, cooking, and skiing.</p>"
        }
      ]
    },
    {
      "title": "How Infosys Topaz leverages Amazon Bedrock to transform technical help desk operations",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "How Infosys Topaz leverages Amazon Bedrock to transform technical help desk operations"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/how-infosys-topaz-leverages-amazon-bedrock-to-transform-technical-help-desk-operations/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/how-infosys-topaz-leverages-amazon-bedrock-to-transform-technical-help-desk-operations/",
      "authors": [
        {
          "name": "Meenakshi Venkatesan, Karthikeyan Senthilkumar, Aninda Chakraborty"
        }
      ],
      "author": "Meenakshi Venkatesan, Karthikeyan Senthilkumar, Aninda Chakraborty",
      "author_detail": {
        "name": "Meenakshi Venkatesan, Karthikeyan Senthilkumar, Aninda Chakraborty"
      },
      "published": "Thu, 21 Aug 2025 17:25:11 +0000",
      "published_parsed": [
        2025,
        8,
        21,
        17,
        25,
        11,
        3,
        233,
        0
      ],
      "tags": [
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Machine Learning",
          "scheme": null,
          "label": null
        },
        {
          "term": "Customer Solutions",
          "scheme": null,
          "label": null
        },
        {
          "term": "Partner solutions",
          "scheme": null,
          "label": null
        },
        {
          "term": "Technical How-to",
          "scheme": null,
          "label": null
        }
      ],
      "id": "19bd3f2c7977d1a8c8fe4f502d6da4b0b8e8d205",
      "guidislink": false,
      "summary": "In this blog, we examine the use case of a large energy supplier whose technical help desk agents answer customer calls and support field agents. We use Amazon Bedrock along with capabilities from Infosys Topaz™ to build a generative AI application that can reduce call handling times, automate tasks, and improve the overall quality of technical support.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this blog, we examine the use case of a large energy supplier whose technical help desk agents answer customer calls and support field agents. We use Amazon Bedrock along with capabilities from Infosys Topaz™ to build a generative AI application that can reduce call handling times, automate tasks, and improve the overall quality of technical support."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>AI-powered apps and AI-powered service delivery are key differentiators in the enterprise space today. A generative AI-based resource can greatly reduce the onboarding time for new employees, enhance enterprise search, assist in drafting content, check for compliance, understand the legal language of data, and more.</p> \n<p><a href=\"https://aws.amazon.com/generative-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Generative AI</a> applications are an emerging and sought-after solution in the enterprise world for customer care centers, customer relationship management centers, and help desks.</p> \n<p><a href=\"https://www.infosys.com/services/data-ai-topaz.html\" rel=\"noopener noreferrer\" target=\"_blank\">Infosys Topaz</a>, an AI-first offering that accelerates business value for enterprises using generative AI, is integrating AWS generative AI capabilities to future proof enterprise AI solutions including <a href=\"https://www.infosys.com/products-and-platforms/cortex.html\" rel=\"noopener noreferrer\" target=\"_blank\">Infosys Cortex</a>, <a href=\"https://www.infosys.com/services/incubating-emerging-technologies/offerings/personalized-smart-video.html\" rel=\"noopener noreferrer\" target=\"_blank\">Infosys Personalized Smart Video</a> (PSV), Infosys Conversational AI Suite, <a href=\"https://www.infosys.com/services/application-development-maintenance/service-offerings/application-management-platform.html\" rel=\"noopener\" target=\"_blank\">Infosys Live Enterprise Automation Platform (LEAP)</a>, and <a href=\"https://www.infosys.com/services/cyber-security/offerings/platform-powered-services.html\" rel=\"noopener noreferrer\" target=\"_blank\">Infosys Cyber Next</a>.</p> \n<p>In this post, we examine the use case of a large energy supplier whose technical help desk support agents answer customer calls and support meter technicians in the field. We use <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a>, along with capabilities from Infosys Topaz, to build a generative AI application that can reduce call handling times, automate tasks, and improve the overall quality of technical support.</p> \n<h2>Business challenges</h2> \n<p>Meter technicians go to customer locations to install, exchange, service, and repair meters. Sometimes they call support agents from the technical help desk to get guidance and support to fix issues that they can’t fix by themselves. The approximate volume of these calls is 5,000 per week, approximately 20,000 per month.</p> \n<p>Some of the challenges faced by support agents and meter technicians include:</p> \n<ul> \n <li>Locating the appropriate information or resources to address inquiries or concerns effectively.</li> \n <li>The average handling time for these calls varies based on the issue category, but calls in the top 10 categories, which represent over 60% of calls, are over 5 minutes.</li> \n <li>60–70% issues are repetitive, and the rest are new issues.</li> \n</ul> \n<p>Maintaining an adequate workforce to provide prompt responses can be costly. It’s expensive and not scalable to hire more support agents and train them with the knowledge needed to provide support. We built an AI-powered technical help desk that can ingest past call transcripts and new call transcripts in near real time. This will help support agents provide resolutions based on past calls, thereby reducing manual search time so they can attend to other priorities.</p> \n<h2>Solution overview</h2> \n<p>The solution involves creating a knowledge base by ingesting and processing call transcripts, so that the AI assistant can provide resolutions based on history. The benefits of an AI-powered technical help desk include:</p> \n<ul> \n <li>Providing all-day availability</li> \n <li>Saving effort for the help desk agents</li> \n <li>Allowing businesses to focus on new issues</li> \n <li>Reducing wait time and shortening call duration</li> \n <li>Automating actions that the help desk agents take on the backend based on their analysis of the issue</li> \n <li>Improving the quality of technical help desk responses, and thereby communication and outcomes</li> \n</ul> \n<p>This post showcases the implementation details, including user-based access controls, caching mechanisms for efficient FAQ retrieval and updates, user metrics tracking, and response generation with time-tracking capabilities.</p> \n<p>The following diagram shows the flow of data and processes from left to right, starting with call transcripts, going through preprocessing, storage, and retrieval, and ending with user interaction and response generation. It emphasizes the role-based access control throughout the system.</p> \n<p>We used Amazon Bedrock because it integrates seamlessly with other AWS services shown in the diagram, such as <a href=\"https://aws.amazon.com/step-functions\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Step Functions</a>, <a href=\"https://aws.amazon.com/dynamodb\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon DynamoDB</a>, and <a href=\"https://aws.amazon.com/opensearch-service/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon OpenSearch Service</a>. This integration improves data flow and management within a single cloud system.</p> \n<p><img alt=\"Architecture Diagram\" class=\"alignnone wp-image-113097 size-full\" height=\"698\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/04/image-1-2.png\" width=\"1794\" /></p> \n<h2>Building the knowledge base: Data flow <a id=\"_Toc139393833\" rel=\"noopener noreferrer\" target=\"_blank\"></a></h2> \n<p>Calls to the technical help desk are recorded for quality and analysis purposes, and the transcripts are stored in JSON format in an <a href=\"https://aws.amazon.com/s3/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Simple Storage Service (Amazon S3)</a> bucket.</p> \n<p>The conversations are parsed into a CSV file for sorting and a large language model (LLM), such as <a href=\"https://aws.amazon.com/bedrock/claude/\" rel=\"noopener noreferrer\" target=\"_blank\">Anthropic’s Claude Sonnet</a> on Amazon Bedrock, is used to summarize the conversation and determine if the context has useful information, based on the length of the call, key words that indicate relevant context, and so on.</p> \n<p>The shortlisted conversations are chunked, and embeddings are generated and stored in an <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon OpenSearch Serverless</a> vector store. The conversations determined to be irrelevant go into another S3 bucket for future reference. This process is automated, as shown in the following figure.</p> \n<p><img alt=\"Process Flow\" class=\"alignnone wp-image-113098 size-full\" height=\"557\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/04/image-2-2.png\" width=\"1093\" /></p> \n<p>A virtual assistant is then built on top of the knowledge base that will assist the support agent.</p> \n<p>The conversations are parsed into a CSV file for simple sorting and an LLM such as <a href=\"https://aws.amazon.com/bedrock/claude/\" rel=\"noopener noreferrer\" target=\"_blank\">Anthropic’s Claude Sonnet</a> on Amazon Bedrock is used to summarize the conversation and determine if the context has useful information, based on the length of the call, key words that indicate relevant context, and so on.</p> \n<p>An <a href=\"https://docs.aws.amazon.com/lambda/latest/operatorguide/event-driven-architectures.html\" rel=\"noopener noreferrer\" target=\"_blank\">event-driven</a> <a href=\"http://aws.amazon.com/lambda\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Lambda</a> function is triggered when new call transcripts are loaded into the S3 bucket. This will trigger a Step Functions workflow.</p> \n<p>From the raw CSV file of call transcripts, only a few fields are extracted: a contact ID that is unique for a particular call session between a customer and a support agent, the <code>participant</code> column indicating the speaker (who can be either a support agent or a customer) and the <code>content</code> column, which is the conversation.</p> \n<p>To build the knowledge base, we used Step Functions to ingest the raw CSV files, as shown in the following workflow.</p> \n<p><img alt=\"Build Knowledge Bases\" class=\"alignnone wp-image-113100 size-full\" height=\"980\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/04/image-3-1.png\" width=\"646\" /></p> \n<p>The automated workflow begins when a user uploads the JSON file to an S3 bucket.</p> \n<ol> \n <li>The Step Functions workflow receives the Amazon S3 URL of the CSV transcripts from a Lambda function. The <code>contactid</code> is unique for a particular call session between the customer and the agent, who are the participants, and the <code>content</code> is the actual conversation.</li> \n <li>The Lambda function (Parse Transcripts from CSV) uses this Amazon S3 URL to download the CSV files and uses Pandas to preprocess the CSV in a format with the contact ID and transcript only. Conversations with the same contact ID are concatenated into a single row.</li> \n <li>The second step is a classification task that ingests, classifies, and keeps or discards conversions. The conversations are passed to the map state. In map state, conversations are handled concurrently. For each conversation row, this state triggers concurrent execution of another Lambda function (Check for Irrelevant Conversations) that will classify each conversation as relevant or irrelevant. \n  <ol type=\"a\"> \n   <li>For this classification task, the Lambda function uses Anthropic’s Claude Sonnet model on Amazon Bedrock. It uses zero-shot chain-of-thought prompting, to first summarize the conversation and then to determine the relevance. If the conversation is disconnected or disjointed (because of signal disturbances or other reasons), or has no meaningful context (when the agent is unable to provide resolution), it’s classified as irrelevant.</li> \n  </ol> </li> \n <li>Finally, the map state takes each instance of the conversation (classified as relevant or irrelevant) and passes to the choice state, which will log the irrelevant conversations into an S3 bucket and relevant conversations are passed to another Lambda function (Handle Relevant Conversations) for further processing.</li> \n <li>The final Lambda function (Log Irrelevant Conversations) reads the relevant conversations and generates the summary, problem, and resolution steps using Anthropic’s Claude Sonnet. The summary generated is used for creating the summary embeddings.</li> \n</ol> \n<p>The following is an example of an irrelevant conversation that is discarded.</p> \n<table border=\"1px\" cellpadding=\"10px\" class=\"styled-table\"> \n <tbody> \n  <tr style=\"background-color: #000000;\"> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">Contactid</span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">Participant</span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">Content</span></td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>66da378c-8d74-467b-86ca-7534158b63c2</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">Help the school speaking</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>66da378c-8d74-467b-86ca-7534158b63c2</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">Your morning call it said Chris Simpson near me, TX 75 is, uh, locked out spinning disc</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>66da378c-8d74-467b-86ca-7534158b63c2</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">No problem. What’s your carry, please?</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>66da378c-8d74-467b-86ca-7534158b63c2</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">Thanks to see 27492.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>66da378c-8d74-467b-86ca-7534158b63c2</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">Thank you. Right, you’ll be kicked off.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>66da378c-8d74-467b-86ca-7534158b63c2</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">Single noise. Anything anyway, mate. When you look back in, you’ll be fine</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>66da378c-8d74-467b-86ca-7534158b63c2</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">Yep.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>66da378c-8d74-467b-86ca-7534158b63c2</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">Alright, Right. Thank you. Choose them.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>66da378c-8d74-467b-86ca-7534158b63c2</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">I think she’s made a bit Right bye.</td> \n  </tr> \n </tbody> \n</table> \n<p>The following is an example of a relevant conversation.</p> \n<table border=\"1px\" cellpadding=\"10px\" class=\"styled-table\"> \n <tbody> \n  <tr style=\"background-color: #000000;\"> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">Contactid</span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">Participant</span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">Content</span></td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">Hello.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">Help those gathers Reagan. Yes.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">Get up, and then I’ll speak to someone about clearing the cash on my T C 75. So, can do. Off job certainly things because you won’t let me sorry minutes, just saying Could not establish network connection.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">Yeah, I’ve got a signal.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">Yeah, it’s not trying to do is connected. We got three D 14. It’s up, right?</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">What should happen because I’m in the four G area.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">Yeah, dragged down the screen twice from the top for me.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">Yep. He? Yeah.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">Yep. And check that survey is eight hasn’t turned itself off.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">Need. Okay, try again.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">There you go, right showing us connected. We can</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">CUSTOMER</td> \n   <td style=\"padding: 10px;\">All right. Can you clear the cat 12 can signal is day to see this message. Contact the T. H. D.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">Yep.</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\"><code>079a57bf-9700-45d3-bbd9-11d2d41370c7</code></td> \n   <td style=\"padding: 10px;\">AGENT</td> \n   <td style=\"padding: 10px;\">There you go. That should take you out any second, okay?</td> \n  </tr> \n </tbody> \n</table> \n<p>The following table shows the final knowledge base schema.</p> \n<table border=\"1px\" cellpadding=\"10px\" class=\"styled-table\" style=\"height: 215px;\" width=\"1196\"> \n <tbody> \n  <tr style=\"background-color: #000000;\"> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">k_id</span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">conversation_history</span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">Summary</span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">Problem</span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">resolution_steps</span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\">summary_embeddings</span></td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\">1</td> \n   <td style=\"padding: 10px;\">AGENT: Hi, how can I help you CUSTOMER: Hi, I am facing a black screen issue.…</td> \n   <td style=\"padding: 10px;\">Customer is facing with a issue …</td> \n   <td style=\"padding: 10px;\">Black Screen issue</td> \n   <td style=\"padding: 10px;\"> \n    <ul> \n     <li>Restart app</li> \n     <li>If issue persist, reinstall app</li> \n    </ul> <p>…</p></td> \n   <td style=\"padding: 10px;\">[0.5078125,-0.071777344,0.15722656,0.46679688,0.56640625,-0.037353516,-0.08544922,0.00012588501, …]</td> \n  </tr> \n </tbody> \n</table> \n<h2>Building an effective RAG pipeline</h2> \n<p>The success of retrieval systems relies on an effective embedding model. The <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-embed-text.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Titan Text Embeddings</a> model is optimized for text retrieval to enable Retrieval Augmented Generation (RAG). Instead of processing massive documents at the same time, we used chunking strategies to improve retrieval. We used a chunk size of 1,000 with an overlapping window of 150–200 for best results. Chunking combined with page boundaries is a simple yet highly effective approach. Sentence window retrieval also returns accurate results.</p> \n<p>Prompting techniques play a crucial role in obtaining effective results. For example, instead of “guidelines for smart meter installation,” an expanded prompt such as “instructions, procedures, regulations, and best practices along with agent experiences for installation of a smart meter” yields better results.</p> \n<p>Building production-ready RAG applications requires a performant vector database as well. The <a href=\"https://aws.amazon.com/opensearch-service/serverless-vector-engine\" rel=\"noopener noreferrer\" target=\"_blank\">vector engine for OpenSearch Serverless</a> provides a scalable and high-performing vector storage and search capability; key features include adding, updating, and deleting vector embeddings in near real time without impacting query performance. See <a href=\"https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Build a contextual chatbot application using Amazon Bedrock Knowledge Bases</a> for more information.</p> \n<h2>Security considerations</h2> \n<p>This architecture implements comprehensive security measures across the components. We use <a href=\"https://aws.amazon.com/secrets-manager/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Secrets Manager</a> to securely store and manage sensitive credentials, API keys, and database passwords, with automatic rotation policies in place. S3 buckets are encrypted using <a href=\"http://aws.amazon.com/kms\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Key Management Service</a> (AWS KMS) with AES-256 encryption, and versioning is enabled for audit purposes. Personally identifiable information (PII) is handled with extreme care— PII data is encrypted and access is strictly controlled through <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) policies and AWS KMS. For OpenSearch Serverless implementation, we make sure data is encrypted both at rest using AWS KMS and in transit using TLS 1.2. Session management includes timeout for inactive sessions, requiring re-authentication for continued access. The system interacts with access control list (ACL) data stored in DynamoDB through a secure middleware layer, where the DynamoDB table is encrypted at rest using AWS managed KMS keys. Data transmissions between services are encrypted in transit using TLS 1.2, and we maintain end-to-end encryption across our entire infrastructure. Access controls are granularly defined and regularly audited through <a href=\"http://aws.amazon.com/cloudtrail\" rel=\"noopener noreferrer\" target=\"_blank\">AWS CloudTrail</a>.</p> \n<h2><strong>Implementing role-based access control</strong></h2> \n<p>We used three different personas to implement role-based access control: an administrator with full access, a technical desk analyst with a medium level of access, and a technical agent with minimal access. We used OpenSearch Serverless collections to manage different access levels. Different call transcripts are ingested into different collections; this is to enable user access to the content they are authorized to based on their roles. A list of user IDs and their roles and allowed access are stored in a DynamoDB table along with the OpenSearch collection and index name.</p> \n<p>We used the <code>authenticate.login</code> method in a <a href=\"https://streamlit.io/generative-ai\" rel=\"noopener noreferrer\" target=\"_blank\">Streamlit</a> authenticator to retrieve the user ID.</p> \n<h2>User interface and agent experience</h2> \n<p>We used Streamlit as a frontend framework to build the TECHNICAL HELP DESK, with access to the content controlled by the user’s role. The UI features an FAQ section displayed at the top of the main page and a search metrics insights section in the sidebar, as shown in the following screenshot.</p> \n<p><img alt=\"FAQ\" class=\"alignnone wp-image-114884 size-full\" height=\"400\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/20/image-4-2-1.png\" width=\"843\" /></p> \n<p>The UI includes the following components:</p> \n<ul> \n <li><strong>Conversation section</strong> – The conversation section contains interactions between the user and the help desk assistant. Users can provide feedback by choosing either the like or dislike button for each response received, as shown in the following screenshot. This feedback is persisted in a DynamoDB table.</li> \n</ul> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-113103\" height=\"592\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/04/image-5-1.png\" width=\"1216\" /></p> \n<ul> \n <li><strong>User metrics insights </strong>– As shown in the following screenshot, the sidebar contains metrics information, including: \n  <ul> \n   <li>Number of queries in the last week</li> \n   <li>Number of total transcripts</li> \n   <li>Number of transcripts added in the last week</li> \n   <li>Number of helpful responses generated</li> \n   <li>Number of dislikes</li> \n   <li>Number of misses (no answer found)</li> \n  </ul> </li> \n</ul> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-113104\" height=\"394\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/04/image-6-1.png\" width=\"481\" /></p> \n<p>These fields are updated asynchronously after each user query. Additional metrics are also stored, such as sentiment, tone of the speakers, nature of responses generated, and satisfaction percentage.</p> \n<ul> \n <li><strong>FAQ </strong>– The queries are stored in a DynamoDB table along with a query count column. When the help desk agent signs in, the queries with the most counts are displayed in this section, as shown in the following table.</li> \n</ul> \n<table border=\"1px\" cellpadding=\"10px\" class=\"styled-table\"> \n <tbody> \n  <tr style=\"background-color: #000000;\"> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\"><strong>Partition key</strong></span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\"><strong>Sort key</strong></span></td> \n   <td style=\"padding: 10px;\"><span style=\"color: #ffffff;\"><strong>Global secondary index</strong></span></td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\">Document name</td> \n   <td style=\"padding: 10px;\">Questions</td> \n   <td style=\"padding: 10px;\">Counter</td> \n  </tr> \n  <tr> \n   <td rowspan=\"3\" style=\"padding: 10px;\">Microsoft Authenticator</td> \n   <td style=\"padding: 10px;\">Overview of MFA</td> \n   <td style=\"padding: 10px;\">1</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\">What is TAP in MFA</td> \n   <td style=\"padding: 10px;\">2</td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px;\">Common issues in MFA</td> \n   <td style=\"padding: 10px;\">1</td> \n  </tr> \n </tbody> \n</table> \n<p>The <strong>Counter</strong> column is created as the global secondary index to retrieve the top five FAQs.</p> \n<p>After the user submits a query, the technical help desk fetches the top similar items from the knowledge base. This is compared with the user’s query and, when a match is found, the <strong>Counter</strong> column is incremented.</p> \n<h2>Cache management</h2> \n<p>We used the <code>st.cache_data()</code> function in Streamlit to store the valid results in memory. The results are persisted across the user sessions.</p> \n<p>The caching function employs an internal hashing mechanism that can be overridden if required. The cached data can be stored either in memory or on disk. Additionally, we can set the data persistence duration as needed for the use case. Cache invalidation or updates can be done when the data changes or after every hour. This, along with the FAQ section, has significantly enhanced performance of the technical help desk, creating faster response times and improving the user experience for customers and support agents.</p> \n<h2>Conclusion</h2> \n<p>In this post, we showed you how we built a generative AI application to significantly reduce call handling times, automate repetitive tasks, and improve the overall quality of technical support.</p> \n<p>The enterprise AI assistant from the <a href=\"https://www.infosys.com/services/data-ai-topaz/offerings/agentic-foundry.htmly\" rel=\"noopener noreferrer\" target=\"_blank\">Infosys Agentic Foundry</a>, part of Infosys Topaz, now handles 70% of the previously human-managed calls. For the top 10 issue categories, average handling time has decreased from over 5 minutes to under 2 minutes, a 60% improvement. The continuous expansion of the knowledge base has reduced the percentage of issues requiring human intervention from 30–40% to 20% within the first 6 months after deployment.</p> \n<p>Post-implementation surveys show a 30% increase in customer satisfaction scores related to technical support interactions.</p> \n<p>To learn more about other solutions built with Amazon Bedrock and Infosys Topaz, see <a href=\"https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Create a multimodal assistant with advanced RAG and Amazon Bedrock</a> and <a href=\"https://www.infosys.com/services/data-ai-topaz/insights/advanced-rag-processing.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">Infosys Topaz Unlocks Insights with Advanced RAG Processing for Oil &amp; Gas Drilling Data</a>.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113288 size-full alignleft\" height=\"128\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/meenakshi.jpeg\" width=\"100\" /><strong>Meenakshi Venkatesan</strong> is a Principal Consultant at Infosys and a part of the AWS Centre Of Excellence at Infosys Topaz. She helps design, develop, and deploy solutions in AWS environments and has interests in exploring the new offerings and services.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113287 size-full alignleft\" height=\"130\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/karthikeyan.jpeg\" width=\"100\" /><strong>Karthikeyan Senthilkumar</strong>&nbsp;is a Senior Systems Engineer at Infosys and a part of the AWS COE at iCETS. He specializes in AWS generative AI and database services.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113286 size-full alignleft\" height=\"103\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/aninda.jpeg\" width=\"100\" /><strong>Aninda Chakraborty</strong>&nbsp;is a Senior Systems Engineer at Infosys and a part of the AWS COE at iCETS. He specializes in generative AI and is passionate about leveraging technology to create innovative solutions that drive progress in this field.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113285 size-full alignleft\" height=\"123\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Ashutosh-Dubey.png\" width=\"100\" /><strong>Ashutosh Dubey</strong> is an accomplished software technologist and Technical Leader at Amazon Web Services, where he specializes in Generative AI solutions architecture. With a rich background in software development and data engineering, he architects enterprise-scale AI solutions that bridge innovation with practical implementation. A respected voice in the tech community, he regularly contributes to industry discourse through speaking engagements and thought leadership on Generative AI applications, Data engineering, and ethical AI practices.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-113284 alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/vishal.jpeg\" width=\"100\" />Vishal Srivastava</strong> is a Senior Solutions Architect with a deep specialization in Generative AI. In his current role, he collaborates closely with NAMER System Integrator (SI) partners, providing expert guidance to architect enterprise-scale AI solutions. Vishal’s expertise lies in navigating the complex landscape of AI technologies and translating them into practical, high-impact implementations for businesses. As a thought leader in the AI space, Vishal is actively engaged in shaping industry conversations and sharing knowledge. He is a frequent speaker at public events, webinars, and conferences, where he offers insights into the latest trends and best practices in Generative AI.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"wp-image-113283 size-full alignleft\" height=\"98\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/dhiraj.jpeg\" width=\"100\" /></strong><strong>Dhiraj Thakur </strong>is a Solutions Architect with Amazon Web Services, specializing in Generative AI and data analytics domains. He works with AWS customers and partners to architect and implement scalable analytics platforms and AI-driven solutions. With deep expertise in Generative AI services and implementation, end-to-end machine learning implementation, and cloud-native data architectures, he helps organizations harness the power of GenAI and analytics to drive business transformation. He can be reached via <a href=\"https://www.linkedin.com/in/dhiraj-thakur-14535632/\" rel=\"noopener noreferrer\" target=\"_blank\">LinkedIn</a>.</p>"
        }
      ]
    },
    {
      "title": "Create personalized products and marketing campaigns using Amazon Nova in Amazon Bedrock",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Create personalized products and marketing campaigns using Amazon Nova in Amazon Bedrock"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/create-personalized-products-and-marketing-campaigns-using-amazon-nova-in-amazon-bedrock/"
        },
        {
          "length": "113356345",
          "type": "video/mp4",
          "href": "https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-19120/FragranceLab_Social_Horizontal_compressed.mp4",
          "rel": "enclosure"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/create-personalized-products-and-marketing-campaigns-using-amazon-nova-in-amazon-bedrock/",
      "authors": [
        {
          "name": "Raechel Frick"
        }
      ],
      "author": "Raechel Frick",
      "author_detail": {
        "name": "Raechel Frick"
      },
      "published": "Wed, 20 Aug 2025 21:50:24 +0000",
      "published_parsed": [
        2025,
        8,
        20,
        21,
        50,
        24,
        2,
        232,
        0
      ],
      "tags": [
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Nova",
          "scheme": null,
          "label": null
        },
        {
          "term": "CPG",
          "scheme": null,
          "label": null
        },
        {
          "term": "Generative AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "Industries",
          "scheme": null,
          "label": null
        },
        {
          "term": "Marketing & Advertising",
          "scheme": null,
          "label": null
        },
        {
          "term": "Media & Entertainment",
          "scheme": null,
          "label": null
        },
        {
          "term": "Retail",
          "scheme": null,
          "label": null
        }
      ],
      "id": "cc357c896bdcc6f65850a3caf9cbb9e8681b6652",
      "guidislink": false,
      "summary": "Built using Amazon Nova in Amazon Bedrock, The&nbsp;Fragrance Lab represents a comprehensive end-to-end application that illustrates the transformative power of generative AI in retail, consumer goods, advertising, and marketing. In this post, we explore the development of The Fragrance Lab. Our vision was to craft a unique blend of physical and digital experiences that would celebrate creativity, advertising, and consumer goods while capturing the spirit of the French Riviera.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Built using Amazon Nova in Amazon Bedrock, The&nbsp;Fragrance Lab represents a comprehensive end-to-end application that illustrates the transformative power of generative AI in retail, consumer goods, advertising, and marketing. In this post, we explore the development of The Fragrance Lab. Our vision was to craft a unique blend of physical and digital experiences that would celebrate creativity, advertising, and consumer goods while capturing the spirit of the French Riviera."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p><em>This post was written with Jake Friedman from Wildlife.</em></p> \n<p>Businesses are seeking innovative ways to differentiate themselves through hyper-personalization and enhanced customer experiences. At the Cannes Lions International Festival of Creativity 2025, AWS showcased <a href=\"https://aws.amazon.com/ai/generative-ai/nova/fragrance-lab/\" rel=\"noopener noreferrer\" target=\"_blank\">The Fragrance Lab</a>, an interactive and inspiring experience that demonstrates how generative AI can support the development of hyper-personalized consumer goods and accelerate advertising creative concept and campaign assets development. Following Cannes Lions 2025, The Fragrance Lab received a Gold and Silver Stevie Award from the <a href=\"https://stevieawards.com/iba/event-category-winners\" rel=\"noopener noreferrer\" target=\"_blank\">International Business Awards</a> in the Brand &amp; Experiences category.</p> \n<p>Built using <a href=\"https://aws.amazon.com/ai/generative-ai/nova/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Nova</a> in <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a>, The&nbsp;Fragrance Lab represents a comprehensive end-to-end application that illustrates the transformative power of generative AI in retail, consumer goods, advertising, and marketing. While our activation at Cannes Lions focused on personalized fragrance development and ad campaign creation, the underlying architecture and methodology can be adapted across diverse categories, from fashion to food and beverage, opening endless possibilities for customized customer experiences.</p> \n<h2>Introducing The Fragrance Lab</h2> \n<p>In this post, we explore the development of The Fragrance Lab. Our vision was to craft a unique blend of physical and digital experiences that would celebrate creativity, advertising, and consumer goods while capturing the spirit of the French Riviera. To bring this vision to life, we collaborated with <a href=\"http://wildlife.la\" rel=\"noopener noreferrer\" target=\"_blank\">Wildlife</a>, a company that is exceptional at transforming AWS generative AI services into compelling physical experiences. Wildlife was fundamental in&nbsp;brainstorming ideas that would inspire customers and showcase novel use cases that AI makes possible.</p> \n<div class=\"wp-video\" style=\"width: 640px;\">\n <video class=\"wp-video-shortcode\" controls=\"controls\" height=\"360\" id=\"video-114313-2\" preload=\"metadata\" width=\"640\">\n  <source src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-19120/FragranceLab_Social_Horizontal_compressed.mp4?_=2\" type=\"video/mp4\" />\n </video>\n</div> \n<h3>Crafting the fragrance</h3> \n<p>As the first step, the experience used <a href=\"https://aws.amazon.com/ai/generative-ai/nova/speech/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Nova Sonic</a>, a speech-to-speech model that engages in intuitive dialogues with attendees to understand their personality and preferences. Nova Sonic extends its capabilities through tool integration, allowing it to manage user traits and interface actions through specialized tools such as <code>addTraitTool</code>, <code>removeTraitTool</code>, and <code>uiActionIntentTool</code>. These tools help maintain conversation state and a consistent flow throughout the experience. The collected conversation data and trait information are then processed through a custom Retrieval Augmented Generation (RAG) system built with <a href=\"https://aws.amazon.com/ai/generative-ai/nova/understanding/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Nova Pro</a>, a highly capable multimodal model that offers our best combination of accuracy, speed, and cost. Nova Pro serves as the intelligence engine for analyzing interactions and extracting essential keywords to determine the perfect fragrance notes and composition. The application also used <a href=\"https://aws.amazon.com/bedrock/guardrails/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Guardrails</a>, which offers customizable safeguards and responsible AI policies to block undesirable topics—such as allergens or harmful content—to offer a seamless customer experience.</p> \n<p>For example, a customer might share with Nova Sonic that they are interested in travel. Nova Pro picked up that exploring new places often “brings a sense of freshness and excitement,” which resulted in a fragrance that feels fresh and invigorating, featuring “a burst of citrus or a floral breeze.” The customer might also share that they enjoy early morning walks across spring fields, which Nova Pro translates into a top note of fresh bergamot, a middle note featuring floral honey, and a base of lavender.&nbsp;The customers’ inputs guide the selection of fragrance notes—from base, to heart, to top notes—which were then expertly mixed by on-site perfumers to create truly personalized scents. Perfumers were able to customize and craft hundreds of unique fragrances per day, aided by AI. A process that would normally take hours for a perfumer was accelerated to minutes, empowering both the customer and the fragrance expert.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114316\" height=\"833\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ML-19120-1.jpeg\" width=\"1430\" /></p> \n<h3>Creating the campaign</h3> \n<p>After the personalized fragrance formula was created and sent to the perfumer queue, <a href=\"https://aws.amazon.com/ai/generative-ai/nova/creative/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Nova Canvas</a> generated customized marketing creative, including the fragrance name, tagline, and imagery that captured the essence of the formula. Attendees were able to further customize the campaign assets using guest inputs such as moody, beachy, or playful. The resulting fragrance image was then transformed into dynamic video content through <a href=\"https://aws.amazon.com/ai/generative-ai/nova/creative/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Nova Reel</a>, which customers could further customize to meet their creative vision and download to save or share. To match the Cannes Lions atmosphere, the campaign videos were generated with a French-accented female voice using <a href=\"https://aws.amazon.com/polly/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Polly</a>.&nbsp;The entire experience is built in Amazon Bedrock, a fully managed service to build and scale generative AI applications with AI models.</p> \n<p>The following data flow diagram shows how multiple Amazon Nova models can be combined for a rich, cohesive, and personalized customer experience.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114317\" height=\"1417\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ML-19120-2.png\" width=\"2710\" /></p> \n<h2>Best practices for implementation</h2> \n<p>The Fragrance Lab centers around interactions with Amazon Nova Sonic, providing users with a natural language interface to express their preferences for a custom scent. Through its tool integration capabilities, Nova Sonic orchestrates the entire experience by managing user traits and triggering appropriate workflows. These workflows seamlessly guide the experience from initial conversation to fragrance development and ultimately to campaign asset creation, driving both the visual elements and progression of the experience. The model’s ability to maintain a conversational state, while defining clear conversational flows, helps ensure a consistent and pleasant experience for every user.</p> \n<p>A well-defined workflow and conversational assistant are pivotal in guiding these conversations to uncover the qualities that are most important to each user. And the system prompt determines the personality, style, and content of your conversational assistant.</p> \n<p><strong>Prompt example:</strong></p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">You are an AI assistant designed to help the user explore their personality and \nemotional landscape in the context of creating a unique fragrance. You engage in warm, \nfree-flowing, playful conversation with the user to draw out their character, \npreferences, moods, and desires. Your end goal is to derive a set of 3 to 5 personality \ntraits that best describe the user. These traits will later be used in a separate \nprocess to match appropriate fragrance ingredients. Your tone is warm, chic, and subtly \nplayful.</code></pre> \n</div> \n<p>Additional contextual information within the prompt also plays a key role in Amazon Nova Sonic effectively maintaining state, while defining the conversational flow helps ensure consistent, pleasant, and concise experiences for every user.</p> \n<p><strong>Prompt example:</strong></p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">1. **Welcoming Users**\n    Welcome the user to the application experience with a brief overview of the\n    process and ask if they are ready to continue.\n2. **Assistant Turns** \n    Ask short and unique open ended questions to the user and choose a personality trait \n    that you think would suit the user best.\n3. **Handling User Turns**\n    Acknowledge the user's answers briefly and warmly.\n    Focus on one trait per turn.\n    Call the \"addTraitTool\", \"removeTraitTool\", \"replaceTraitTool\", or \"clearTraitsTool\" \n    tools to manage traits.\n    If the user says to go back, skip, customize, or confirm/submit it means you should \n    call the \"uiActionIntentTool\" </code></pre> \n</div> \n<p>With direct references to our tools in the conversational flow, the user interface feels reactive and connected to the user’s input while providing opportunities for the assistant to demonstrate its expertise on this subject, which comes into the spotlight when user traits and preferences are later mapped to a set of available ingredients and raw fragrance materials.</p> \n<p>This complex fragrance recipe development is handled by Nova Pro, using its accuracy and speed to generate consistently high-quality scents. To draw from a wealth of fragrance knowledge in real time, RAG was implemented to extend Nova Pro capabilities beyond pre-trained knowledge with access to knowledge sources that include essential scent design principles, a deep understanding of each available ingredient, their profiles and potential roles within the fragrance, and their possible connections to users’ aromatic identities.</p> \n<p>The resulting fragrances are then visualized using Nova Canvas and Nova Reel. The creative models generate original compositions that reveal the fragrance name, ingredients, and a visual identity within a high-end creative campaign asset. A set of conditioning images featuring unbranded fragrance bottles help to anchor each image (as shown in the following image).</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114320\" height=\"363\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ML-19120-3.png\" width=\"1429\" /></p> \n<p><strong>Prompt example:</strong></p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">A high-end fragrance ad environment inspired by a [persona description]. A clear, \nunbranded perfume bottle is visually centered and tightly framed. Key ingredients [top \nnote ingredient], [middle note ingredient], [base note ingredient], and [booster \ningredient] are arranged to surround the bottle in a balanced composition, appearing \nbehind, besides, and partially in front of the base. The scene evokes [atmospheric/mood \ndescriptors] using [light/color language]. The setting should feel [stylistic direction],\nlike a [reference style (e.g., fashion editorial, lifestyle spread, luxury campaign)].</code></pre> \n</div> \n<h2>Results</h2> \n<p>Attendees at Cannes Lions took away a physical fragrance mixed by&nbsp;on-site perfumers. While developing hyper-personalized consumer goods might not be scalable across all use cases, brands can innovate with artificial intelligence and achieve manufacturing outcomes that weren’t previously possible. The advertising campaign concept and asset development use case is easy to implement for brands, agencies, and media networks, allowing users to iterate and optimize campaign creative quickly.&nbsp;Using Amazon Bedrock, additional features could be added like translations and sizes, depending on requirements.</p> \n<p>You can <a href=\"https://www.youtube.com/watch?v=_rvYYWWWnUI&amp;feature=youtu.be\" rel=\"noopener noreferrer\" target=\"_blank\">watch a video walk through</a> of The Fragrance Lab onsite at Cannes Lions 2025, and check out the following example campaign outputs.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-114464 size-full\" height=\"571\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/15/TFL-Sample-Images.png\" width=\"1293\" /></p> \n<h2>Conclusion</h2> \n<p>The Fragrance Lab demonstrates the power of Amazon Nova in Amazon Bedrock and how customers can create fully personalized consumer experiences. This use case can be replicated across various retail and consumer goods categories including skincare and cosmetics, fashion and accessories, food and beverage, home goods, and wellness products—all benefiting from natural conversation interaction, AI-powered product development, product identity, and creative marketing campaign generation. Get started with Amazon Nova in Amazon Bedrock today.</p> \n<hr /> \n<h3>About the authors</h3> \n<p><strong>Raechel Frick</strong>&nbsp;is a Sr Product Marketing Manager at AWS. With over 20 years of experience in the tech industry, she brings a customer-first approach and growth mindset to building integrated marketing programs. Based in the greater Seattle area, Raechel balances her professional life with being a soccer mom and after-school carpool manager, demonstrating her ability to excel both in the corporate world and family life.</p> \n<p><strong>Gaby Ferreres</strong> is the Head of Industry Marketing for Media &amp; Entertainment, Sports, Games, Advertising &amp; Marketing at AWS, where she works with technology and industry leaders to accelerate innovation on behalf of customers. She is a global marketing leader and creator of experiences that elevate customer journeys. Before AWS, she held different positions at Microsoft, Telefonica, and more.</p> \n<p><strong>Ashley Weston </strong>is Sr. Marketing Event Manager for Global Third-Party Programs at AWS, where she partners with industry marketing to deliver the highest visibility and most business-critical events for AWS.</p> \n<p><strong>Tiffany Pfremmer</strong> is Sr. Industry Marketing Manager at Amazon Web Services (AWS) where she leads strategic integrated marketing initiatives across the Media &amp; Entertainment, Games, and Sports verticals to deliver marketing campaigns that connect AWS cloud solutions with customer opportunities.</p> \n<p><strong>Jake Friedman </strong>is the President and Co-founder at Wildlife, where he leads a team launching interactive experiences and content campaigns for global brands. His work has been recognized with the Titanium Grand Prix at the Cannes Lions International Festival of Creativity for “boundary-busting, envy-inspiring work that marks a new direction for the industry and moves it forward”. You can find him on <a href=\"https://www.linkedin.com/in/jakefriedman/\" rel=\"noopener noreferrer\" target=\"_blank\">LinkedIn</a>.</p> \n<hr /> \n<h3>About Wildlife</h3> \n<p><a href=\"https://www.wildlife.la/\" rel=\"noopener noreferrer\" target=\"_blank\">Wildlife</a> fuses a digitally born skillset with a future proof mindset to deliver breakthrough products, experiences and campaigns for daring partners. We live by a motto: Technology changes, story doesn’t.</p>"
        }
      ]
    },
    {
      "title": "Tyson Foods elevates customer search experience with an AI-powered conversational assistant",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Tyson Foods elevates customer search experience with an AI-powered conversational assistant"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/tyson-foods-elevates-customer-search-experience-with-an-ai-powered-conversational-assistant/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/tyson-foods-elevates-customer-search-experience-with-an-ai-powered-conversational-assistant/",
      "authors": [
        {
          "name": "Anveshi Charuvaka"
        }
      ],
      "author": "Anveshi Charuvaka",
      "author_detail": {
        "name": "Anveshi Charuvaka"
      },
      "published": "Wed, 20 Aug 2025 21:44:28 +0000",
      "published_parsed": [
        2025,
        8,
        20,
        21,
        44,
        28,
        2,
        232,
        0
      ],
      "tags": [
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Bedrock Knowledge Bases",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon OpenSearch Service",
          "scheme": null,
          "label": null
        },
        {
          "term": "CPG",
          "scheme": null,
          "label": null
        },
        {
          "term": "Customer Solutions",
          "scheme": null,
          "label": null
        },
        {
          "term": "Experience-Based Acceleration",
          "scheme": null,
          "label": null
        }
      ],
      "id": "063916dbcd88c0a92ab97ec47ed03720b8dc55a3",
      "guidislink": false,
      "summary": "In this post, we explore how Tyson Foods collaborated with the AWS Generative AI Innovation Center to revolutionize their customer interaction through an intuitive AI assistant integrated into their website. The AI assistant was built using Amazon Bedrock,",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we explore how Tyson Foods collaborated with the AWS Generative AI Innovation Center to revolutionize their customer interaction through an intuitive AI assistant integrated into their website. The AI assistant was built using Amazon Bedrock,"
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p><a href=\"https://www.tysonfoodservice.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Tyson Foodservice</a> operates as a critical division within <a href=\"https://www.tysonfoods.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Tyson Foods Inc.</a>, using its extensive protein production capabilities to supply a diverse array of foodservice clients across multiple sectors. As one of the largest protein providers in the US, Tyson Foods produces approximately 20% of the nation’s beef, pork, and chicken, which forms the foundation of its foodservice offerings.</p> \n<p>Tyson Foodservice operates through a B2B model, selling products to distributors rather than directly to end consumers, while serving diverse foodservice operators, including restaurants, schools, healthcare facilities, and convenience stores. Until recently, Tyson had limited direct engagement with over 1 million unattended operators who purchased their products through distributors without direct company relationships. To bridge this gap, Tyson has implemented a generative AI assistant on their website, enabling them to scale sales efforts, gather customer insights, and establish direct communication channels. The company’s website now functions as a critical interface where operators can explore products, access menu trends, and discover tailored solutions for their specific foodservice segments, all enhanced by AI-driven personalization that better serves both established customers and previously unattended operators.</p> \n<p>In this post, we explore how Tyson Foods collaborated with the <a href=\"https://aws.amazon.com/ai/generative-ai/innovation-center/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Generative AI Innovation Center</a> to revolutionize their customer interaction through an intuitive AI assistant <a href=\"https://www.tysonfoodservice.com/\" rel=\"noopener noreferrer\" target=\"_blank\">integrated into their website</a>. The AI assistant was built using <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a>, a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI.</p> \n<p><img alt=\"Tyson FoodService AI chat assistant interface\" class=\"alignnone wp-image-112992 size-full\" height=\"916\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/01/tyson-bp-image-1.jpeg\" style=\"margin: 10px 0px 10px 0px;\" width=\"1243\" /></p> \n<h2>Solution overview</h2> \n<p>In this section, we describe the overall architecture of the solution. The workflow includes the following high-level steps:</p> \n<ol> \n <li>A user uses the search bar in <a href=\"https://www.tysonfoodservice.com/\" rel=\"noopener noreferrer\" target=\"_blank\">https://www.tysonfoodservice.com/</a>. The query string is converted to embeddings using Amazon Bedrock and the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Titan Text Embeddings model</a>. The search application performs a k-nearest neighbors (k-NN) vector search to find relevant results in <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon OpenSearch Serverless</a> and return those results to the website. The search application is deployed in <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Elastic Container Service</a> (Amazon ECS) using <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Fargate</a> as the capacity provider and exposed as a REST API using an <a href=\"https://aws.amazon.com/elasticloadbalancing/application-load-balancer/\" rel=\"noopener noreferrer\" target=\"_blank\">Application Load Balancer</a> protected by <a href=\"https://aws.amazon.com/waf/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS WAF</a>.</li> \n <li>The user uses the AI assistant interface to ask questions in natural language. The query is processed by the agent node using <a href=\"https://aws.amazon.com/bedrock/claude/\" rel=\"noopener noreferrer\" target=\"_blank\">Anthropic’s Claude 3.5 Sonnet</a> on Amazon Bedrock. Depending on the subject of the query, the agent might orchestrate multiple agents to return relevant information to the user. The application is deployed using a similar architecture to the semantic search component with the addition of an <a href=\"https://aws.amazon.com/rds/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Relational Database Service</a> (Amazon RDS) database cluster to persist the user high-value actions for analytics purposes.</li> \n <li>Products, recipes, ingredients and other relevant data are available from external sources in JSON format. These are processed using Amazon Bedrock and the Amazon Titan Text Embeddings model to create semantic search embeddings. Then these are ingested into OpenSearch Serverless. The ingestion process run in a different ECS cluster using Fargate as the capacity provider.</li> \n</ol> \n<p>The following diagram illustrates this architecture.</p> \n<p><img alt=\"Tyson FoodService AI assistant architecture\" class=\"alignnone wp-image-112991 size-full\" height=\"674\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/01/tyson-bp-image-2.png\" width=\"1046\" /></p> \n<p>In the following sections, we discuss the solution’s key components and benefits in more detail.</p> \n<h2>Improved semantic search</h2> \n<p>The earlier iteration of search on the Tyson Foodservice website relied on keyword-based search. Traditional keyword-based search on CPG websites like Tyson Foodservice often falters when customers search for products using industry terminology that varies from official catalog descriptions. Chefs searching for “pulled chicken” might miss relevant products labeled as “shredded chicken,” or those looking for “wings” might not see results for “party wings” or “drummettes.” This disconnect frustrates food service professionals who need specific ingredients under tight deadlines and ultimately drives them to competitors where they can more quickly find what they need, resulting in lost revenue opportunities for Tyson. Semantic search transforms this experience by understanding the conceptual relationships between culinary terms, preparation methods, and product applications. A chef searching for “buffalo-style appetizers” would receive results for wings, boneless bites, and similar products regardless of exact keyword matches. By recognizing menu trends, cooking techniques, and professional kitchen terminology, semantic search helps foodservice operators quickly find the Tyson products that meet their exact operational needs, even when using language that differs from catalog descriptions.</p> \n<p>Tyson Foodservice implemented their semantic search capability using OpenSearch Serverless, a fully managed service that minimized the operational complexity of maintaining search infrastructure. This solution automatically scales compute and storage resources to match query volume and product catalog size without requiring dedicated administrative overhead. The serverless architecture helped Tyson rapidly deploy advanced natural language processing capabilities across their entire product database while maintaining cost-efficiency, because they only pay for the resources they actually use. With OpenSearch Serverless, Tyson incorporated vector embeddings and powerful query capabilities that understand foodservice terminology variations, preparation methods, and culinary applications, transforming how operators discover products that meet their specific needs even when their search terms don’t exactly match catalog descriptions.</p> \n<p>For indexing Tyson’s diverse content library of products, recipes, and articles, we implemented a preprocessing workflow that transforms raw metadata into optimized semantic search queries. We used large language models (LLMs) to analyze and extract only the most relevant elements from each content piece, creating meaningful search strings specifically designed for semantic indexing. This approach made sure that purely presentational website copy and non-essential informational text were filtered out, and search-critical elements like culinary applications, preparation methods, and ingredient specifications received proper emphasis in the index. By curating what content gets indexed rather than including everything verbatim, we dramatically improved search relevance while reducing index bloat, so OpenSearch Serverless delivered more precise results that truly match the intent behind chef and operator queries. For indexing the text as semantic vectors, we used Amazon Titan Text Embeddings V2 on Amazon Bedrock.</p> \n<p>The following example prompt illustrates the transformation using only the title, description, and reasons to buy metadata. This generic strategy can be customized according to the customer’s specific needs.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">SEARCH_STRING_PROMPT = \"\"\" Given a product title, description, and reasons to\nbuy, create a single, concise search string suitable for indexing in a vector\ndatabase. This string should focus on distinguishing features, assuming all\nproducts are for foodservice operators unless explicitly stated otherwise.\nEnclose the generated search string within &lt;search_string&gt; XML tags. \n\nFollow these guidelines:\n1. Start with the brand name and product line (if applicable).\n2. Include the main product type and specific identifying features.\n3. List concrete attributes such as preparation state, packaging, or quantity.\n4. Mention specific varieties or assortments included in the product.\n5. Incorporate key points from the reasons to buy, focusing on unique and\n   specific selling points.\n6. Avoid generic terms or those common to all products in the category (e.g.,\n   \"food service\", \"restaurant\", \"operator\").\n7. Omit cliché marketing terms (e.g., \"versatile\", \"high-quality\", \"innovative\")\n   unless they have a specific, demonstrable meaning in the context of the\n   product.\n8. Use precise descriptors that differentiate the product from others in its\n   category.\n9. Omit articles (a, an, the) and unnecessary connecting words.\n10. Use lowercase for all terms except proper nouns.\n11. Separate terms with single spaces.\n12. Aim for a length of 15-20 words.\n13. Prioritize terms that potential buyers are most likely to use in specific\n    search queries.\n    \nExample input:\n&lt;title&gt;Tyson® Heritage Valley™ IF Unbreaded 8 Piece Cut Chicken&lt;/title&gt;\n&lt;description&gt;Order a variety of crispy, seasoned chicken cuts with \nHeritage Valley™ Uncooked, Ice Glazed 8 Piece Cut Chicken. Featuring an \nassortment of breasts, drumsticks, thighs and wings, our chicken portions \nare completely customizable and perfect for center-of-plate features. \nSeparately packaged for quick and easy preparation and portion control, \nour packaging helps your staff reduce waste by allowing them to use what \nthey need, when they need. Ready to cook from frozen, simply fry and \nserve as an assortment for a buffet protein choice.\n&lt;/description&gt;\n&lt;reasons_to_buy&gt;\n['Bone-in assortment of breasts, drumsticks, thighs and wings.', \n'Individually quick frozen, locking in natural juices and tenderness.', \n'Different cuts separately bagged for quick and easy preparation and cleanup.', \n'Ready to cook from frozen.']\n&lt;/reasons_to_buy&gt;\n\nExample output: &lt;search_string&gt;tyson heritage valley unbreaded raw 8-piece\nchicken bone-in breasts drumsticks thighs wings individually-frozen\nseparate-bags cook-from-frozen juicy center-of-plate&lt;/search_string&gt;\n\nNow, create a similar search string for the following product:\n&lt;title&gt;{title}&lt;/title&gt;\n&lt;description&gt;{description}&lt;/description&gt;\n&lt;reasons_to_buy&gt;{reasons_to_buy}&lt;/reasons_to_buy&gt;\n\"\"\"\n</code></pre> \n</div> \n<h2>Agentic chat built using Anthropic’s Claude 3.5 Sonnet on Amazon Bedrock and LangGraph</h2> \n<p>Tyson Foodservice has integrated a powerful generative AI assistant into their website, using Anthropic’s Claude 3.5 Sonnet on Amazon Bedrock and <a href=\"https://www.langchain.com/langgraph\" rel=\"noopener noreferrer\" target=\"_blank\">LangGraph</a>. This AI assistant delivers a seamless conversational search experience that offers comprehensive support across Tyson’s extensive range of products, recipes, and articles, providing contextual guidance through natural conversation. Its capabilities include:</p> \n<ul> \n <li><strong>Personalized search</strong> – Uses semantic search to find relevant products, recipes, and articles. The AI assistant customizes recommendations by learning about the user’s business and role, creating a tailored experience while gathering valuable customer insights for Tyson.</li> \n <li><strong>Detailed product information</strong> – Provides comprehensive details about specific Tyson products, including descriptions, ingredients, preparation methods, and suggested applications.</li> \n <li><strong>Distributor services</strong> – Helps users locate nearby distributors and check product availability in their area.</li> \n <li><strong>Purchasing assistance</strong> – Offers information on how to buy Tyson products and connects customers with sales representatives when needed.</li> \n <li><strong>Promotion awareness</strong> – Keeps customers informed about current Tyson Foodservice promotions and special offers.</li> \n <li><strong>Feedback channel</strong> – Provides a streamlined way for customers to submit product and service feedback directly to Tyson.</li> \n <li><strong>Natural conversational flow</strong> – Maintains context throughout the interaction, allowing users to reference previous results and ask follow-up questions for a more human-like conversation experience.</li> \n</ul> \n<p>The following diagram illustrates the high-level architecture of the AI assistant. The system uses the tool calling capabilities of Anthropic’s Claude to implement the AI assistant’s agentic behavior. We used LangGraph to streamline the implementation process, because it provides several convenient primitives specifically designed for building agentic systems with LLMs.</p> \n<p><img alt=\"Tyson Food Services agentic chat tool calling\" class=\"alignnone wp-image-112990 size-full\" height=\"508\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/01/tyson-bp-image-3.jpeg\" width=\"621\" /></p> \n<p>The main components of the architecture are:</p> \n<ul> \n <li><strong>Agent node</strong> – The agent node is implemented using a large prompt that directly receives the user message and responds using the conversational capabilities of the LLM. It also defines the agentic behavior by using the tool calling capability: whenever serving the user’s request requires calling a tool, the agent node issues a tool request.</li> \n <li><strong>Tool execution node </strong>– This node implements a generic tool executor that connects to various tools. Whenever a tool call is issued by the agent node, this node handles the execution of the tool call. The tool calling node executes the tools, which are defined as Python functions, and returns the results to the agent node to be transformed or summarized and presented to the user. LangGraph provides a generic implementation of the <a href=\"https://langchain-ai.github.io/langgraph/how-tos/tool-calling/\" rel=\"noopener noreferrer\" target=\"_blank\">ToolNode</a> that can also be extended to implement additional functionality.</li> \n <li><strong>Tools layer </strong>– Tools are implemented as simple programmatic functions that take inputs and return outputs. These tools augment the capabilities of LLMs by performing functions like retrieving data or submitting feedback. The tools are stateless and agnostic to the current conversation between the user and agent. The LLM agent extracts the input parameters required to execute these tools. These tools in our implementation are a thin wrapper around the services and database layer that implement the actual functionality.</li> \n</ul> \n<p>The following system prompt provides a general guidance for implementing the agent node:</p> \n<pre><code class=\"lang-python\">import date\n\nAGENT_SYSTEM_PROMPT = \"\"\"\n# Tyson Foodservice (TFS) Customer Support Assistant\n\n## Core Role and Purpose\nYou are a helpful customer support assistant for Tyson Foodservice a.k.a TFS\nhosted on their https://www.tysonfoodservice.com/ website.  You will be helpful\nand answer the customers questions. The customers are mainly interested in\nlearning about the products for their specific needs.\nRefrain from engaging in any conversation unrelated to tyson food search of\nproducts, recipes or distributors. If the user asks any unrelated questions the\npolitely decline and mention your purpose. Do not provide and additional\ninformation or advice.\n \nYour job is to stay factual and only provide relevant information from the\ncurrent context or retrieved using the tools. Do not offer your own suggestions.\nCustomers are looking for concrete information that is available in the Tyson\nFoodservice database.\n\n## About Tyson Foodservice\nTyson Foods is a major American multinational corporation and one of the world's\nlargest processors and marketers of chicken, beef, and pork.\n\n### Distributors\nTyson foods mainly sells their products through distributors and does not sell\nthem directly. Each distributor is identified by a unique identifier named\ndistributor_id which is used as parameters for the tools, do not use the\ndistributor name as query parameter.\n\n### Foodservice Operators\nFoodservice Operators, or simply Operators, are Tyson Foods' primary customers.\nThese encompass diverse businesses in the foodservice sector, each with unique\nneeds. Understanding the distinct personas of various Operator types is crucial\nfor Tyson Foods to:\n- Tailor product offerings effectively\n- Develop targeted marketing strategies\n- Create relevant recipe suggestions\n- Address specific operational challenges\nBy analyzing different Operator segments (e.g., quick-service restaurants, fine\ndining, educational institutions, healthcare facilities), Tyson Foods can\ncustomize its products, offer innovative menu solutions, and provide value-added\nservices. This approach positions Tyson Foods as a strategic partner, driving\ngrowth and maintaining competitiveness in the foodservice industry.\n\n## Using Tools\nYou will be provide a variety of tools to perform your job, use them wisely and\nask the customer for relevant information that they have not provided. E.g. if\nthe search tool requires persona and the customer has not provided it then ask\nthe customer.\n- Do not explicitly declare the tools to the users as the users are not aware of\n  the internal workings of the tools.\n- Do not try to intrepret the results of the search tool and show them as it is\n  to the user.\n- Operators may have their preferred distributor they buy from so let them\n  confirm or select their distributor before checking for availability of\n  products.\n- Customers might sometimes search for things that are not available in tyson\n  food catalog. If the search did not produce any results then just inform the\n  user and do not suggest any external sources.\n- When trying to determine the parameters for a tool, do not infer them from\n  other parameters. E.g. do not infer the User's name from their email.\n  Explicitly ask for the name.\n- If the users complain or praise the chatbot then you can ask for their\n  feedback in the chatbot and use the `submit_feedback` tool to submit the\n  feedback. Ask the user to provide the relevant contact information.\n\n## Product, Recipes, and Articles Search\nSearch functionality is a critical tool on Tyson's website, allowing users to\nfind products, recipes, and articles. It enables searches across three main\nentity types:\n- **Products**: The core offerings of Tyson Foods. These are identified by a\n  unique GTIN (Global Trade Item Number).\n- **Recipes**: Culinary ideas provided by Tyson Foods to encourage product use.\n  Each recipe incorporates one or more Tyson products.\n- **Articles**: Informative content on various topics, created by Tyson Foods\n  for their customers.\n- Do not provide any items or suggestions outside of the ones that are found\n  through search.\n- When the user asks to for details or a product or compare two or more\n  products, retrieve the details of the products first using the tools to get\n  product details.\n- While users of the site are mainly looking for products, they might also be\ninterested in recipes and articles so it's important to not omit them when\ndisplaying the search results.\n\n### User Profile or Persona\nIn order to serve the user's better, the search tool can accept the user's\npersona as an input. User profile or persona is a concise description of the\ntype of role that a user performs in the foodservice industry. A few examples\nof persona are\n- Restaurant owners looking to optimize costs\n- Chef looking for unique ingredients\n- K12 operators looking for healthy menu items\nThey can also be simple roles if the user has not provided any additional\ninformation. Examples are\n- Restaurant owner\n- Chef\n- Hotel Manager\nThe user persona should not include the search query that they are using for\nfinding products E.g. these are not good personas\n- Restaurant owner looking for chicken nuggets\nThe above is not a good persona because it includes the product\n\n### Search query string\nSearch queries should be simple and specific to the products or recipes and\nshould not contain the operator information\nHere are some examples: \n- Instead of \"healthy chicken wings for K12\" use \"chicken wings\"\n- Instead of \"mexican beef patties for Deli operation\" use \"mexican beef\n  patties\"\n\n### Product Results Display \nWhen listing the product results, always display them in the following format as\na numbered list. This will be displayed in the UI using markdown. \n1. **Title**\n- GTIN\n- description - This is a brief description\n- [Product Page](Product url link)\n\n### Recipes Results Display\nWhen displaying recipes. Display the following\n1. **Title**\n- description - This is a brief description\n- [Recipe Page](Recipe url link)\n\n## Contact or provide feedback\n- If the users want to reach out to Tyson foods team then they can use the form\n  using this link [Contact\n  Us](https://www.tysonfoodservice.com/connect/contact-us) \n- Users can submit their feedback using the chatbot using tools. When submitting\n  feedback to Tyson extract user's message verbatim and do not rephrase it.\n\n## How to buy\nIf the user wants to buy a product then they have two options. \n1. through distributor (preferred option)\n2. reaching out to tysons sales representative by filling a form\nIf the user has not already indicated their preference then present these two\noptions. \nWhen the user asks for ordering information you do not need to retrieve all the\nproduct details again, only specify the title of the product and be concise with\nthe details.\n\n### Order through distributor\nIf they user is interested in buying through a distributor then let them\nidentify their preferred distributor and then for a specific product or products\nthey have identified provide the ordering link obtained through the user of\nappropriate tool. Also help them check if a product is available with their\ndistributor.\n\n### Find a tyson Sales Rep\nIf the user is not interested in a purchasing through a distributor then direct\nthem to submit a form through this link which will submit their information to a\nsales team and someone will reach out to them. Here is the link to the form\n<a href=\"https://www.tysonfoodservice.com/connect/find-a-sales-rep\" rel=\"noopener noreferrer\">https://www.tysonfoodservice.com/connect/find-a-sales-rep</a> \n\nCurrent date (YYYY-MM-DD): \"\"\" + date.today().strftime(\"%Y-%m-%d\") + \"\\n\" \n</code></pre> \n<h2>Capturing high-value actions: Turning conversations into insights</h2> \n<p>In designing Tyson Foodservice’s AI assistant, we implemented an innovative solution for capturing high-value actions that transforms customer interactions into strategic business intelligence. This capability provides deeper contextual understanding of customer interests and needs than traditional web analytics. Whereas conventional analytics tools track user behavior through page views, clicks, and time-on-site metrics, our solution uses the rich conversational data generated through natural dialogue. This provides Tyson with unprecedented visibility into customer interests, pain points, and purchase intentions.</p> \n<p>The system identifies and logs specific high-value interactions whenever users request detailed product information, inquire about specific product categories, ask about preparation methods or recipe ideas, seek distributor information in their region, or express interest in bulk purchasing or promotions. This approach creates a powerful feedback loop for Tyson Foodservice. As customers naturally express their needs and interests through conversation, the system captures these signals in an aggregate, privacy-respecting manner. Tyson can use these insights to identify trending product categories and potential gaps in their portfolio, understand regional variations in customer interests, recognize seasonal patterns in product inquiries, refine marketing strategies based on direct customer language, and improve inventory management through better demand forecasting. The technical implementation uses the tool-calling capabilities of Anthropic’s Claude 3.5 Sonnet in a straightforward but effective way. Rather than analyzing chat logs after the fact, we integrated the capture mechanism directly into the AI assistant’s operational workflow through LangGraph, allowing for real-time insight collection during customer interactions. When the LLM invokes certain tools to retrieve information requested by users, these tool calls simultaneously trigger the capture of high-value action data. We’ve designed a configurable system where specific tools are designated as high-value action triggers that record meaningful interactions while fulfilling the user’s immediate request.This dual-purpose approach makes sure that valuable business intelligence is gathered as a natural byproduct of providing excellent customer service, without requiring additional processing or analysis steps. The system includes configurable parameters that allow Tyson to adjust which user intents and actions qualify as high value based on evolving business priorities. By transforming every customer conversation into structured, actionable data, Tyson Foodservice can now measure customer interest with unprecedented precision while delivering a superior search experience that feels natural to users.</p> \n<h2>Conclusion</h2> \n<p>In this post, we demonstrated a powerful approach to implementing natural conversational AI assistants that seamlessly integrate with existing website functionalities and provide intuitive language interactions for users. By using Amazon Bedrock FMs and OpenSearch Serverless, businesses can quickly expose their website’s capabilities through conversation rather than complex interfaces. The high-value action capture mechanism further enhances this solution by gathering valuable customer insights directly from natural interactions, creating a rich source of business intelligence without additional user friction. This framework provides a flexible blueprint for implementing AI-powered assistants across retail and CPG websites. Organizations can adapt this approach to their specific needs, such as product discovery, customer support, or personalized recommendations. The combination of semantic search with conversational AI creates experiences that understand user intent while maintaining the context necessary for natural dialogue.</p> \n<p>If you’re interested in building a similar AI assistant that orchestrates multiple tools, you can get started with <a href=\"https://aws.amazon.com/bedrock/agents/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Agents</a>, a fully managed AWS solution designed specifically for this purpose. Amazon Bedrock Agents simplifies the process of creating, testing, and deploying conversational experiences that can execute complex tasks across your business systems. With the right architecture and implementation approach demonstrated in this post, you can develop AI-powered interactions that deliver measurable business value while significantly enhancing your customer journey.</p> \n<p>For developers exploring AI agent frameworks today, AWS recently introduced <a href=\"https://aws.amazon.com/blogs/opensource/introducing-strands-agents-an-open-source-ai-agents-sdk/\" rel=\"noopener noreferrer\" target=\"_blank\">Strands Agents</a>, an open source SDK that takes a model-driven approach to building agents with just a model, tools, and a prompt. Unlike workflow-based frameworks, Strands adopts a model-first philosophy that uses advanced reasoning capabilities, offering an interesting alternative approach to frameworks like LangGraph.</p> \n<p>Try out these solutions for your own use case, and share your feedback in the comments.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-113447 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/anveshi.jpeg\" width=\"100\" /> Anveshi Charuvaka </strong>is a Senior Applied Scientist at AWS’s Generative AI Innovation Center, where he partners with customers to turn Generative AI into solutions for mission-critical business problems. He holds a PhD in Machine Learning and brings over 10 years of experience applying innovative ML and GenAI techniques to complex, real-world challenges.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-full wp-image-113448 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/barret.jpg\" width=\"100\" /><strong>Barret Miller</strong> leads the Digital Enterprise Organization at Tyson Foods, where he spearheads progress in emerging technologies, artificial intelligence, and Smart Office initiatives. With more than 17 years of expertise in software development, data, analytics, and AI, Barret excels at leveraging innovative technology paradigms, including Agentic AI, to tackle and enhance complex business processes.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-full wp-image-113451 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/vincil.png\" width=\"100\" /> <strong>Vincil Bishop</strong> is a Senior Deep Learning Architect in the Generative AI Innovation Center. Vincil has 25 years of experience in the IT industry and holds a PhD in Systems Engineering from Colorado State University. Vincil specializes in the design and implementation of AI solutions that help solve customers’ toughest business challenges.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-full wp-image-113450 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/tes.png\" width=\"100\" /> <strong>Tesfagabir Meharizghi</strong> is an Applied Scientist at the AWS Generative AI Innovation Center, where he leads projects and collaborates with enterprise customers across various industries to leverage cutting-edge generative AI technologies in solving complex business challenges. He specializes in identifying and prioritizing high-impact use cases, developing scalable AI solutions, and fostering knowledge-sharing partnerships with stakeholders.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-full wp-image-113449 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/tanay.png\" width=\"100\" />&nbsp; <strong>Tanay Chowdhury</strong> is a Data Scientist at Generative AI Innovation Center at Amazon Web Services who helps customers solve their business problems using generative AI and machine learning. He has done MS with Thesis in Machine Learning from University of Illinois and has extensive experience in solving customer problem in the field of data science.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-medium wp-image-113459 alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/angona.jpg\" width=\"100\" /> <strong>Angel Goni</strong> is a Principal Solutions Architect at AWS with 15+ years of IT experience across the Financial Services, Retail, and Consumer Packaged Goods sectors. Angel specializes in utilizing cloud technology to impact business KPIs, with particular expertise in multicloud strategies, SAP migrations, and supply chain improvement.</p>"
        }
      ]
    },
    {
      "title": "Enhance AI agents using predictive ML models with Amazon SageMaker AI and Model Context Protocol (MCP)",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Enhance AI agents using predictive ML models with Amazon SageMaker AI and Model Context Protocol (MCP)"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/enhance-ai-agents-using-predictive-ml-models-with-amazon-sagemaker-ai-and-model-context-protocol-mcp/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/enhance-ai-agents-using-predictive-ml-models-with-amazon-sagemaker-ai-and-model-context-protocol-mcp/",
      "authors": [
        {
          "name": "Saptarshi Banerjee"
        }
      ],
      "author": "Saptarshi Banerjee",
      "author_detail": {
        "name": "Saptarshi Banerjee"
      },
      "published": "Wed, 20 Aug 2025 20:26:08 +0000",
      "published_parsed": [
        2025,
        8,
        20,
        20,
        26,
        8,
        2,
        232,
        0
      ],
      "tags": [
        {
          "term": "Advanced (300)",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Machine Learning",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "Artificial Intelligence",
          "scheme": null,
          "label": null
        },
        {
          "term": "Generative AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "AI/ML",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker",
          "scheme": null,
          "label": null
        }
      ],
      "id": "6e3e7620c7486a30cc17e6b1991f6f9d4b44a9e3",
      "guidislink": false,
      "summary": "In this post, we demonstrate how to enhance AI agents’ capabilities by integrating predictive ML models using Amazon SageMaker AI and the MCP. By using the open source Strands Agents SDK and the flexible deployment options of SageMaker AI, developers can create sophisticated AI applications that combine conversational AI with powerful predictive analytics capabilities.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we demonstrate how to enhance AI agents’ capabilities by integrating predictive ML models using Amazon SageMaker AI and the MCP. By using the open source Strands Agents SDK and the flexible deployment options of SageMaker AI, developers can create sophisticated AI applications that combine conversational AI with powerful predictive analytics capabilities."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p><a href=\"https://aws.amazon.com/ai/machine-learning/\" rel=\"noopener noreferrer\" target=\"_blank\">Machine learning</a> (ML) has evolved from an experimental phase to becoming an integral part of business operations. Organizations now actively deploy ML models for precise sales forecasting, customer segmentation, and churn prediction. While traditional ML continues to transform business processes, <a href=\"https://aws.amazon.com/generative-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">generative AI</a> has emerged as a revolutionary force, introducing powerful and accessible tools that reshape customer experiences.</p> \n<p>Despite generative AI’s prominence, traditional ML solutions remain essential for specific predictive tasks. Sales forecasting, which depends on historical data and trend analysis, is most effectively handled by established ML algorithms including random forests, gradient boosting machines (like XGBoost), autoregressive integrated moving average (ARIMA) models, long short-term memory (LSTM) networks, and linear regression techniques. Traditional ML models, such as K-means and hierarchical clustering, also excel in customer segmentation and churn prediction applications. Although generative AI demonstrates exceptional capabilities in creative tasks such as content generation, product design, and personalized customer interactions, traditional ML models maintain their superiority in data-driven predictions. Organizations can achieve optimal results by using both approaches together, creating solutions that deliver accurate predictions while maintaining cost efficiency.</p> \n<p>To achieve this, we showcase in this post how customers can expand AI agents’ capabilities by integrating predictive ML models and <a href=\"https://modelcontextprotocol.io/introduction\" rel=\"noopener noreferrer\" target=\"_blank\">Model Context Protocol (MCP)</a>—an open protocol that standardizes how applications provide context to <a href=\"https://aws.amazon.com/what-is/large-language-model/\" rel=\"noopener noreferrer\" target=\"_blank\">large language models</a> (LLMs)—on <a href=\"https://aws.amazon.com/sagemaker-ai\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker AI</a>. We demonstrate a comprehensive workflow that enables AI agents to make data-driven business decisions by using ML models hosted SageMaker. Through the use of <a href=\"https://strandsagents.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Strands Agents SDK</a>—an open source SDK that takes a model-driven approach to building and running AI agents in only a few lines of code—and flexible integration options, including direct endpoint access and MCP, we show you how to build intelligent, scalable AI applications that combine the power of conversational AI with predictive analytics.</p> \n<h2>Solution overview</h2> \n<p>This solution enhances AI agents by having ML models deployed on Amazon SageMaker AI endpoints integrate with AI Agents, to enable them to make data-driven business decisions through ML predictions. An AI agent is an LLM-powered application that uses an LLM as its core “brain” to autonomously observe its environment, plan actions, and execute tasks with minimal human input. It integrates reasoning, memory, and tool use to perform complex, multistep problem-solving by dynamically creating and revising plans, interacting with external systems, and learning from past interactions to optimize outcomes over time. This enables AI agents to go beyond simple text generation, acting as independent entities capable of decision-making and goal-directed actions in diverse real-world and enterprise scenarios.For this solution, the AI agent is developed using the Strands Agents SDK, which allows for rapid development from simple assistants to complex workflows. Predictive ML models are hosted on Amazon SageMaker AI and will be used as tools by the AI agent. This can happen in two ways: agents can directly invoke SageMaker endpoints for more direct access to model inference capabilities or use the MCP protocol to facilitate the interaction between AI agents and the ML models. Both options are valid: direct tool invocation doesn’t require additional infrastructure by embedding the tool calling directly in the agent code itself, whereas MCP enables dynamic discovery of the tools and decoupling of agent and tool execution through the introduction of an additional architectural component, the MCP server itself. For scalable and secure implementation of the tool calling logic, we recommend the MCP approach. Although we’re recommending MCP, we discuss and implement the direct endpoint access as well, to give readers the freedom to choose the approach that they prefer.</p> \n<p>Amazon SageMaker AI offers two methods to host multiple models behind a single endpoint: inference components and multi-model endpoints. This consolidated hosting approach enables efficient deployment of multiple models in one environment, which optimizes computing resources and minimizes response times for model predictions. For demonstration purposes, this post deploys only one model on one endpoint. If you want to learn more about inference components, refer to the Amazon SageMaker AI documentation <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html#deployed-shared-utilization\" rel=\"noopener noreferrer\" target=\"_blank\">Shared resource utilization with multiple models</a>. To learn more about multi-model endpoints, refer to the Amazon SageMaker AI documentation <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html\" rel=\"noopener noreferrer\" target=\"_blank\">Multi-model endpoints</a>.</p> \n<h2>Architecture</h2> \n<p>In this post, we define a workflow for empowering AI agents to make data-driven business decisions by invoking predictive ML models using Amazon SageMaker AI. The process begins with a user interacting through an interface, such as a chat-based assistant or application. This input is managed by an AI agent developed using the open source Strands Agents SDK. Strands Agents adopts a model-driven approach, which means developers define agents with only a prompt and a list of tools, facilitating rapid development from simple assistants to complex autonomous workflows.</p> \n<p>When the agent is prompted with a request that requires a prediction (for example, “what will be the sales for H2 2025?”), the LLM powering the agent decided to interact with the Amazon SageMaker AI endpoint hosting the ML model. This can happen in two ways: directly using the endpoint as a custom tool of the Strands Agents Python SDK or by calling the tool through MCP. With MCP, the client application can discover the tools exposed by the MCP server, obtain the list of required parameters, and format the request to the Amazon SageMaker inference endpoint. Alternatively, agents can directly invoke SageMaker endpoints using tool annotations (such as <code>@tool</code>), bypassing the MCP server for more direct access to model inference capabilities.</p> \n<p>Finally, the prediction generated by the SageMaker hosted model is routed back through the agent and ultimately delivered to the user interface, enabling real-time, intelligent responses.</p> \n<p>The following diagram illustrates this process. The complete code for this solution is available on <a href=\"https://github.com/dgallitelli/generative-ai-on-amazon-sagemaker/tree/main/workshops/diy-agents-with-sagemaker-and-bedrock/99-use-cases/sagemaker-endpoint-as-tool\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub</a>.</p> \n<p><img alt=\"A diagram of a software process AI-generated content may be incorrect.\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/06/image-1-8.png\" /></p> \n<h2>Prerequisites</h2> \n<p>To get started with this solution, make sure you have:</p> \n<ul> \n <li>An AWS account that will contain all your AWS resources.</li> \n <li>An <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) role to access SageMaker AI. To learn more about how IAM works with SageMaker AI, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Identity and Access Management for Amazon SageMaker AI</a>.</li> \n <li>Access to <a href=\"https://aws.amazon.com/sagemaker/studio/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker Studio</a> and a SageMaker AI notebook instance or an interactive development environment (IDE) such as <a href=\"https://www.jetbrains.com/pycharm/\" rel=\"noopener noreferrer\" target=\"_blank\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Visual Studio Code</a>. We recommend using SageMaker Studio for straightforward deployment and inference.</li> \n <li>Access to accelerated instances (GPUs) for hosting the LLMs.</li> \n</ul> \n<h2>Solution implementation</h2> \n<p>In this solution, we implement a complete workflow that demonstrates how to use ML models deployed on Amazon SageMaker AI as specialized tools for AI agents. This approach enables agents to access and use ML capabilities for enhanced decision-making without requiring deep ML expertise. We play the role of a data scientist tasked with building an agent that can predict demand for one product. To achieve this, we train a time-series forecasting model, deploy it, and expose it to an AI agent.</p> \n<p>The first phase involves training a model using Amazon SageMaker AI. This begins with preparing training data by generating synthetic time series data that incorporates trend, seasonality, and noise components to simulate realistic demand patterns. Following data preparation, feature engineering is performed to extract relevant features from the time series data, including temporal features such as day of week, month, and quarter to effectively capture seasonality patterns. In our example, we train an XGBoost model using the XGBoost container available as 1P container in Amazon SageMaker AI to create a regression model capable of predicting future demand values based on historical patterns. Although we use XGBoost for this example because it’s a well-known model used in many use cases, you can use your preferred container and model, according to the problem you’re trying to solve. For the sake of this post, we won’t detail an end-to-end example of training a model using XGBoost. To learn more about this, we suggest checking out the documentation <a href=\"https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#use-xgboost-with-the-sagemaker-python-sdk\" rel=\"noopener noreferrer\" target=\"_blank\">Use XGBoost with the SageMaker Python SDK</a>. Use the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">from&nbsp;sagemaker.xgboost.estimator import&nbsp;XGBoost\n\nxgb_estimator = XGBoost(...)\nxgb_estimator.fit({'train': train_s3_path, 'validation': val_s3_path})</code></pre> \n</div> \n<p>Then, the trained model is packaged and deployed to a SageMaker AI endpoint, making it accessible for real-time inference through API calls:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">predictor = xgb_estimator.deploy(\n &nbsp; &nbsp;initial_instance_count=1,\n &nbsp; &nbsp;instance_type=instance_type,\n &nbsp; &nbsp;serializer=JSONSerializer(),\n &nbsp; &nbsp;deserializer=JSONDeserializer()\n)</code></pre> \n</div> \n<p>After the model is deployed and ready for inferences, you need to learn how to invoke the endpoint. To invoke the endpoint, write a function like this:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-ruby\">ENDPOINT_NAME = \"serverless-xgboost\"\nREGION = boto3.session.Session().region_name\n\ndef invoke_endpoint(payload: list):\n&nbsp;&nbsp; &nbsp;\"\"\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Use the model deployed on the Amazon SageMaker AI endpoint to generate predictions.\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Args:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;payload: a list of lists containing the inputs to generate predictions from\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Returns:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;predictions: an NumPy array of predictions\n&nbsp;&nbsp; &nbsp;\"\"\"\n&nbsp;&nbsp; &nbsp;sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=REGION)\n&nbsp;&nbsp; &nbsp;response = sagemaker_runtime.invoke_endpoint(\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;EndpointName=ENDPOINT_NAME,\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Body=json.dumps(payload),\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ContentType=\"application/json\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Accept=\"application/json\"\n&nbsp;&nbsp; &nbsp;)\n&nbsp;&nbsp; &nbsp;predictions = json.loads(response['Body'].read().decode(\"utf-8\"))\n&nbsp;&nbsp; &nbsp;return np.array(predictions)</code></pre> \n</div> \n<p>Note that the function <code>invoke_endpoint()</code> has been written with proper docstring. This is key to making sure that it can be used as a tool by LLMs because the description is what allows them to choose the right tool for the right task. YOu can turn this function into a Strands Agents tool thanks to the <code>@tool</code> decorator:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">from&nbsp;strands import&nbsp;tool\n\n@tool()\ndef&nbsp;invoke_endpoint(payload: list):\n&nbsp; &nbsp; ....</code></pre> \n</div> \n<p>And to use it, pass it to a Strands agent:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">from&nbsp;strands import&nbsp;Agent\n\nagent = Agent(\n&nbsp; &nbsp; model=\"us.amazon.nova-pro-v1:0\", \n&nbsp;&nbsp;&nbsp;&nbsp;tools=[generate_prediction_with_sagemaker]\n)\n\nagent(\n&nbsp;&nbsp; &nbsp;\"Invoke the endpoint with this input:\\n\\n\"\n&nbsp;&nbsp; &nbsp;f\"&lt;input&gt;{test_sample}&lt;/input&gt;\\n\\n\"\n&nbsp;&nbsp; &nbsp;\"Provide the output in JSON format {'predictions':&lt;predictions&gt;}\"\n)</code></pre> \n</div> \n<p>As you run this code, you can confirm the output from the agent, which correctly identifies the need to call the tool and executes the function calling loop:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">&lt;thinking&gt; To fulfill the User's request, I need to invoke the Amazon SageMaker \nendpoint with the provided input data. The input is a list of lists, which is the \nrequired format for the 'generate_prediction_with_sagemaker' tool. I will use this \ntool to get the predictions. &lt;/thinking&gt; \n\nTool #1: generate_prediction_with_sagemaker The predictions from the Amazon SageMaker\nendpoint are as follows: \n```json {&nbsp; \"predictions\": [89.8525238, 52.51485062, 58.35247421, 62.79786301, 85.51475525] } ```</code></pre> \n</div> \n<p>As the agent receives the prediction result from the endpoint tool, it can then use this as an input for other processes. For example, the agent could write the code to create a plot based on these predictions and show it to the user in the conversational UX. It could send these values directly to business intelligence (BI) tools such as <a href=\"https://aws.amazon.com/quicksight\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon QuickSight</a> or <a href=\"http://www.tableau.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Tableau</a> and automatically update enterprise resource planning (ERP) or customer relationship management (CRM) tools such as <a href=\"http://www.sap.com/\" rel=\"noopener noreferrer\" target=\"_blank\">SAP</a> or <a href=\"http://www.salesforce.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Salesforce</a>.</p> \n<h3>Connecting to the endpoint through MCP</h3> \n<p>You can further evolve this pattern by having an MCP server invoke the endpoint rather than the agent itself. This allows for the decoupling of agent and tool logic and an improved security pattern because the MCP server will be the one with the permission to invoke the endpoint. To achieve this, implement an MCP server using the <a href=\"https://gofastmcp.com/getting-started/welcome?gad_source=1&amp;gad_campaignid=22521620347&amp;gbraid=0AAAAACeCpg_Hi0k3Ql_OeCU0q96xoSh9M&amp;gclid=CjwKCAjwprjDBhBTEiwA1m1d0n-o5wCNkDPbAbxBPgTP5-ui-wwO_LZaNqvVmUKj-1QsJ9SjpEBh-xoCJnwQAvD_BwE\" rel=\"noopener noreferrer\" target=\"_blank\">FastMCP</a> framework that wraps the SageMaker endpoint and exposes it as a tool with a well-defined interface. A tool schema must be specified that clearly defines the input parameters and return values for the tool, facilitating straightforward understanding and usage by AI agents. Writing the proper docstring when defining the function achieves this. Additionally, the server must be configured to handle authentication securely, allowing it to access the SageMaker endpoint using AWS credentials or AWS roles. In this example, we run the server on the same compute as the agent and use <code>stdio</code> as communication protocol. For production workloads, we recommend running the MCP server on its own AWS compute and using transport protocols based on HTTPS (for example, Streamable HTTP). If you want to learn how to deploy MCP servers in a serverless fashion, refer to <a href=\"https://github.com/awslabs/run-model-context-protocol-servers-with-aws-lambda\" rel=\"noopener noreferrer\" target=\"_blank\">this official AWS GitHub repository</a>. Here’s an example MCP server:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-php\">from&nbsp;mcp.server.fastmcp import&nbsp;FastMCP\n\nmcp =&nbsp;FastMCP(\"SageMaker App\")\nENDPOINT_NAME =&nbsp;os.environ[\"SAGEMAKER_ENDPOINT_NAME\"]\n\n@mcp.tool()\nasync&nbsp;def&nbsp;invoke_endpoint(payload: list):\n&nbsp;&nbsp; &nbsp;\"\"\"&nbsp;Use the model ... \"\"\"\n&nbsp; &nbsp; [...]\n&nbsp;&nbsp;&nbsp;&nbsp;\nif&nbsp;__name__&nbsp;==&nbsp;\"__main__\":\n&nbsp;&nbsp; &nbsp;mcp.run(=\"stdio\")</code></pre> \n</div> \n<p>Finally, integrate the ML model with the agent framework. This begins with setting up Strands Agents to establish communication with the MCP server and incorporate the ML model as a tool. A comprehensive workflow must be created to determine when and how the agent should use the ML model to enhance its capabilities. The implementation includes programming decision logic that enables the agent to make informed decisions based on the predictions received from the ML model. The phase concludes with testing and evaluation, where the end-to-end workflow is validated by having the agent generate predictions for test scenarios and take appropriate actions based on those predictions.</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from&nbsp;mcp import&nbsp;StdioServerParameters\nfrom&nbsp;mcp.client.stdio import&nbsp;stdio_client\nfrom&nbsp;strands.tools.mcp import&nbsp;MCPClient\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n&nbsp;&nbsp; &nbsp;command=\"python3\", &nbsp;# Executable\n&nbsp;&nbsp; &nbsp;args=[\"server.py\"], &nbsp;# Optional command line arguments\n&nbsp;&nbsp; &nbsp;env={\"SAGEMAKER_ENDPOINT_NAME\":&nbsp;\"&lt;your-endpoint-name&gt;\"}\n)\n\n# Create an agent with MCP tools\nwith stdio_mcp_client:\n&nbsp;&nbsp; &nbsp;# Get the tools from the MCP server\n&nbsp;&nbsp; &nbsp;tools = stdio_mcp_client.list_tools_sync()\n&nbsp;&nbsp; &nbsp;# Create an agent with these tools\n&nbsp;&nbsp; &nbsp;agent = Agent(model=\"us.amazon.nova-pro-v1:0\", tools=tools)\n&nbsp;&nbsp; &nbsp;# Invoke the agent\n&nbsp;&nbsp; &nbsp;agent(\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"Invoke the endpoint with this input:\\n\\n\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;f\"&lt;input&gt;{test_sample}&lt;/input&gt;\\n\\n\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"Provide the output in JSON format {'predictions':&lt;predictions&gt;}\"\n&nbsp;&nbsp; &nbsp;)</code></pre> \n</div> \n<h2>Clean up</h2> \n<p>When you’re done experimenting with the Strands Agents Python SDK and models on Amazon SageMaker AI, you can delete the endpoint you’ve created to stop incurring unwanted charges. To do that, you can use either the <a href=\"https://aws.amazon.com/console/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Management Console</a>, the <a href=\"https://sagemaker.readthedocs.io/en/stable/\" rel=\"noopener noreferrer\" target=\"_blank\">SageMaker Python SDK</a>, or the <a href=\"https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS SDK for Python (boto3)</a>:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\"># SageMaker Python SDK\npredictor.delete_model()\npredictor.delete_endpoint()\n\n# Alternatively, boto3\nsagemaker_runtime.delete_endpoint(EndpointName=endpoint_name)</code></pre> \n</div> \n<h2>Conclusion</h2> \n<p>In this post, we demonstrated how to enhance AI agents’ capabilities by integrating predictive ML models using Amazon SageMaker AI and the MCP. By using the open source Strands Agents SDK and the flexible deployment options of SageMaker AI, developers can create sophisticated AI applications that combine conversational AI with powerful predictive analytics capabilities. The solution we presented offers two integration paths: direct endpoint access through tool annotations and MCP-based integration, giving developers the flexibility to choose the most suitable approach for their specific use cases. Whether you’re building customer service chat assistants that need predictive capabilities or developing complex autonomous workflows, this architecture provides a secure, scalable, and modular foundation for your AI applications. As organizations continue to seek ways to make their AI agents more intelligent and data-driven, the combination of Amazon SageMaker AI, MCP, and the Strands Agents SDK offers a powerful solution for building the next generation of AI-powered applications.</p> \n<p>For readers unfamiliar with connecting MCP servers to workloads running on Amazon SageMaker AI, we suggest <a href=\"https://aws.amazon.com/blogs/machine-learning/extend-large-language-models-powered-by-amazon-sagemaker-ai-using-model-context-protocol/\" rel=\"noopener noreferrer\" target=\"_blank\">Extend large language models powered by Amazon SageMaker AI using Model Context Protocol</a> in the AWS Artificial Intelligence Blog, which details the flow and the steps required to build agentic AI solutions with Amazon SageMaker AI.</p> \n<p>To learn more about AWS commitment to the MCP standard, we recommend reading <a href=\"https://aws.amazon.com/blogs/opensource/open-protocols-for-agent-interoperability-part-1-inter-agent-communication-on-mcp/\" rel=\"noopener noreferrer\" target=\"_blank\">Open Protocols for Agent Interoperability Part 1: Inter-Agent Communication on MCP</a> in the AWS Open Source Blog, where we announced that AWS is joining the steering committee for MCP to make sure developers can build breakthrough agentic applications without being tied to one standard. To learn more about how to use MCP with other technologies from AWS, such as <a href=\"https://aws.amazon.com/bedrock/agents/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Agents</a>, we recommend reading <a href=\"https://aws.amazon.com/blogs/machine-learning/harness-the-power-of-mcp-servers-with-amazon-bedrock-agents/\" rel=\"noopener noreferrer\" target=\"_blank\">Harness the power of MCP servers with Amazon Bedrock Agents</a> in the AWS Artificial Intelligence Blog. Finally, a great way to securely deploy and scale MCP servers on AWS is provided in the AWS Solutions Library at <a href=\"https://aws.amazon.com/solutions/guidance/deploying-model-context-protocol-servers-on-aws/\" rel=\"noopener noreferrer\" target=\"_blank\">Guidance for Deploying Model Context Protocol Servers on AWS</a>.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113976 size-full alignleft\" height=\"121\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/10/saptarshi.jpeg\" width=\"100\" /><strong>Saptarshi Banerjee </strong>serves as a Senior Solutions Architect at AWS, collaborating closely with AWS Partners to design and architect mission-critical solutions. With a specialization in generative AI, AI/ML, serverless architecture, Next-Gen Developer Experience tools and cloud-based solutions, Saptarshi is dedicated to enhancing performance, innovation, scalability, and cost-efficiency for AWS Partners within the cloud ecosystem.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-full wp-image-4648 alignleft\" height=\"125\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/06/image-3-5.png\" width=\"100\" /><strong>Davide Gallitelli </strong>is a Senior Worldwide Specialist Solutions Architect for Generative AI at AWS, where he empowers global enterprises to harness the transformative power of AI. Based in Europe but with a worldwide scope, Davide partners with organizations across industries to architect custom AI agents that solve complex business challenges using AWS ML stack. He is particularly passionate about democratizing AI technologies and enabling teams to build practical, scalable solutions that drive organizational transformation.</p>"
        }
      ]
    },
    {
      "title": "Simplify access control and auditing for Amazon SageMaker Studio using trusted identity propagation",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Simplify access control and auditing for Amazon SageMaker Studio using trusted identity propagation"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/simplify-access-control-and-auditing-for-amazon-sagemaker-studio-using-trusted-identity-propagation/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/simplify-access-control-and-auditing-for-amazon-sagemaker-studio-using-trusted-identity-propagation/",
      "authors": [
        {
          "name": "Durga Sury"
        }
      ],
      "author": "Durga Sury",
      "author_detail": {
        "name": "Durga Sury"
      },
      "published": "Tue, 19 Aug 2025 20:00:45 +0000",
      "published_parsed": [
        2025,
        8,
        19,
        20,
        0,
        45,
        1,
        231,
        0
      ],
      "tags": [
        {
          "term": "Advanced (300)",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon SageMaker Studio",
          "scheme": null,
          "label": null
        },
        {
          "term": "AWS IAM Identity Center",
          "scheme": null,
          "label": null
        },
        {
          "term": "Technical How-to",
          "scheme": null,
          "label": null
        }
      ],
      "id": "a2c8109d46394c581954ccf9b60dc68943f6f167",
      "guidislink": false,
      "summary": "In this post, we explore how to enable and use trusted identity propagation in Amazon SageMaker Studio, which allows organizations to simplify access management by granting permissions to existing AWS IAM Identity Center identities. The solution demonstrates how to implement fine-grained access controls based on a physical user's identity, maintain detailed audit logs across supported AWS services, and support long-running user background sessions for training jobs.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we explore how to enable and use trusted identity propagation in Amazon SageMaker Studio, which allows organizations to simplify access management by granting permissions to existing AWS IAM Identity Center identities. The solution demonstrates how to implement fine-grained access controls based on a physical user's identity, maintain detailed audit logs across supported AWS services, and support long-running user background sessions for training jobs."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>AWS supports <a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/trustedidentitypropagation-overview.html\" rel=\"noopener noreferrer\" target=\"_blank\">trusted identity propagation</a>, a feature that allows AWS services to securely propagate a user’s identity across service boundaries. With trusted identity propagation, you have fine-grained access controls based on a physical user’s identity rather than relying on IAM roles. This integration allows for the implementation of access control through services such as <a href=\"https://aws.amazon.com/s3/features/access-grants/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon S3 Access Grants</a> and maintains detailed audit logs of user actions across supported AWS services such as <a href=\"https://aws.amazon.com/emr/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon EMR</a>. Furthermore, it supports long-running user background sessions for training jobs, so you can log out of your interactive ML application while the background job continues to run.</p> \n<p><a href=\"https://aws.amazon.com/sagemaker-ai/studio/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker Studio</a> now supports trusted identity propagation, offering a powerful solution for enterprises seeking to enhance their ML system security. By integrating trusted identity propagation with SageMaker Studio, organizations can simplify access management by granting permissions to existing <a href=\"https://aws.amazon.com/iam/identity-center/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS IAM Identity Center</a>&nbsp;identities.</p> \n<p>In this post, we explore how to enable and use trusted identity propagation in SageMaker Studio, demonstrating its benefits through practical use cases and implementation guidelines. We walk through the setup process, discuss key considerations, and showcase how this feature can transform your organization’s approach to security and access controls.</p> \n<h2>Solution overview</h2> \n<p>In this section, we review the architecture for the proposed solution and the steps to enable trusted identity propagation for your SageMaker Studio domain.</p> \n<p>The following diagram shows the interaction between the different components that allow the user’s identity to propagate from their identity provider and IAM Identity Center to downstream services such as Amazon EMR and <a href=\"https://aws.amazon.com/athena/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Athena</a>.</p> \n<p><img alt=\"architecture diagram showing trusted identity propagation between multiple aws services\" class=\"alignnone size-full wp-image-114633\" height=\"534\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/18/ml-19006-arch-diagram-1.png\" width=\"936\" /></p> \n<p>With a trusted identity propagation-enabled SageMaker Studio domain, users can access data across <a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/trustedidentitypropagation-integrations.html\" rel=\"noopener noreferrer\" target=\"_blank\">supported AWS services</a> using their end user identity and group membership, in addition to access allowed by their domain or user execution role. In addition, API calls from SageMaker Studio notebooks and supported AWS services and <a href=\"https://aws.amazon.com/sagemaker-ai\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SageMaker AI</a> features log the user identity in <a href=\"http://aws.amazon.com/cloudtrail\" rel=\"noopener noreferrer\" target=\"_blank\">AWS CloudTrail</a>. For a list of supported AWS services and SageMaker AI features, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/trustedidentitypropagation-compatibility.html\" rel=\"noopener noreferrer\" target=\"_blank\">Trusted identity propagation architecture and compatibility</a>. In the following sections, we show how to enable trusted identity propagation for your domain.</p> \n<p>This solution applies for SageMaker Studio domains set up using IAM Identity Center as the method of authentication. If your domain is set up using IAM, see <a href=\"https://aws.amazon.com/blogs/machine-learning/implement-user-level-access-control-for-multi-tenant-ml-platforms-on-amazon-sagemaker-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">Implement user-level access control for multi-tenant ML platforms on Amazon SageMaker AI</a> for best practices on managing and scaling access control.</p> \n<h2>Prerequisites</h2> \n<p>To follow along with this post, you must have the following:</p> \n<ul> \n <li>An AWS account with an organization instance of IAM Identity Center configured through <a href=\"https://aws.amazon.com/organizations/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Organizations</a></li> \n <li>Administrator permissions (or elevated permissions allowing modification of IAM principals, and SageMaker administrator access to create and update domains)</li> \n</ul> \n<h2>Create or update the SageMaker execution role</h2> \n<p>For trusted identity propagation to work, the SageMaker execution role (domain and user profile execution role), should allow the <code>sts:SetContext</code> permissions, in addition to <code>sts:AssumeRole</code>, in its trust policy. For a new SageMaker AI domain, create a domain execution role by following the instructions in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-create-execution-role\" rel=\"noopener noreferrer\" target=\"_blank\">Create execution role</a>. For existing domains, follow the instructions in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-get-execution-role\" rel=\"noopener noreferrer\" target=\"_blank\">Get your execution role</a> to find the user or domain’s execution role.</p> \n<p>Next, to update the trust policy for the role, complete the following steps:</p> \n<ol> \n <li>In the navigation pane of the IAM console, choose <strong>Roles</strong>.</li> \n <li>In the list of roles in your account, choose the domain or user execution role.</li> \n <li>On the <strong>Trust relationships</strong> tab, choose <strong>Edit trust policy</strong>.</li> \n <li>Update the trust policy with the following statement:</li> \n</ol> \n<div class=\"hide-language\"> \n <div class=\"hide-language\"> \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-json\">{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n     .....\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": [\n          \"sagemaker.amazonaws.com\",\n        ]\n      },\n      \"Action\": [\n        \"sts:AssumeRole\",\n        \"sts:SetContext\"\n      ],\n      \"Condition\": {\n\t\"aws:SourceAccount\": \"&lt;account&gt;\"\n         }\n       }\n    }\n  ]\n}</code></pre> \n  </div> \n </div> \n</div> \n<ol start=\"5\"> \n <li>Choose <strong>Update policy</strong> to save your changes.</li> \n</ol> \n<p>Trusted identity propagation only works for private spaces at the time of launch.</p> \n<h2>Create a SageMaker AI domain with trusted identity propagation enabled</h2> \n<p>SageMaker AI domains using IAM Identity Center for authentication can only be set up in the same AWS Region as the IAM Identity Center instance. To create a new SageMaker domain, follow the steps in <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-custom.html\" rel=\"noopener noreferrer\" target=\"_blank\">Use custom setup for Amazon SageMaker AI</a>. For <strong>Trusted identity propagation</strong>, select <strong>Enable trusted identity propagation for all users on this domain</strong>, and continue with the rest of the setup to create a domain and assign users and groups, choosing the role you created in the previous step.</p> \n<p><img alt=\"screenshot of a create domain workflow\" class=\"alignnone size-full wp-image-114634\" height=\"1339\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/18/ml-19006-create-domain-1.png\" width=\"1397\" /></p> \n<h2>Update an existing SageMaker AI domain</h2> \n<p>You can also update your existing SageMaker AI domain to enable trusted identity propagation. You can enable trusted identity propagation even while the domain or user has active SageMaker Studio applications. However, for the changes to be applied, the active applications must be restarted. You can use the <code>EffectiveTrustedIdentityPropagationStatus</code> field in the response to the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeApp.html\" rel=\"noopener noreferrer\" target=\"_blank\">DescribeApp</a> API for running applications to determine if the application has trusted identity propagation enabled.</p> \n<p>To enable trusted identity propagation for the domain using the SageMaker AI console, choose <strong>Edit</strong> under <strong>Authentication and permissions</strong> on the <strong>Domain settings </strong>tab.</p> \n<p><img alt=\"screenshot of a edit domain workflow\" class=\"alignnone size-full wp-image-114329\" height=\"614\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ml-19006-edit-domain.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1428\" /></p> \n<p>For <strong>Trusted identity propagation</strong>, select <strong>Enable trusted identity propagation for all users on this domain</strong>, and choose <strong>Submit</strong> to save the changes.</p> \n<p><img alt=\"screenshot showing update domain settings to enable trusted identity propagation\" class=\"alignnone size-full wp-image-114335\" height=\"876\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ml-19006-update-domain.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1430\" /></p> \n<h2>(Optional) Update user background session configuration in IAM Identity Center</h2> \n<p>IAM Identity Center now supports running user background sessions, and the session duration is set by default to 7 days. With background sessions, users can launch long-running SageMaker training jobs that assume the user’s identity context along with the SageMaker execution role. As an administrator, you can enable or disable user background sessions, and modify the session duration for user background sessions. As of the time of writing, the maximum session duration that you can set for user background sessions is 90 days. The user’s session is stopped at the end of the specified duration, and consequently, the training job will also fail at the end of the session duration.</p> \n<p>To disable or update the session duration, navigate to the IAM Identity Center console, choose <strong>Settings </strong>in the navigation pane, and choose <strong>Configure</strong> under <strong>Session duration</strong>.</p> \n<p><img alt=\"iam identity center console screenshot showing edit button for configuring interactive sessions\" class=\"alignnone size-full wp-image-114334\" height=\"696\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ml-19006-session-duration.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1431\" /></p> \n<p>For <strong>User background sessions</strong>, select <strong>Enable user background sessions</strong> and use the dropdown to change the session duration. If user background sessions are disabled, the user must be logged in for the duration of the training job; otherwise, the training job will fail once the user logs out. Updating this configuration doesn’t affect current running sessions and only applies to newly created user background sessions. Choose <strong>Save</strong> to save your settings.</p> \n<p><img alt=\"screenshot showing how to update the background session duration for users in iam identity center\" class=\"alignnone size-full wp-image-114330\" height=\"880\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ml-19006-edit-session-duration.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1619\" /></p> \n<h2>Use cases</h2> \n<p>Imagine you’re an enterprise with hundreds or even thousands of users, each requiring varying levels of access to data across multiple teams. You’re responsible for maintaining an AI/ML system on SageMaker AI and managing access permissions across diverse data sources such as <a href=\"https://aws.amazon.com/s3\">Amazon Simple Storage Service (Amazon S3)</a>, <a href=\"https://aws.amazon.com/redshift/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Redshift</a>, and <a href=\"https://aws.amazon.com/lake-formation/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Lake Formation</a>. Traditionally, this has involved maintaining complex IAM policies for users, services, and resources, including bucket policies where applicable. This approach is not only tedious but also makes it challenging to track and audit data access without maintaining a separate role for each user.</p> \n<p>This is precisely the scenario that trusted identity propagation aims to address. With trusted identity propagation support, you can now maintain service-specific roles with minimal permissions, such as <code>s3:GetDataAccess</code> or <code>LakeFormation:GetDataAccess</code>, along with additional permissions to start jobs, view job statuses, and perform other necessary tasks. For data access, you can assign fine-grained policies directly to individual users. For instance, Jane might have read access to customer data and full access to sales and pricing data, whereas Laura might only have read access to sales trends. Both Jane and Laura can assume the same SageMaker AI role to access their SageMaker Studio applications, while maintaining separate data access permissions based on their individual identities.In the following sections, we explore how this can be achieved for common use cases, demonstrating the power and flexibility of trusted identity propagation in simplifying data access management while maintaining robust security and auditability.</p> \n<h3>Scenario 1: Experiment with Amazon S3 data in notebooks</h3> \n<p>S3 Access Grants provide a simplified way to manage data access at scale. Unlike traditional IAM roles and policies that require a detailed knowledge of IAM concepts, and frequent policy updates as new resources are added, with S3 Access Grants, you can define access to data based on familiar database-like grants that automatically scale with your data. This approach significantly reduces the operational overhead of managing thousands of IAM policies and bucket policies, and overcomes the limitations of IAM permissions, while strengthening security through access patterns. If you don’t have S3 Access Grants set up, see <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-grants-instance-create.html\" rel=\"noopener noreferrer\" target=\"_blank\">Create an S3 Access Grant instance</a> to get started. For detailed architecture and use cases, you can also refer to <a href=\"https://aws.amazon.com/blogs/storage/scaling-data-access-with-amazon-s3-access-grants/\" rel=\"noopener noreferrer\" target=\"_blank\">Scaling data access with Amazon S3 Access Grants</a>. After you have set up S3 Access Grants, you can grant access to your datasets to users based on their identity in IAM Identity Center.</p> \n<p>To use S3 Access Grants from SageMaker Studio, update the following IAM roles with policies and trust policies.</p> \n<p>For the domain or user execution role, add the following <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html\" rel=\"noopener noreferrer\" target=\"_blank\">inline policy</a>:</p> \n<div class=\"hide-language\"> \n <div class=\"hide-language\"> \n  <pre><code class=\"lang-json\">{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowDataAccessAPI\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetDataAccess\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:&lt;region&gt;:&lt;account&gt;:access-grants/default\"\n            ]\n        },\n        {\n            \"Sid\": \"RequiredForTIP\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:SetContext\",\n            \"Resource\": \"arn:aws:iam::&lt;account&gt;:role/&lt;s3-access-grants-role&gt;\"\n        }\n    ]\n}</code></pre> \n </div> \n</div> \n<p>Make sure the S3 Access Grants role’s trust policy allows the <code>sts:SetContext</code> action in addition to <code>sts:AssumeRole</code>. The following is a sample trust policy:</p> \n<div class=\"hide-language\"> \n <div class=\"hide-language\"> \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-json\">{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": [\n                    \"access-grants.s3.amazonaws.com\"\n                ]\n            },\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"sts:SetContext\"\n            ],\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:SourceArn\": \"arn:aws:s3:&lt;region&gt;:&lt;account&gt;:access-grants/default\"\n                }\n            }\n        }\n    ]\n</code></pre> \n  </div> \n  <pre></pre> \n </div> &nbsp; \n</div> Now, the user can access the data as allowed by S3 Access Grants for your user identity by calling the \n<code>GetDataAccess</code> API to return temporary credentials, and by assuming the temporary credentials to read or write to their prefixes. For example, the following code shows how to use Boto3 to get temporary credentials and assume the credentials to get access to Amazon S3 locations that are allowed through S3 Access Grants: \n<div class=\"hide-language\">\n  &nbsp; \n <pre><code class=\"lang-python\">import boto3\nfrom botocore.config import Config\n\ndef get_access_grant_credentials(account_id: str, target: str, \n                                 permission: str = 'READ'):\n    s3control = boto3.client('s3control')\n    response = s3control.get_data_access(\n        AccountId=account_id,\n        Target=target,\n        Permission=permission\n    )\n    return response['Credentials']\n\ndef create_s3_client_from_credentials(credentials) -&gt; boto3.client:\n    return boto3.client(\n        's3',\n        aws_access_key_id=credentials['AccessKeyId'],\n        aws_secret_access_key=credentials['SecretAccessKey'],\n        aws_session_token=credentials['SessionToken']\n    )\n\n# Create client\ncredentials = get_access_grant_credentials('&lt;account&gt;',\n                                        \"s3://&lt;bucket&gt;/&lt;allowed-prefix&gt;/\")\ns3 = create_s3_client_from_credentials(credentials)\n\n# Will succeed\ns3.list_objects(Bucket=\"&lt;bucket&gt;\", Prefix=\"&lt;allowed-prefix&gt;\")\n\n# Will fail\ns3.list_objects(Bucket=\"&lt;bucket&gt;\", Prefix=\"&lt;any-other-prefix&gt;\")</code></pre> \n</div> \n<h3>Scenario 2: Access Lake Formation through Athena</h3> \n<p>Lake Formation provides centralized governance and fine-grained access control management for data stored in Amazon S3 and metadata in the <a href=\"https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Glue Data Catalog</a>. The Lake Formation permission model operates in conjunction with IAM permissions, offering granular controls at the database, table, column, row, and cell levels. This dual-layer security model provides comprehensive data governance while maintaining flexibility in access patterns.</p> \n<p>Data governed through Lake Formation can be accessed through various AWS analytics services. In this scenario, we demonstrate using Athena, a serverless query engine that integrates seamlessly with Lake Formation’s permission model. For other services like Amazon EMR on EC2, make sure the resource is configured to support trusted identity propagation, including setting up security configurations and making sure the EMR cluster is configured with IAM roles that support trusted identity propagation.</p> \n<p>The following instructions assume that you have already set up Lake Formation. If not, see <a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/initial-lf-config.html\" rel=\"noopener noreferrer\" target=\"_blank\">Set up AWS Lake Formation</a> and follow the <a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started-tutorials.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Lake Formation tutorials</a> to set up Lake Formation and bring in your data.</p> \n<p>Complete the following steps to access your governed data in trusted identity propagation-enabled SageMaker Studio notebooks using Athena:</p> \n<ol> \n <li>Integrate Lake Formation with IAM Identity Center by following the instructions in <a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/identity-center-integration.html\" rel=\"noopener noreferrer\" target=\"_blank\">Integrating IAM Identity Center</a>. At a high level, this includes creating an IAM role allowing creating and updating application configurations in Lake Formation and IAM Identity Center, and providing the single sign-on (SSO) instance ID.</li> \n <li>Grant permissions to the IAM Identity Center user to the relevant resources (database, table, row or column) using Lake Formation. See <a href=\"https://docs.aws.amazon.com/lake-formation/latest/dg/granting-catalog-permissions.html\" rel=\"noopener noreferrer\" target=\"_blank\">Granting permissions on Data Catalog resources</a> instructions.</li> \n <li>Create an Athena workgroup that supports trusted identity propagation by following instructions in <a href=\"https://docs.aws.amazon.com/athena/latest/ug/creating-workgroups.html\" rel=\"noopener noreferrer\" target=\"_blank\">Create a workgroup</a> and choosing <strong>IAM Identity Center</strong> as the method of authentication. Make sure the user has access to write to the query results location provided here using S3 Access Grants, because Athena uses access grants by default when choosing IAM Identity Center as the authentication method.</li> \n <li>Update the Athena workgroup’s IAM role with the following trust policy (add <code>sts:SetContext</code> to the existing trust policy). You can find the IAM role by choosing the workgroup you created earlier and looking for <strong>Role name</strong>.</li> \n</ol> \n<div class=\"hide-language\"> \n <div class=\"hide-language\"> \n  <pre><code class=\"lang-json\">{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AthenaTrustPolicy\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"athena.amazonaws.com\"\n            },\n            \"Action\": [\n                \"sts:AssumeRole\",\n                \"sts:SetContext\"\n            ],\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:SourceAccount\": \"&lt;account-id&gt;\"\n                },\n                \"ArnLike\": {\n                    \"aws:SourceArn\": \"arn:aws:athena:&lt;region&gt;:&lt;account-id&gt;:workgroup/&lt;workgroup-name&gt;\"\n                }\n            }\n        }\n    ]\n}</code></pre> \n </div> \n</div> \n<p>The setup is now complete. You can now launch SageMaker Studio using an IAM Identity Center user, launch a JupyterLab or Code Editor application, and query the database. See the following example code to get started:</p> \n<div class=\"hide-language\"> \n <div class=\"hide-language\"> \n  <pre><code class=\"lang-python\">import time\nimport boto3\nimport pandas as pd\nathena_client = boto3.client(\"athena\")\n\ndatabase = \"&lt;database-name&gt;\"\ntable = \"&lt;table-name&gt;\"\nquery = f\"SELECT * FROM {database}.{table}\"\noutput_location = \"s3://&lt;bucket-name&gt;/queries\"  # bucket name and location from Step 3\n\nresponse = athena_client.start_query_execution(\n    QueryString=query,\n    QueryExecutionContext={'Database': database},\n    ResultConfiguration={'OutputLocation': output_location}\n)\n\n# Get the query execution ID\nquery_execution_id = response['QueryExecutionId']\n\n# wait for query to complete\nwhile True:\n    query_status = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n    status = query_status['QueryExecution']['Status']['State']\n    if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n        break\n    time.sleep(1)\n\n# If the query succeeded, fetch and display results\nif status == 'SUCCEEDED':\n    results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n    \n    # Extract column names and data\n    columns = [col['Name'] for col in results['ResultSet']['ResultSetMetadata']['ColumnInfo']]\n    data = []\n    for row in results['ResultSet']['Rows'][1:]:  # Skip the header row\n        data.append([field.get('VarCharValue', '') for field in row['Data']])\n    \n    # Create a pandas DataFrame\n    df = pd.DataFrame(data, columns=columns)\n    \n    # Display the first few rows\n    print(df.head())\nelse:\n    print(f\"Query failed with status: {status}\")</code></pre> \n </div> \n</div> \n<h3>Scenario 3: Create a training job supported with user background sessions</h3> \n<p>For a trusted identity propagation-enabled domain, a user background session is a session that continues to run even if the end-user has logged out of their interactive session such as JupyterLab applications in SageMaker Studio. For example, the user can initiate a training job from their SageMaker Studio space, and the job can run in the background for days or weeks regardless of the user’s activity, and use the user’s identity to access data and log audit trails. If your domain doesn’t have trusted identity propagation enabled, you can continue to run training jobs and processing jobs as before; however, if trusted identity propagation is enabled, make sure your user background session time is updated to reflect the duration of your training jobs, because the default is set automatically to 7 days. If you have enabled user background sessions, update your SageMaker Studio domain or user’s execution role with the following permissions to provide a seamless experience for data scientists:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-json\">{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowDataAccessAPI\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetDataAccess\",\n                \"s3:GetAccessGrantsInstanceForPrefix\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:&lt;region&gt;:&lt;account&gt;:access-grants/default\"\n            ]\n        },\n        {\n            \"Sid\": \"RequiredForTIP\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:SetContext\",\n            \"Resource\": \"arn:aws:iam::&lt;account&gt;:role/&lt;s3-access-grants-role&gt;\"\n        }\n    ]\n}</code></pre> \n</div> \n<p>With this setup, a data scientist can use an Amazon S3 location that they have access to through S3 Access Grants. SageMaker automatically looks for data access using S3 Access Grants and falls back to the job’s IAM role otherwise. For example, in the following SDK call to create the training job, the user provides the S3 Amazon URI where the data is stored, they have access to it through S3 Access Grants, and they can run this job without additional setup:</p> \n<div class=\"hide-language\"> \n <div class=\"hide-language\"> \n  <div class=\"hide-language\"> \n   <pre><code class=\"lang-python\">    response = sm.create_training_job(\n        TrainingJobName=training_job_name,\n        AlgorithmSpecification={\n            'TrainingImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04',\n            'TrainingInputMode': 'File',\n            ...\n                    RoleArn='arn:aws:iam::&lt;account&gt;:role/tip-domain-role',\n        InputDataConfig=[\n            {\n                'ChannelName': 'training',\n                'DataSource': {\n                    'S3DataSource': {\n                        'S3DataType': 'S3Prefix',\n                        'S3Uri': 's3://&lt;s3-ag-enabled-bucket&gt;/&lt;s3-ag-enabled-prefix&gt;',\n                        'S3DataDistributionType': 'FullyReplicated'\n                    }\n                },\n                'CompressionType': 'None',\n                'RecordWrapperType': 'None'\n            },\n            ...\n        }</code></pre> \n  </div> \n </div> \n</div> \n<h4>(Optional) View and manage user background sessions on IAM Identity Center</h4> \n<p>When training jobs are run as user background sessions, you can view these sessions as user background sessions on IAM Identity Center. The administrator can view a list of all user background sessions and optionally stop a session if the user has left the team, for example. When the user background session is ended, the training job subsequently fails.</p> \n<p>To view a list of all user background sessions, on the IAM Identity Center console, choose <strong>Users</strong> and choose the user you want view the user background sessions for. Choose the <strong>Active sessions</strong> tab to view a list of sessions. The user background session can be identified by the <strong>Session type</strong> column, which shows if the session is interactive or a user background session. The list also shows the job’s Amazon Resource Name (ARN) under the <strong>Used by</strong> column.</p> \n<p>To end a session, select the session and choose <strong>End sessions</strong>.</p> \n<p><img alt=\"screenshot showing list of active sessions in iam identity center console\" class=\"alignnone size-full wp-image-114332\" height=\"650\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ml-19006-list-active-sessions.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1392\" /></p> \n<p>You will be prompted to confirm the action. Enter confirm to confirm that you want to end the session and choose <strong>End sessions</strong> to stop the user background session.</p> \n<p><img alt=\"screenshot showing how to end a user interactive session from iam identity center console\" class=\"alignnone size-full wp-image-114331\" height=\"427\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ml-19006-end-session.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"599\" /></p> \n<h3>Scenario 4: Auditing using CloudTrail</h3> \n<p>After trusted identity propagation is enabled for your domain, you can now track the user that performed specific actions through CloudTrail. To try this out, log in to SageMaker Studio, and create and open a JupyterLab space. Open a terminal and enter <code>aws s3 ls</code> to list the available buckets in your Region.</p> \n<p>On the CloudTrail console, choose <strong>Event history</strong> in the navigation pane. Update the <strong>Lookup attributes</strong> to <strong>Event name</strong> and in the search box, enter <code>ListBuckets</code>. You should see a list of events, as shown in the following screenshot (it might take up to 5 minutes for the logs to be available in CloudTrail).</p> \n<p><img alt=\"screenshot showing list of cloudtrail events\" class=\"alignnone size-full wp-image-114333\" height=\"601\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ml-19006-list-events-ct.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1424\" /></p> \n<p>Choose the event to view its details (verify the user name is <strong>SageMaker</strong> if you have also listed buckets through the AWS console or APIs). In the event details, you should be able to see an additional field called <code>onBehalfOf</code> that has the user’s identity.</p> \n<p><img alt=\"screenshot showing cloudtrail event details with the onBehalfOf field highlighted\" class=\"alignnone size-full wp-image-114328\" height=\"749\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ml-19006-ct-event.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1422\" /></p> \n<p>Supported services and SageMaker AI features called from a trusted identity propagation-enabled SageMaker Studio domain will have the <code>OnBehalfOf</code> field in CloudTrail.</p> \n<h2>Clean up</h2> \n<p>If you have created a SageMaker Studio domain for the purposes of trying out trusted identity propagation, delete the domain and its associated <a href=\"https://aws.amazon.com/efs/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Elastic File System</a> (Amazon EFS) volume to avoid incurring additional charges. Before deleting a domain, you must delete all the users and their associated spaces and applications. For detailed instructions, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-running-stop.html\" rel=\"noopener noreferrer\" target=\"_blank\">Stop and delete your Studio running applications and spaces</a>.</p> \n<p>If you created a SageMaker training job, they are ephemeral, and the compute is shut down automatically when the job is complete.</p> \n<p>Athena is a serverless analytics service that charges per query billing. No cleanup is necessary, but for best practices, <a href=\"https://docs.aws.amazon.com/athena/latest/ug/deleting-workgroups.html\" rel=\"noopener noreferrer\" target=\"_blank\">delete the workgroup</a> to remove unused resources.</p> \n<h2>Conclusion</h2> \n<p>In this post, we showed you how to enable trusted identity propagation for SageMaker AI domains that use IAM Identity Center as the mode of authentication. With trusted identity propagation, administrators can manage user authorization to other AWS services through the user’s physical identity in conjunction with IAM roles. Administrators can streamline permissions management by maintaining a single domain execution role and manage granular access to other AWS services and data sources through the user’s identity. In addition, trusted identity propagation supports auditing, so administrators can track user activity without the need for managing a role for each user profile.</p> \n<p>To learn more about enabling this feature and its use cases, see <a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/trustedidentitypropagation-integrations.html\" rel=\"noopener noreferrer\" target=\"_blank\">Trusted identity propagation use cases</a> and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/trustedidentitypropagation.html\" rel=\"noopener noreferrer\" target=\"_blank\">Trusted identity propagation with Studio</a>. This post covered a subset of supported applications; we encourage you to check out the documentation and choose the services that best serve your use case and share your feedback!</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><img alt=\"author-ajjaisin\" class=\"wp-image-114323 size-thumbnail alignleft\" height=\"115\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/ajjaisin-100x115.png\" width=\"100\" /><strong>Amit Shyam Jaisinghani</strong> is a Software Engineer on the SageMaker Studio team at Amazon Web Services, and he earned his Master’s degree in Computer Science from Rochester Institute of Technology. Since joining Amazon in 2019, he has built and enhanced several AWS services, including AWS WorkSpaces and Amazon SageMaker Studio. Outside of work, he explores hiking trails, plays with his two cats, Missy and Minnie, and enjoys playing Age of Empire.</p> \n<p style=\"clear: both;\"><strong><img alt=\"author-surydurg\" class=\"size-thumbnail wp-image-114336 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/surydurg-100x100.png\" width=\"100\" />Durga Sury</strong> is a Senior Solutions Architect at Amazon SageMaker, where she helps enterprise customers build secure and scalable AI/ML systems. When she’s not architecting solutions, you can find her enjoying sunny walks with her dog, immersing herself in murder mystery books, or catching up on her favorite Netflix shows.</p> \n<p style=\"clear: both;\"><strong><img alt=\"author-khushbsr\" class=\"size-full wp-image-114324 alignleft\" height=\"100\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/khushbsr.png\" width=\"100\" />Khushboo Srivastava</strong> is a Senior Product Manager for Amazon SageMaker. She enjoys building products that simplify machine learning workflows for customers, and loves playing with her 1-year old daughter.</p> \n<p style=\"clear: both;\"><strong><img alt=\"author-kmaniva\" class=\"size-thumbnail wp-image-114325 alignleft\" height=\"111\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/13/kmaniva-100x111.png\" width=\"100\" />Krishnan Manivannan</strong> is a Senior Software Engineer at Amazon Web Services and a founding member of the SageMaker AI API team. He has 8 years of experience in the architecture and security of large-scale machine learning services. His specialties include API design, service scalability, identity and access management, and inventing new approaches for building and operating distributed systems. Krishnan has led multiple engineering efforts from design through global launch, delivering reliable and secure systems for customers worldwide.</p>"
        }
      ]
    },
    {
      "title": "Benchmarking document information localization with Amazon Nova",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Benchmarking document information localization with Amazon Nova"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/benchmarking-document-information-localization-with-amazon-nova/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/benchmarking-document-information-localization-with-amazon-nova/",
      "authors": [
        {
          "name": "Ryan Razkenari"
        }
      ],
      "author": "Ryan Razkenari",
      "author_detail": {
        "name": "Ryan Razkenari"
      },
      "published": "Tue, 19 Aug 2025 18:17:36 +0000",
      "published_parsed": [
        2025,
        8,
        19,
        18,
        17,
        36,
        1,
        231,
        0
      ],
      "tags": [
        {
          "term": "Advanced (300)",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Nova",
          "scheme": null,
          "label": null
        },
        {
          "term": "Artificial Intelligence",
          "scheme": null,
          "label": null
        },
        {
          "term": "Generative AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "Technical How-to",
          "scheme": null,
          "label": null
        }
      ],
      "id": "a5a180f1a3fa1d0a64bcaffbcfcb7473f0b54933",
      "guidislink": false,
      "summary": "This post demonstrates how to use foundation models (FMs) in Amazon Bedrock, specifically Amazon Nova Pro, to achieve high-accuracy document field localization while dramatically simplifying implementation. We show how these models can precisely locate and interpret document fields with minimal frontend effort, reducing processing errors and manual intervention.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "This post demonstrates how to use foundation models (FMs) in Amazon Bedrock, specifically Amazon Nova Pro, to achieve high-accuracy document field localization while dramatically simplifying implementation. We show how these models can precisely locate and interpret document fields with minimal frontend effort, reducing processing errors and manual intervention."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>Every day, enterprises process thousands of documents containing critical business information. From invoices and purchase orders to forms and contracts, accurately locating and extracting specific fields has traditionally been one of the most complex challenges in document processing pipelines. Although optical character recognition (OCR) can tell us what text exists in a document, determining where specific information is located has required sophisticated computer vision solutions.</p> \n<p>The evolution of this field illustrates the complexity of the challenge. Early object detection approaches like <a href=\"https://arxiv.org/abs/1506.02640\" rel=\"noopener noreferrer\" target=\"_blank\">YOLO (You Only Look Once)</a> revolutionized the field by reformulating object detection as a regression problem, enabling real-time detection. <a href=\"https://arxiv.org/abs/1708.02002v2\" rel=\"noopener noreferrer\" target=\"_blank\">RetinaNet</a> advanced this further by addressing class imbalance issues through Focal Loss, and <a href=\"https://arxiv.org/abs/2005.12872\" rel=\"noopener noreferrer\" target=\"_blank\">DETR</a> introduced transformer-based architectures to minimize hand-designed components. However, these approaches shared common limitations: they required extensive training data, complex model architectures, and significant expertise to implement and maintain.</p> \n<p>The emergence of multimodal large language models (LLMs) represents a paradigm shift in document processing. These models combine advanced vision understanding with natural language processing capabilities, offering several groundbreaking advantages:</p> \n<ul> \n <li>Minimized use of specialized computer vision architectures</li> \n <li>Zero-shot capabilities without the need for supervised learning</li> \n <li>Natural language interfaces for specifying location tasks</li> \n <li>Flexible adaptation to different document types</li> \n</ul> \n<p>This post demonstrates how to use foundation models (FMs) in <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a>, specifically <a href=\"https://aws.amazon.com/ai/generative-ai/nova/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Nova Pro</a>, to achieve high-accuracy document field localization while dramatically simplifying implementation. We show how these models can precisely locate and interpret document fields with minimal frontend effort, reducing processing errors and manual intervention. Through comprehensive benchmarking on the <a href=\"https://arxiv.org/abs/2311.11856\" rel=\"noopener noreferrer\" target=\"_blank\">FATURA dataset</a>, we provide benchmarking of performance and practical implementation guidance.</p> \n<h2>Understanding document information localization</h2> \n<p>Document information localization goes beyond traditional text extraction by identifying the precise spatial position of information within documents. Although OCR tells us what text exists, localization tells us where specific information resides—a crucial distinction for modern document processing workflows. This capability enables critical business operations ranging from automated quality checks and sensitive data redaction to intelligent document comparison and validation.</p> \n<p>Traditional approaches to this challenge relied on a combination of rule-based systems and specialized computer vision models. These solutions often required extensive training data, careful template matching, and continuous maintenance to handle document variations. Financial institutions, for instance, would need separate models and rules for each type of invoice or form they processed, making scalability a significant challenge. Multimodal models with localization capabilities available on Amazon Bedrock fundamentally change this paradigm. Rather than requiring complex computer vision architectures or extensive training data, these multimodal LLMs can understand both the visual layout and semantic meaning of documents through natural language interactions. By using models with the capability to localize, organizations can implement robust document localization with significantly reduced technical overhead and greater adaptability to new document types.</p> \n<p>Multimodal models with localization capabilities, such as those available on Amazon Bedrock, fundamentally change this paradigm. Rather than requiring complex computer vision architectures or extensive training data, these multimodal LLMs can understand both the visual layout and semantic meaning of documents through natural language interactions. By using models with the capability to localize, organizations can implement robust document localization with significantly reduced technical overhead and greater adaptability to new document types.</p> \n<h2>Solution overview</h2> \n<p>We designed a simple localization solution that takes a document image and text prompt as input, processes it through selected FMs on Amazon Bedrock, and returns the field locations using either absolute or normalized coordinates. The solution implements two distinct prompting strategies for document field localization:</p> \n<ul> \n <li><strong>Image dimension strategy</strong> – Works with absolute pixel coordinates, providing explicit image dimensions and requesting bounding box locations based on the document’s actual size</li> \n <li><strong>Scaled coordinate strategy</strong> – Uses a normalized 0–1000 coordinate system, making it more flexible across different document sizes and formats</li> \n</ul> \n<p>The solution has a modular design to allow for straightforward extension to support custom field schemas through configuration updates rather than code changes. This flexibility, combined with the scalability of Amazon Bedrock, makes the solution suitable for both small-scale document processing and enterprise-wide deployment. In the following sections, we demonstrate the setup and implementation strategies used in our solution for document field localization using Amazon Bedrock FMs. You can see more details in our <a href=\"https://github.com/aws-samples/sample-document-information-localization\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub repository</a>.</p> \n<h2>Prerequisites</h2> \n<p>For this walkthrough, you should have the following prerequisites:</p> \n<ul> \n <li>An <a href=\"https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&amp;client_id=signup\" rel=\"noopener noreferrer\" target=\"_blank\">AWS account</a> with Amazon Bedrock access</li> \n <li>Permissions to use Amazon Nova Pro</li> \n <li>Python 3.8+ with the boto3 library installed</li> \n</ul> \n<h3>Initial set ups</h3> \n<p>Complete the following setup steps:</p> \n<ol> \n <li>Configure the Amazon Bedrock runtime client with appropriate retry logic and timeout settings:</li> \n</ol> \n<div class=\"code-block\"> \n <pre><code class=\"language-python\">import boto3\nfrom botocore.config import Config\n\n# Configure Bedrock client with retry logic\nBEDROCK_CONFIG = Config(\n    region_name='us-west-2',\n    signature_version='v4',\n    read_timeout=500,\n    retries={\n        'max_attempts': 10,\n        'mode': 'adaptive'\n    }\n)\n\n# Initialize Bedrock runtime client\nbedrock_runtime = boto3.client(\"bedrock-runtime\", config=BEDROCK_CONFIG)</code></pre> \n</div> \n<ol start=\"2\"> \n <li>Define your field configuration to specify which elements to locate in your documents:</li> \n</ol> \n<div class=\"code-block\"> \n <pre><code class=\"language-python\"># sample config\nfield_config = {\n    \"invoice_number\": {\"type\": \"string\", \"required\": True},\n    \"total_amount\": {\"type\": \"currency\", \"required\": True},\n    \"date\": {\"type\": \"date\", \"required\": True}\n}</code></pre> \n</div> \n<ol start=\"3\"> \n <li>Initialize the <code>BoundingBoxExtractor</code> with your chosen model and strategy:</li> \n</ol> \n<div class=\"code-block\"> \n <pre><code class=\"language-python\">extractor = BoundingBoxExtractor(\n    model_id=NOVA_PRO_MODEL_ID,  # or other FMs on Amazon Bedrock\n    prompt_template_path=\"path/to/prompt/template\",\n    field_config=field_config,\n    norm=None  # Set to 1000 for scaled coordinate strategy\n)\n\n# Process a document    \nbboxes, metadata = extractor.get_bboxes(\n    document_image=document_image,\n    document_key=\"invoice_001\" # Optional tracking key\n)</code></pre> \n</div> \n<h2>Implement prompting strategies</h2> \n<p>We test two prompt strategies in this workflow: image dimension and scaled coordinate.</p> \n<p>The following is a sample prompt template for the image dimension strategy:</p> \n<div class=\"code-block\"> \n <pre><code class=\"language-text\">\"\"\"\nYour task is to detect and localize objects in images with high precision.\nAnalyze each provided image (width = {w} pixels, height = {h} pixels) and return only a JSON object with bounding box data for detected objects.\n\nOutput Requirements:\n1. Use absolute pixel coordinates based on provided width and height.\n2. Ensure high accuracy and tight-fitting bounding boxes.\n\nDetected Object Structure:\n- \"element\": Use one of these labels exactly: {elements}\n- \"bbox\": Array with coordinates [x1, y1, x2, y2] in absolute pixel values.\n\nJSON Structure:\n```json\n{schema}\n```\n\nProvide only the specified JSON format without extra information.\n\"\"\"</code></pre> \n</div> \n<p>The following is a sample prompt template for the scaled coordinate strategy:</p> \n<div class=\"code-block\"> \n <pre><code class=\"language-text\">\"\"\"\nYour task is to detect and localize objects in images with high precision.\nAnalyze each provided image and return only a JSON object with bounding box data for detected objects.\n\nOutput Requirements:\nUse (x1, y1, x2, y2) format for bounding box coordinates, scaled between 0 and 1000.\n\nDetected Object Structure:\n- \"element\": Use one of these labels exactly: {elements}\n- \"bbox\": Array [x1, y1, x2, y2] scaled between 0 and 1000.\n\nJSON Structure:\n```json\n{schema}\n```\n\nProvide only the specified JSON format without extra information.\n\"\"\"</code></pre> \n</div> \n<h2>Evaluate performance</h2> \n<p>We implement evaluation metrics to monitor accuracy:</p> \n<div class=\"code-block\"> \n <pre><code class=\"language-python\">evaluator = BBoxEvaluator(field_config=field_config)\nevaluator.set_iou_threshold(0.5)  # Adjust based on requirements\nevaluator.set_margin_percent(5)   # Tolerance for position matching\n\n# Evaluate predictions\nresults = evaluator.evaluate(predictions, ground_truth)\nprint(f\"Mean Average Precision: {results['mean_ap']:.4f}\")</code></pre> \n</div> \n<p>This implementation provides a robust foundation for document field localization while maintaining flexibility for different use cases and document types. The choice between image dimension and scaled coordinate strategies depends on your specific accuracy requirements and document variation.</p> \n<h2>Benchmarking results</h2> \n<p>We conducted our benchmarking study using FATURA, a public invoice dataset specifically designed for document understanding tasks. The dataset comprises 10,000 single-page invoices saved as JPEG images, representing 50 distinct layout templates with 200 invoices per template. Each document is annotated with 24 key fields, including invoice numbers, dates, line items, and total amounts. The annotations provide both the text values and precise bounding box coordinates in JSON format, making it ideal for evaluating field localization tasks. The dataset has the following key characteristics:</p> \n<ul> \n <li>Documents: 10,000 invoices (JPEG format)</li> \n <li>Templates: 50 distinct layouts (200 documents each)</li> \n <li>Fields per document: 24 annotated fields</li> \n <li>Annotation format: JSON with bounding boxes and text values</li> \n <li>Field types: Invoice numbers, dates, addresses, line items, amounts, taxes, totals</li> \n <li>Image resolution: Standard A4 size at 300 DPI</li> \n <li>Language: English</li> \n</ul> \n<p>The following figure shows sample invoice templates showcasing layout variation.</p> \n<div style=\"margin: 20px 0;\">\n <img alt=\"Invoice Template 1\" class=\"invoice-sample\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-4-sample-docs.png\" style=\"width: 25%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" />\n <img alt=\"Invoice Template 2\" class=\"invoice-sample\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-3-sample-docs.png\" style=\"width: 25%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" />\n <img alt=\"Invoice Template 3\" class=\"invoice-sample\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-2-sample-docs.png\" style=\"width: 25%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" />\n <img alt=\"Invoice Template 4\" class=\"invoice-sample\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-1-sample-docs.png\" style=\"width: 25%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" />\n</div> \n<p>The following figure is an example of annotation visualization.</p> \n<div style=\"margin: 20px 0;\">\n <img alt=\"Sample Results 1\" class=\"invoice-sample\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-5-sample-results.png\" style=\"width: 48%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" />\n <img alt=\"Sample Results 2\" class=\"invoice-sample\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-6-sample-results.png\" style=\"width: 48%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" />\n</div> \n<p>Before conducting the full-scale benchmark, we performed an initial experiment to determine the optimal prompting strategy. We selected a representative subset of 50 images, comprising 5 samples from 10 different templates, and evaluated three distinct approaches:</p> \n<ul> \n <li>Image dimension: \n  <ul> \n   <li>Method: Provides explicit pixel dimensions and requests absolute coordinate bounding boxes</li> \n   <li>Input: Image bytes, image dimensions, output schema</li> \n  </ul> </li> \n <li>Scaled coordinate: \n  <ul> \n   <li>Method: Uses normalized 0-1000 coordinate system</li> \n   <li>Input: Image bytes, output schema</li> \n  </ul> </li> \n <li>Added gridlines: \n  <ul> \n   <li>Method: Enhances image with visual gridlines at fixed intervals</li> \n   <li>Input: Modified image with gridlines bytes, image dimensions, output schema</li> \n  </ul> </li> \n</ul> \n<p>The following figure compares performance for different approaches for Mean Average Precision (mAP).</p> \n<div style=\"margin: 20px 0;\">\n <img alt=\"Mean AP Distribution\" class=\"alignnone wp-image-112804 size-full\" height=\"1786\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18377-nova_pro_mean_ap_distribution.png\" style=\"width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" width=\"2669\" />\n</div> \n<p>Building on insights from our initial strategy evaluation, we conducted benchmarking using the complete FATURA dataset of 10,000 documents. We employed the scaled coordinate approach for Amazon Nova models, based on their respective optimal performance characteristics from our initial testing. Our evaluation framework assessed Amazon Nova Pro through standard metrics, including Intersection over Union (IoU) and Average Precision (AP). The evaluation spanned all 50 distinct invoice templates, using an IoU threshold of 0.5 and a 5% margin tolerance for field positioning.</p> \n<p>The following are our sample results in JSON:</p> \n<div class=\"code-block\"> \n <pre><code class=\"language-json\">{\n    \"template\": \"template1\",\n    \"instance\": \"Instance0\",\n    \"metrics\": {\n        \"mean_ap\": 0.8421052631578947,\n        \"field_scores\": {\n            \"TABLE\": [0.9771107575829314, 1.0, 1.0, 1.0, 1.0],\n            \"BUYER\": [0.3842328422050217, 0.0, 0.0, 0, 0.0],\n            \"DATE\": [0.9415158516000428, 1.0, 1.0, 1.0, 1.0],\n            \"DISCOUNT\": [0.8773709977744115, 1.0, 1.0, 1.0, 1.0],\n            \"DUE_DATE\": [0.9338410331219548, 1.0, 1.0, 1.0, 1.0],\n            \"GSTIN_BUYER\": [0.8868145680064249, 1.0, 1.0, 1.0, 1.0],\n            \"NOTE\": [0.7926162009357707, 1.0, 1.0, 1.0, 1.0],\n            \"PAYMENT_DETAILS\": [0.9517931284002012, 1.0, 1.0, 1.0, 1.0],\n            \"PO_NUMBER\": [0.8454266053075804, 1.0, 1.0, 1.0, 1.0],\n            \"SELLER_ADDRESS\": [0.9687004508445741, 1.0, 1.0, 1.0, 1.0],\n            \"SELLER_EMAIL\": [0.8771026147909002, 1.0, 1.0, 1.0, 1.0],\n            \"SELLER_SITE\": [0.8715647216012751, 1.0, 1.0, 1.0, 1.0],\n            \"SUB_TOTAL\": [0.8049954543667662, 1.0, 1.0, 1.0, 1.0],\n            \"TAX\": [0.8751563641702513, 1.0, 1.0, 1.0, 1.0],\n            \"TITLE\": [0.850667327423512, 1.0, 1.0, 1.0, 1.0],\n            \"TOTAL\": [0.7226784112051814, 1.0, 1.0, 1.0, 1.0],\n            \"TOTAL_WORDS\": [0.9099353099528785, 1.0, 1.0, 1.0, 1.0],\n            \"GSTIN_SELLER\": [0.87170328009624, 1.0, 1.0, 1.0, 1.0],\n            \"LOGO\": [0.679425211111111, 1.0, 1.0, 1.0, 1.0]\n        }\n    },\n    \"metadata\": {\n        \"usage\": {\n            \"inputTokens\": 2250,\n            \"outputTokens\": 639,\n            \"totalTokens\": 2889\n        },\n        \"metrics\": {\n            \"latencyMs\": 17535\n        }\n    }\n}</code></pre> \n</div> \n<p>The following figure is an example of successful localization for Amazon Nova Pro.</p> \n<div style=\"margin: 20px auto; width: 70%;\">\n <img alt=\"\" class=\"alignnone size-full wp-image-112358\" height=\"841\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-7-sample-results.png\" style=\"width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" width=\"595\" />\n</div> \n<p>The results demonstrate Amazon Nova Pro’s strong performance in document field localization. Amazon Nova Pro achieved a mAP of 0.8305. It demonstrated consistent performance across various document layouts, achieving a mAP above 0.80 across 45 of 50 templates, with the lowest template-specific mAP being 0.665. Although Amazon Nova Pro showed relatively high processing failures (170 out of 10,000 images), it still maintained high overall performance. Most low AP results were attributed to either complete processing failures (particularly over-refusal by its guardrail filters and malformed JSON output) or field misclassifications (particularly confusion between similar fields, such as buyer vs. seller addresses).</p> \n<p>The following table summarizes the overall performance metrics.</p> \n<table border=\"1px\" cellpadding=\"10px\" class=\"styled-table\"> \n <tbody> \n  <tr> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"></td> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><strong>Mean IoU</strong></td> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><strong>Mean AP</strong></td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\">Amazon Nova Pro</td> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\">0.7423</td> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\">0.8331</td> \n  </tr> \n </tbody> \n</table> \n<p>The following graph shows the performance distribution for each individual extraction of approximately 20 labels for 10,000 documents.</p> \n<div style=\"margin: 20px 0;\">\n <img alt=\"\" class=\"alignnone wp-image-112803 size-full\" height=\"1768\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18377-nova_pro_iou_distribution.png\" style=\"width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" width=\"2669\" />\n</div> \n<p>Field-specific analysis reveals that Amazon Nova Pro excels at locating structured fields like invoice numbers and dates, consistently achieving precision and recall scores above 0.85. It demonstrates particularly strong performance with text fields, maintaining robust accuracy even when dealing with varying currency formats and decimal representations. This resilience to format variations makes it especially valuable for processing documents from multiple sources or regions.</p> \n<p>The following graph summarizes field-specific performance. The graph shows AP success percentage for each label, across all documents for each model. It is sorted by highest success.</p> \n<div style=\"margin: 20px 0;\">\n <img alt=\"\" class=\"alignnone wp-image-112805 size-full\" height=\"1768\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18377-nova_pro_results.png\" style=\"width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" width=\"2669\" />\n</div> \n<h2>Conclusion</h2> \n<p>This benchmarking study demonstrates the significant advances in document field localization by multimodal FMs. Through comprehensive testing on the FATURA dataset, we’ve shown that these models can effectively locate and extract document fields with minimal setup effort, dramatically simplifying traditional computer vision workflows. Amazon Nova Pro emerges as an excellent choice for enterprise document processing, delivering a mAP of 0.8305 with consistent performance across diverse document types. Looking ahead, we see several promising directions for further optimization. Future work could explore extending the solution in agentic workflows to support more complex document types and field relationships.</p> \n<p>To get started with your own implementation, you can find the complete solution code in our <a href=\"https://github.com/aws-samples/sample-document-information-localization\" rel=\"noopener noreferrer\" target=\"_blank\">GitHub repository</a>. We also recommend reviewing the <a href=\"https://docs.aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock documentation</a> for the latest model capabilities and best practices.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-112361 size-full alignleft\" height=\"160\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-image-11.jpeg\" width=\"120\" /> <strong>Ryan Razkenari</strong>&nbsp;is a Deep Learning Architect at the AWS Generative AI Innovation Center, where he uses his expertise to create cutting-edge AI solutions. With a strong background in AI and analytics, he is passionate about building innovative technologies that address real-world challenges for AWS customers.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-112362 alignleft\" height=\"160\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-image-12.jpeg\" width=\"120\" />Harpreet Cheema </strong>is a Deep Learning Architect at the AWS Generative AI Innovation Center. He is very passionate in the field of machine learning and in tackling different problems in the ML domain. In his role, he focuses on developing and delivering Generative AI focused solutions for real-world applications.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-112363 alignleft\" height=\"160\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-image-13.jpeg\" width=\"120\" />Spencer Romo</strong> is a Senior Data Scientist with extensive experience in deep learning applications. He specializes in intelligent document processing while maintaining broad expertise in computer vision, natural language processing, and signal processing. Spencer’s innovative work in remote sensing has resulted in multiple patents. Based in Austin, Texas, Spencer loves working directly with customers to understand their unique problems and identify impactful AI solutions. Outside of work, Spencer competes in 24 Hours of Lemons racing series, embracing the challenge of high-performance driving on a budget.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-112364 alignleft\" height=\"160\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-image-14.jpeg\" width=\"120\" />Mun Kim</strong>&nbsp;is a Machine Learning Engineer at the AWS Generative AI Innovation Center. Mun brings expertise in building machine learning science and platform that help customers harness the power of generative AI technologies. He works closely with AWS customers to accelerate their AI adoption journey and unlock new business value.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-112365 alignleft\" height=\"160\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/26/ML-18377-image-15.jpeg\" width=\"120\" />Wan Chen</strong> is an Applied Science Manager at the Generative AI Innovation Center. As a ML/AI veteran in tech industry, she has wide range of expertise on traditional machine learning, recommender system, deep learning and Generative AI. She is a stronger believer of Superintelligence, and is very passionate to push the boundary of AI research and application to enhance human life and drive business growth. She holds Ph.D in Applied Mathematics from University of British Columbia, and had worked as postdoctoral fellow in Oxford University.</p>"
        }
      ]
    },
    {
      "title": "How Infosys built a generative AI solution to process oil and gas drilling data with Amazon Bedrock",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "How Infosys built a generative AI solution to process oil and gas drilling data with Amazon Bedrock"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/how-infosys-built-a-generative-ai-solution-to-process-oil-and-gas-drilling-data-with-amazon-bedrock/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/how-infosys-built-a-generative-ai-solution-to-process-oil-and-gas-drilling-data-with-amazon-bedrock/",
      "authors": [
        {
          "name": "Dhiraj Thakur"
        }
      ],
      "author": "Dhiraj Thakur",
      "author_detail": {
        "name": "Dhiraj Thakur"
      },
      "published": "Tue, 19 Aug 2025 18:04:25 +0000",
      "published_parsed": [
        2025,
        8,
        19,
        18,
        4,
        25,
        1,
        231,
        0
      ],
      "tags": [
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Machine Learning",
          "scheme": null,
          "label": null
        },
        {
          "term": "Artificial Intelligence",
          "scheme": null,
          "label": null
        },
        {
          "term": "Energy (Oil & Gas)",
          "scheme": null,
          "label": null
        },
        {
          "term": "Generative AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "Partner solutions",
          "scheme": null,
          "label": null
        }
      ],
      "id": "acb56cc97880f74e0fe2bf1bcf0e26f9b0742a50",
      "guidislink": false,
      "summary": "We built an advanced RAG solution using Amazon Bedrock leveraging Infosys Topaz™ AI capabilities, tailored for the oil and gas sector. This solution excels in handling multimodal data sources, seamlessly processing text, diagrams, and numerical data while maintaining context and relationships between different data elements. In this post, we provide insights on the solution and walk you through different approaches and architecture patterns explored, like different chunking, multi-vector retrieval, and hybrid search during the development.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "We built an advanced RAG solution using Amazon Bedrock leveraging Infosys Topaz™ AI capabilities, tailored for the oil and gas sector. This solution excels in handling multimodal data sources, seamlessly processing text, diagrams, and numerical data while maintaining context and relationships between different data elements. In this post, we provide insights on the solution and walk you through different approaches and architecture patterns explored, like different chunking, multi-vector retrieval, and hybrid search during the development."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>Enterprises across industries like healthcare, finance, manufacturing, and legal services face escalating challenges in processing vast amounts of multimodal data that combines text, images, charts, and complex technical formats. As organizations generate multimodal content at unprecedented speed and scale, document processing methods increasingly fail to handle the intricacies of specialized domains where technical terminology, interconnected data relationships, and industry-specific formats create operational bottlenecks. These conventional (non-AI) processing approaches struggle with the unique characteristics of enterprise documents: highly technical terminology, complex multimodal data formats, and interconnected information spread across various document types. This results in inefficient data extraction, missed insights, and time-consuming manual processing that hinders organizational productivity and decision-making.One such industry example is oil and gas, which generates vast amounts of complex technical data through drilling operations, presenting significant challenges in data processing and knowledge extraction. These documents, such as detailed well completion reports, drilling logs, and intricate lithology diagrams, contain crucial information that drives operational decisions and strategic planning.</p> \n<p>To overcome such challenges, we built an advanced RAG solution using <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a> leveraging <a href=\"https://www.infosys.com/services/data-ai-topaz.html\" rel=\"noopener noreferrer\" target=\"_blank\">Infosys Topaz</a><img alt=\"™\" class=\"wp-smiley\" src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/2122.png\" style=\"height: 1em;\" /> AI capabilities, tailored for the oil and gas sector. This solution excels in handling multimodal data sources, seamlessly processing text, diagrams, and numerical data while maintaining context and relationships between different data elements. The specialized approach helps organizations unlock valuable insights from their technical documentation, streamline their workflows, and make more informed decisions based on comprehensive data analysis.</p> \n<p>In this post, we provide insights on the solution and walk you through different approaches and architecture patterns explored, like different chunking, multi-vector retrieval, and hybrid search during the development.</p> \n<h2>Solution overview</h2> \n<p>The solution is built using AWS services, including <a href=\"https://aws.amazon.com/ai/generative-ai/nova/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Nova Pro</a>, <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Knowledge Bases</a>, <a href=\"https://aws.amazon.com/opensearch-service/serverless-vector-database/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon OpenSearch Serverless as a Vector Database</a>, <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Titan Text Embeddings</a>, <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-embed.html\" rel=\"noopener noreferrer\" target=\"_blank\">Cohere Embed English model</a>, and BGE Reranker, allowing for scalability and cost-effectiveness. We also used <a href=\"https://aws.amazon.com/q/developer/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Q Developer</a>, an AI-powered assistant for software development for frontend and backend development of our solution powered by Infosys Topaz’s generative AI capabilities. The solution uses distributed processing to handle large volumes of data, so the system can handle a high volume of requests without compromising performance. The real-time indexing system allows for new documents to be incorporated into the system as soon as they are available, so that the information is up-to-date.</p> \n<p>The following are some of the key components of the solution:</p> \n<ul> \n <li><strong>Document processing</strong> – PyMuPDF for PDF parsing, OpenCV for image processing.</li> \n <li><strong>Embedding generation</strong> – Cohere Embed English on Amazon Bedrock for generating vector embeddings of document content and user queries. A hierarchical parent-child chunking architecture that preserves document structure and contextual relationships.</li> \n <li><strong>Vector storage</strong> – <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon OpenSearch Serverless</a> for hybrid search capabilities combining semantic vector search with traditional keyword search (although Amazon Bedrock Knowledge Bases provides a managed RAG solution, this implementation uses a custom RAG architecture to deliver enhanced value and flexibility). This multi-vector retrieval mechanism with separate embedding spaces was required for maintaining the context between textual and visual data.</li> \n <li><strong>Model</strong> – Amazon Nova model for domain-specific response generation.</li> \n <li><strong>Reranking</strong> – BGE reranker, for improving search result relevance by reordering retrieved documents based on semantic similarity to the query.</li> \n</ul> \n<p>The following diagram is a high-level overview of the architecture of the solution.</p> \n<p><img alt=\"Architecture Diagram\" class=\"alignnone size-full wp-image-113119\" height=\"803\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/04/image-1-3.png\" width=\"1242\" /></p> \n<p>Many approaches were used during the build phase to get the desired accuracy. In the following sections, we discuss these approaches in detail.</p> \n<h2>RAG exploration and initial approach</h2> \n<p>The following figure shows some sample images from the oil and well drilling reports. The image on the left is a performance chart of a well drilling operation with the details of the drilling instrument. The image on the top right is of the split sections of the drilling instrument, followed below by the drilling data in a tabular form.</p> \n<p><img alt=\"Drilling Data\" class=\"alignnone size-full wp-image-113120\" height=\"628\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/04/image-2-3.png\" width=\"730\" /></p> \n<p><strong>Image Source : <a href=\"https://public.neats.nopta.gov.au/nopims/wells\" rel=\"noopener noreferrer\" target=\"_blank\">Wells Search | NEATS</a></strong></p> \n<p><strong>© Commonwealth of Australia [<em>year</em> of publishing- 2018]</strong></p> \n<p>Over a thousand such technical images (including lithology diagrams, well completion charts, and drilling visualizations) were preprocessed using Amazon Nova Pro, a multimodal language model. An iterative prompting strategy was employed to generate comprehensive descriptions:</p> \n<ul> \n <li>Initial image analysis to extract basic technical elements</li> \n <li>Refined prompting with domain-specific context to capture specialized terminology</li> \n <li>Multiple inference iterations to provide completeness and accuracy of technical descriptions</li> \n</ul> \n<p>This process converted visual technical information into detailed textual descriptions that preserve the original technical context.The process included the following key components:</p> \n<ul> \n <li><strong>Text content processing</strong> – The textual content from drilling reports was processed using Amazon Titan Text Embedding v2 model with: \n  <ul> \n   <li>Fixed-size chunking of 1,500 tokens with 100-token overlap</li> \n   <li>Preservation of original document structure and technical relationships</li> \n  </ul> </li> \n <li><strong>Image content integration</strong>– The detailed image descriptions generated were integrated without chunking to maintain complete technical context</li> \n <li><strong>Vector storage</strong> – The processed content (chunked text and complete image descriptions) was ingested into an OpenSearch Serverless vector database</li> \n <li><strong>RAG implementation</strong> – RAG-enabled semantic search and retrieval is used across both textual content and image-derived descriptions</li> \n</ul> \n<p>This approach worked well with text questions but was less effective with image-related questions and finding information from images. The lack of a chunking strategy for images resulted in the entire description of each image ingested as a single unit into the search index. This made it difficult for the embedding model to pinpoint exact locations of specific information, especially for technical terms that might be buried within longer descriptions.In the following sections, we discuss the other approaches explored to overcome the limitations presented by each of the previous approaches.</p> \n<h2>Multi-vector embeddings with ColBERT</h2> \n<p>To use a vision model, we created multi-vector embeddings for each image. We then used the ColBERT embedding model for fine-grained text representations. User queries were converted into embeddings, and similarity scores between query and document embeddings were calculated. These embeddings were stored using tensor-based storage, and no chunking was applied. We observed the following:</p> \n<ul> \n <li><strong>Outcome</strong> – We encountered difficulties in storing and managing the complex ColBERT embeddings in traditional vector stores. Debugging and analyzing retrieved documents became cumbersome. Despite context-rich queries, selecting the proper document pages remained challenging.</li> \n</ul> \n<p><strong>Limitations and key learnings</strong> – This approach demonstrated the potential of advanced embedding techniques for image-based document retrieval. However, it also highlighted the challenges in implementing and managing such a system effectively, particularly in the complex domain of oil and gas. Overall, the use of vision models enhanced document understanding, and fine-grained representation of visual and textual content was achieved.</p> \n<h2>Fixed chunking with Amazon Titan Embeddings</h2> \n<p>Adopting a more traditional text-based approach, we introduced a fixed chunking strategy. PDF pages were converted to images, and text content was extracted from these images. A fixed chunking strategy of 500 tokens per chunk was implemented. We used Amazon Bedrock Knowledge Bases with OpenSearch Serverless vector storage, upgraded to Amazon Titan Embeddings v2, and retained the Amazon Nova Pro model. We observed the following:</p> \n<ul> \n <li><strong>Outcome</strong> – The ability to find and retrieve information based on technical keyword searches improved. More focused chunks allowed for a more accurate representation of specific concepts.</li> \n <li><strong>Limitations and key learnings</strong> – Providing comprehensive, long-form answers was challenging. Rigid chunking sometimes split related information across different chunks. This approach underscored the importance of balancing chunk size with information coherence, improving our handling of technical terms but highlighting the need for more sophisticated chunking strategies to maintain context.</li> \n</ul> \n<h2>Parent-child hierarchy with Cohere Embeddings</h2> \n<p>Building on our previous learnings, we introduced a more sophisticated chunking strategy using a parent-child hierarchy. PDF pages were converted to images and text was extracted. We implemented a parent-child chunking hierarchy with parent chunks of 1,500 tokens and child chunks of 512 tokens. We switched to Cohere English embeddings, used Amazon Bedrock Knowledge Bases and OpenSearch Serverless vector storage, and continued using the Amazon Nova Pro model. We observed the following:</p> \n<ul> \n <li><strong>Outcome</strong> – This approach balanced the need for context with the ability to pinpoint specific information. It significantly improved the ability to answer a wide range of queries, maintaining context while offering precise information retrieval.</li> \n <li><strong>Limitations and key learnings</strong> – Careful structuring of content significantly enhanced the performance of both embedding and QnA models. The parent-child structure proved particularly effective for handling the complex, nested nature of oil and gas documentation.</li> \n</ul> \n<h2>Hybrid search with optimized chunking</h2> \n<p>Our final approach retained the advanced features of the previous method while introducing a crucial change in the search methodology. PDF pages were converted to images and text was extracted. We implemented a hybrid search system within the Amazon Bedrock knowledge base. The parent-child chunking hierarchy was retained with parent chunks of 1,200 tokens and child chunks of 512 tokens. We continued using Cohere English embeddings and the Amazon Nova Pro model, and implemented a BGE reranker to refine search results. We observed the following:</p> \n<ul> \n <li><strong>Outcome</strong> – This approach combined the strengths of semantic search and traditional keyword-based search. It addressed the limitations of purely embedding-based searches and improved the handling of specific technical terms and exact phrases.</li> \n <li><strong>Limitations and key learnings</strong> – This final approach represents a highly evolved RAG system, offering the best of both worlds: the ability to understand context and nuance through embeddings, and the precision of keyword matching for specific technical queries.</li> \n</ul> \n<p>The following are some of the tangible results of the hybrid strategy:</p> \n<ul> \n <li>Average query response time: Less than 2 seconds</li> \n <li>Retrieval accuracy (measured against human expert baseline): 92%</li> \n <li>User satisfaction rating: 4.7/5 based on feedback from field engineers and geologists</li> \n</ul> \n<h2>Hybrid RAG approach and optimization strategy</h2> \n<p>Let’s explore the key components and strategies that make the final approach effective for oil and gas drilling reports. Each of the following sections outlines the differentiators in the solution.</p> \n<h3>Multimodal processing capabilities</h3> \n<p>The solution is designed to handle the diverse types of information found in oil and gas documents. The system processes both textual content (technical jargon, well logs, production figures) and visual elements (well schematics, seismic charts, lithology graphs) while maintaining contextual relationships between them. This makes sure that when processing a well completion report, the system can extract key parameters from text, analyze accompanying schematics, and link textual formation descriptions to their visual representations in lithology charts.For example, when processing a well completion report, the system can:</p> \n<ul> \n <li>Extract key parameters from the text (such as total depth and casing sizes)</li> \n <li>Analyze the accompanying well schematic</li> \n <li>Link textual descriptions of formations to their visual representation in lithology charts</li> \n</ul> \n<h3>Domain-specific vocabulary handling</h3> \n<p>The system incorporates a comprehensive dictionary of industry terms and acronyms specific to oil and gas operations. Standard natural language processing (NLP) models often misinterpret technical terminology like “fish left in hole” or fail to recognize specialized abbreviations like “BOP” and “TVD.” By implementing domain-specific vocabulary handling, the system accurately interprets queries and maintains semantic understanding of technical concepts. This helps prevent misinterpretation of critical drilling information and provides relevant document retrieval.For example, when processing a query about “fish left in hole at 5000 ft MD,” the system understands:</p> \n<ul> \n <li>“Fish” refers to lost equipment, not an actual fish</li> \n <li>“MD” means measured depth</li> \n <li>The relevance of this information to drilling operations and potential remediation steps</li> \n</ul> \n<h3>Hybrid hierarchy chunking strategy</h3> \n<p>Traditional fixed-size chunking often breaks apart related technical information, losing critical context in oil and gas documents. The solution implements a hybrid hierarchy approach with parent chunks (1,200 tokens) maintaining document-level context and child chunks (512 tokens) containing detailed technical information. Dynamic chunk sizing adjusts based on content complexity, using natural language processing to identify logical break points. This preserves the integrity of technical descriptions while enabling precise information retrieval across large, complex documents.For example, when processing a well completion report, the system will:</p> \n<ul> \n <li>Create a large parent chunk for the overall well summary</li> \n <li>Generate smaller child chunks for specific sections like casing details or perforation intervals</li> \n <li>Dynamically adjust chunk size for the lithology description based on its complexity</li> \n <li>Implement cross-references between the casing schedule and the well schematic description</li> \n</ul> \n<h3>Multi-vector retrieval implementation</h3> \n<p>Oil and gas documents contain diverse content types that require different retrieval approaches. The system creates separate embedding spaces for text, diagrams, and numerical data, implementing dense vector search for semantic similarity and sparse vector search for exact technical terminology matches. Cross-modal retrieval connects information across different content types, and contextual query expansion automatically includes relevant industry-specific terms. This hybrid approach delivers comprehensive retrieval whether users search for conceptual information or specific technical parameters.For example, for a query like “recent gas shows in Permian Basin wells,” the system will:</p> \n<ul> \n <li>Use dense vector search to understand the concept of “gas shows”</li> \n <li>Use sparse vector search to find exact matches for “Permian Basin”</li> \n <li>Expand the query to include related terms like “hydrocarbon indicators”</li> \n <li>Apply temporal filtering to focus on recent reports</li> \n <li>Use spatial awareness to limit results to the Permian Basin area</li> \n</ul> \n<h3>Temporal and spatial awareness</h3> \n<p>Drilling operations are inherently tied to specific locations and time periods, making context crucial for accurate information retrieval. The system incorporates understanding of well locations and operational timelines, allowing for queries that consider geographical and chronological contexts. For example, searching for “recent gas shows in Permian Basin wells” uses both temporal filtering and spatial awareness to provide relevant, location-specific results. This optimization makes sure retrieved information matches the operational context of the user’s needs.For example, when generating a response about drilling fluid properties, the system will:</p> \n<ul> \n <li>Retrieve relevant information from multiple sources</li> \n <li>Cross-check numerical values for consistency</li> \n <li>Use reflective prompting to make sure critical parameters are addressed</li> \n <li>Apply the reranking model to prioritize the most relevant and accurate information</li> \n <li>Present the response along with confidence scores and source citations</li> \n</ul> \n<h3>Reflective response generation</h3> \n<p>Technical accuracy is paramount in oil and gas operations, where incorrect information can have serious consequences. The system implements reflective prompting mechanisms that prompt the language model to critically evaluate its own responses against source documents and industry standards. Response reranking uses scoring models that evaluate technical accuracy, contextual relevance, and adherence to industry best practices. This multi-layered validation approach makes sure generated responses meet the high accuracy standards required for technical decision-making in drilling operations.</p> \n<h2>Advanced RAG strategies</h2> \n<p>To further enhance our system’s capabilities, we implemented several advanced RAG strategies:</p> \n<ul> \n <li>Hypothetical document embeddings: \n  <ul> \n   <li>Generates synthetic questions based on document content</li> \n   <li>Creates embeddings for these hypothetical questions</li> \n   <li>Improves retrieval for complex, multi-part queries</li> \n   <li>Particularly effective for handling what-if scenarios in drilling operations</li> \n  </ul> </li> \n <li>Recursive retrieval: \n  <ul> \n   <li>Implements multi-hop information gathering</li> \n   <li>Allows the system to follow chains of related information across multiple documents</li> \n   <li>Essential for answering complex queries that require synthesizing information from various sources</li> \n  </ul> </li> \n <li>Semantic routing: \n  <ul> \n   <li>Intelligently routes queries to appropriate knowledge bases or document subsets</li> \n   <li>Optimizes search efficiency by focusing on the most relevant data sources</li> \n   <li>Crucial for handling the diverse types of documents in oil and gas operations</li> \n  </ul> </li> \n <li>Query transformation: \n  <ul> \n   <li>Automatically refines and reformulates user queries for optimal retrieval</li> \n   <li>Applies industry-specific knowledge to interpret ambiguous terms</li> \n   <li>Breaks down complex queries into series of simpler, more targeted searches</li> \n  </ul> </li> \n</ul> \n<p>For example, for a complex query like “Compare the production decline rates of horizontal wells in the Eagle Ford to those in the Bakken over the last 5 years,” the system will:</p> \n<ul> \n <li>Use hypothetical document embeddings to generate relevant sub-questions about decline rates, horizontal wells, and specific formations</li> \n <li>Apply recursive retrieval to gather data from production reports, geological surveys, and economic analyses</li> \n <li>Route different aspects of the query to appropriate knowledge bases (such as separate databases for Eagle Ford and Bakken data)</li> \n <li>Transform the query into a series of more specific searches, considering factors like well completion techniques and reservoir characteristics</li> \n</ul> \n<h2>Business outcome</h2> \n<p>The implementation of this advanced RAG solution has delivered significant business value for oil and gas operations:</p> \n<ul> \n <li><strong>Operational efficiency</strong> – Significant reduction in decision-making time for drilling and field engineers</li> \n <li><strong>Cost optimization</strong> – 40–50% decrease in manual document processing costs through automated information extraction</li> \n <li><strong>Enhanced productivity</strong> – Field engineers and geologists spend 60% less time searching for technical information, focusing instead on high-value analysis</li> \n <li><strong>Risk mitigation</strong> – Consistent 92% retrieval accuracy provides reliable access to critical technical knowledge, reducing operational decision risks</li> \n</ul> \n<h2>Conclusion</h2> \n<p>Our journey in developing this advanced RAG solution for the oil and gas industry demonstrates the power of combining AI techniques with domain-specific knowledge. By addressing the unique challenges of technical documentation in this field, we have created a system that not only retrieves information but understands and synthesizes it in a way that adds real value to operations. Amazon Bedrock is at the center of this solution, with Amazon Q Developer for the application frontend and backend development, and capabilities from Infosys Topaz<img alt=\"™\" class=\"wp-smiley\" src=\"https://s.w.org/images/core/emoji/14.0.0/72x72/2122.png\" style=\"height: 1em;\" /> – an AI-first offering that accelerates business value for enterprises using generative AI.We see significant potential for further advancement, s in this area, such as integration with real-time sensor data for dynamic information retrieval, enhanced visualization capabilities for complex geological and engineering data, and predictive analytics by combining historical retrieval patterns with operational data.</p> \n<p>For more information on Amazon Bedrock and the latest Amazon Nova models, refer to the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock User Guide</a> and <a href=\"https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Nova User Guide</a>.</p> \n<hr /> \n<h3>About the Authors</h3> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-113283 alignleft\" height=\"98\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/dhiraj.jpeg\" width=\"100\" />Dhiraj Thakur </strong>is a Solutions Architect with Amazon Web Services, specializing in Generative AI and data analytics domains. He works with AWS customers and partners to architect and implement scalable analytics platforms and AI-driven solutions. With deep expertise in Generative AI services and implementation, end-to-end machine learning implementation, and cloud-native data architectures, he helps organizations harness the power of GenAI and analytics to drive business transformation. He can be reached via <a href=\"https://www.linkedin.com/in/dhiraj-thakur-14535632/\" rel=\"noopener noreferrer\" target=\"_blank\">LinkedIn</a>.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113288 size-full alignleft\" height=\"128\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/meenakshi.jpeg\" width=\"100\" /><strong>Meenakshi Venkatesan</strong> is a Principal Consultant at Infosys and a part of the AWS partnerships team at Infosys Topaz CoE. She helps in designing, developing, and deploying in AWS environments and has interests in exploring the new offerings and services.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113292 size-full alignleft\" height=\"121\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-5-2-1.png\" width=\"100\" /><strong>Keerthi Prasad</strong> is a Senior Technology Architect at Infosys and a part of the AWS partnerships team at Infosys Topaz CoE. He provides guidance and assistance to customers in building various solutions in the AWS Cloud. He also supports AWS partners and customers in their generative AI adoption journey.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113293 size-full alignleft\" height=\"110\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Suman-Debnath.png\" width=\"100\" /><strong>Suman Debnath</strong> is an Associate Principal at Infosys and a part of Infosys Topaz delivery. He has played multiple roles, such as architect, program manager, and data scientist, building scalable enterprise systems and AI/ML and generative AI applications on the cloud for oil and gas, healthcare, and financial clients.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113294 size-full alignleft\" height=\"126\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/ganesh.png\" width=\"100\" /><strong>Ganesh</strong> is a Enterprise Architect and Data Scientist at Infosys and part of Topaz Delivery. He has a master’s degree in computer science and machine learning. He has played multiple roles such as architect, program manager and data scientist building scalable enterprise systems.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-113295 size-full alignleft\" height=\"94\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/yash.png\" width=\"100\" /><strong>Yash Sharma</strong> is a Digital Specialist Engineer with Infosys and part of the AWS team at ICETS with a passion for emerging generative AI services. He has successfully led and contributed to numerous generative AI projects. He is always eager to expand his knowledge and stay ahead of industry trends, bringing the latest insights and techniques to work.</p> \n<p style=\"clear: both;\"><strong><img alt=\"\" class=\"size-full wp-image-113287 alignleft\" height=\"130\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/karthikeyan.jpeg\" width=\"100\" />Karthikeyan Senthilkumar</strong> is a Senior Systems Engineer at Infosys and a part of the AWSCOE at iCETS. He specializes in AWS services with a focus on emerging technologies.</p>"
        }
      ]
    },
    {
      "title": "Streamline employee training with an intelligent chatbot powered by Amazon Q Business",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Streamline employee training with an intelligent chatbot powered by Amazon Q Business"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/streamline-employee-training-with-an-intelligent-chatbot-powered-by-amazon-q-business/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/streamline-employee-training-with-an-intelligent-chatbot-powered-by-amazon-q-business/",
      "authors": [
        {
          "name": "Neha Bhupatiraju"
        }
      ],
      "author": "Neha Bhupatiraju",
      "author_detail": {
        "name": "Neha Bhupatiraju"
      },
      "published": "Tue, 19 Aug 2025 14:02:50 +0000",
      "published_parsed": [
        2025,
        8,
        19,
        14,
        2,
        50,
        1,
        231,
        0
      ],
      "tags": [
        {
          "term": "Amazon Q",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Q Business",
          "scheme": null,
          "label": null
        },
        {
          "term": "Technical How-to",
          "scheme": null,
          "label": null
        }
      ],
      "id": "7a0ae177ce6347625011534ba7305a9d99376846",
      "guidislink": false,
      "summary": "In this post, we explore how to design and implement custom plugins for Amazon Q Business to create an intelligent chatbot that streamlines employee training by retrieving answers from training materials. The solution implements secure API access using Amazon Cognito for user authentication and authorization, processes multiple document formats, and includes features like RAG-enhanced responses and email escalation capabilities through custom plugins.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we explore how to design and implement custom plugins for Amazon Q Business to create an intelligent chatbot that streamlines employee training by retrieving answers from training materials. The solution implements secure API access using Amazon Cognito for user authentication and authorization, processes multiple document formats, and includes features like RAG-enhanced responses and email escalation capabilities through custom plugins."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p><a href=\"https://aws.amazon.com/q/business/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Q Business</a> is a generative AI-powered assistant for interacting with organizational knowledge and enterprise systems. In addition to providing built-in connectors and plug-ins to connect seamlessly to over 40 popular enterprise systems, Amazon Q Business provides the ability to interact seamlessly with other third-party applications using custom plugins. Some of the enterprise systems that use Amazon Q Business include Salesforce, Zendesk, Confluence, Jira, ServiceNow, and Microsoft SharePoint. With custom plugins, you can integrate Amazon Q Business with various enterprise systems such as ticketing systems, email services, and other business applications, thus facilitating the creation of a comprehensive enterprise solutions. In this post, we explore how to design and implement custom plugins for Amazon Q Business, showcasing practical examples of integrating with common enterprise systems while helping to ensure secure access through <a href=\"https://aws.amazon.com/cognito\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Cognito</a> authentication.</p> \n<h2>Solution overview</h2> \n<p>We will build an Amazon Q Business application serving as an intelligent chatbot that facilitates new employee training by retrieving answers from the provided training materials. The solution implements secure API access using Amazon Cognito for user authentication and authorization, helping to ensure that only authorized users can access the system. It can process documents in multiple formats including PDF, DOC, DOCX, and TXT with a maximum file size of 50 MB per document and can index up to 100,000 documents. The chatbot effectively answers the questions posed by new employees by using Retrieval Augmented Generation (RAG) techniques to enhance its response capabilities. If the chatbot can’t locate the requested information, it presents a dynamic option to the user to submit an email directly to the training support team through the chatbot using the custom plugins for Amazon Q Business. We include an <a href=\"https://aws.amazon.com/cloudformation\" rel=\"noopener noreferrer\" target=\"_blank\">AWS CloudFormation</a> template for deployment and management of our solution.</p> \n<p>The following illustration shows how Amazon Q Business delivers training content using RAG techniques, stores materials in <a href=\"https://aws.amazon.com/s3\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Storage Service (Amazon S3)</a>, processes requests through <a href=\"https://aws.amazon.com/lambda\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Lambda</a> functions, and enables user escalations through a custom plugin. CloudFormation automates the deployment of these integrated services, as shown in the following figure.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-1-6.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113386\" height=\"874\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-1-6.png\" width=\"847\" /></a></p> \n<h2>Features and benefits</h2> \n<p>The solution provides three key capabilities that work together to create an efficient and user-friendly training support system. These features help organizations reduce support overhead while making sure users can get the help they need. The solution’s intelligent query handling uses RAG techniques to process user questions accurately and provide context-aware responses from indexed training materials. This capability reduces the burden on human trainers by enabling employees to find answers up to 10 times faster than traditional search methods. According to AWS case studies, organizations implementing Amazon Q Business have seen significant efficiency gains: support tickets have decreased by up to 30% through enhanced self-service capabilities, while employees save an average of 20–30 hours per month on document search and summarization tasks. The system has demonstrated the ability to handle up to 80% of routine, repetitive questions automatically, leading to 50% faster onboarding and training processes through automated knowledge access. When users need additional support, they can use the dynamic email escalation feature to contact the training team directly with a single click. This seamless integration maintains a smooth user experience while making sure complex or specialized queries receive prompt attention from subject matter experts. Organizations can typically implement this solution within 2–3 business days using a pre-configured CloudFormation template, which will minimize deployment effort and technical overhead. The architecture uses the elastic infrastructure of AWS to scale automatically, supporting enterprise-wide deployments through its ability to process and index millions of documents across multiple data sources. The solution scales according to AWS service quotas, with specific limits on knowledge bases (100,000 documents each), applications (10 per account), and concurrent users (based on your AWS account’s service quotas). The infrastructure automatically adjusts resources based on query volume and user demand, facilitating consistent performance even during peak usage periods.</p> \n<h2>Deployment Steps</h2> \n<p>Use the following steps to set up your training chatbot solution. You will configure email notifications using <a href=\"https://aws.amazon.com/ses\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Email Service (Amazon SES)</a>, create an S3 bucket for training materials, deploy two CloudFormation templates, and set up user access for the Amazon Q Business chatbot.</p> \n<h3>Prerequisites</h3> \n<p>Download the files needed from the S3 bucket:</p> \n<ul> \n <li><a href=\"https://aws-blogs-artifacts-public.s3.us-east-1.amazonaws.com/artifacts/ML-17015/qbusiness-application.yaml\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Q Business application template</a></li> \n <li><a href=\"https://aws-blogs-artifacts-public.s3.us-east-1.amazonaws.com/artifacts/ML-17015/email-plugin.yaml\" rel=\"noopener noreferrer\" target=\"_blank\">Email plugin schema</a></li> \n <li><a href=\"https://aws-blogs-artifacts-public.s3.us-east-1.amazonaws.com/artifacts/ML-17015/Training-Materials.docx\" rel=\"noopener noreferrer\" target=\"_blank\">Mock training materials</a></li> \n</ul> \n<p>Enable <a href=\"https://aws.amazon.com/iam/identity-center\" rel=\"noopener noreferrer\" target=\"_blank\">AWS IAM Identity Center</a>:</p> \n<ol> \n <li>Go to the AWS Management Console and go to AWS IAM Identity Center.</li> \n <li>Choose <strong>Enable IAM Identity Center</strong>.</li> \n <li>Wait a few minutes for the service to be enabled.<br /> <img alt=\"\" class=\"alignnone wp-image-114063 size-full\" height=\"436\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/11/Screenshot-2025-08-11-at-2.47.01 PM.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1506\" /></li> \n</ol> \n<h3>Step 1: Configure the customer service email address on Amazon SES</h3> \n<p>The following steps add the email IDs that will be used to send and receive emails through the custom plugin and Amazon SES.</p> \n<ol> \n <li>Open the <a href=\"https://us-east-1.console.aws.amazon.com/ses/home?region=us-east-1#/account\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon SES console</a>.</li> \n <li>Choose <strong>Configuration</strong> and then <strong>Identities</strong> on the left navigation pane.</li> \n <li>Choose <strong>Create identity </strong>to add an identity.</li> \n <li>Select <strong>Email address</strong> as the identity type.</li> \n <li>Enter the email address you want to use.</li> \n <li>Choose <strong>Create identity</strong> to submit the request.<br /> <img alt=\"\" class=\"alignnone wp-image-114061 size-full\" height=\"210\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/11/Screenshot-2025-08-11-at-2.47.49 PM.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1500\" /></li> \n</ol> \n<ol start=\"7\"> \n <li>Confirm the email address by following the link on the email Amazon SES sends you and then your identity will be confirmed (you should receive the email in about 2 minutes).</li> \n</ol> \n<h3>Step 2: Create an S3 bucket with your training materials in it</h3> \n<p>The following steps create the S3 bucket that will act as a data source for the Amazon Q Business application.</p> \n<ol> \n <li>Open the <a href=\"https://us-east-1.console.aws.amazon.com/s3/home?region=us-east-1\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon S3 console</a>.</li> \n <li>Choose <strong>Create bucket</strong>.</li> \n <li>Enter a unique bucket name (for example, <code>company-training-materials-2025</code>).</li> \n <li>Upload your training materials into this bucket. \n  <ul> \n   <li>Mock training data was part of the material downloaded in the prerequisites.</li> \n  </ul> </li> \n</ol> \n<h3>Step 3: Deploy the first AWS CloudFormation stack – Qbusiness-application.yaml</h3> \n<p>The CloudFormation template will create the necessary resources for deploying the application with the custom plugin.</p> \n<ol> \n <li>In your AWS account, open the <a href=\"https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1\" rel=\"noopener noreferrer\" target=\"_blank\">CloudFormation console</a>.</li> \n <li>Choose <strong>Create stack</strong> and <strong>With new resources (standard) </strong>to start.</li> \n <li>Select <strong>Choose an existing template</strong> and <strong>Upload a template file</strong> and upload <code>Qbusiness-application.yaml</code>.</li> \n <li>Fill in the required parameters (for example: Amazon Cognito details, <code>S3BucketName</code>, <code>SESSourceEmail</code>, <code>IdcInstanceArn</code>). \n  <ol type=\"a\"> \n   <li>You can keep the default names for the roles.</li> \n   <li><code>CognitoUserEmail</code> should be the email address associated with a user’s account within an Amazon user pool.</li> \n   <li><code>CognitoUserPassword</code> is a temporary password associated with the preceding <code>CognitoUserEmail</code>. Add <code>temporary password</code>.</li> \n   <li><code>S3BucketName</code> should be the bucket that has the training and data source materials in it that you created in Step 2 (for example <code>company-training-materials-2025</code>).</li> \n   <li><code>SESSourceEmail</code> should be the email that serves as the customer service email address that users can reach out to for further assistance (the same address that you verified in Step 1).</li> \n   <li><code>IdcInstanceArn</code> is the Amazon Resource Name (ARN) of your IAM Identity Center instance. To find your <code>IdcInstanceArn</code>: \n    <ol> \n     <li>Navigate to IAM Identity Center console.</li> \n     <li>Choose <strong>Go to Settings</strong> on the left-hand side.</li> \n     <li>Example: <code>arn:aws:sso:::instance/ssoins-722339a1b72acd7b</code>.</li> \n    </ol> </li> \n  </ol> </li> \n <li>Choose <strong>Next</strong>, leave the settings on the next page at the default values.</li> \n <li>Select the <strong>I acknowledge that AWS CloudFormation might create IAM resources with custom names</strong> check box, choose <strong>Next</strong> and then choose <strong>Submit</strong> to start the creation of the stack.</li> \n <li>Wait until the stack status is <strong>CREATE_COMPLETE</strong>.</li> \n <li>Navigate to the <strong>Outputs</strong> tab on the stack and copy the <strong>ApiEndpoint</strong>, <strong>CognitoAuthorizationUrl</strong> and <strong>CognitoTokenUrl</strong> to your clipboard.<br /> <img alt=\"\" class=\"alignnone size-full wp-image-114628\" height=\"186\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/18/Screenshot-2025-08-18-at-4.54.31 PM.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1028\" /><br /> <img alt=\"\" class=\"alignnone size-full wp-image-114629\" height=\"310\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/18/Screenshot-2025-08-18-at-4.54.52 PM.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1026\" /></li> \n</ol> \n<h3>Step 4: Prepare the custom plugin schema</h3> \n<p>The following steps help edit the API schema, which has the necessary paths and responses, to call the custom plugin.</p> \n<ol> \n <li>Download the <code>email-plugin.yaml</code> file and open it in an editor.</li> \n <li>In the beginning of the YAML file, paste the API endpoint URL you copied in the previous step prior where it says <strong>Enter ApiEndpoint</strong>. Make sure to remove the backslash at the end of the URL.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-6-3.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113389\" height=\"294\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-6-3.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1222\" /></a></li> \n</ol> \n<ol start=\"3\"> \n <li>Paste the <code>CognitoAuthorizationURL</code> and <code>CognitoTokenUrl</code> from the previous step where it says to enter the URLs, as shown in the previous image.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-7-2.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113390\" height=\"450\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-7-2.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1636\" /></a></li> \n</ol> \n<ol start=\"4\"> \n <li>Save this YAML file.</li> \n</ol> \n<h3>Step 5: Set up the custom plugin</h3> \n<ol> \n <li>Navigate to the Amazon Q Business console and choose <strong>Actions </strong>and then <strong>Plugins</strong> navigation pane.</li> \n <li>Choose <strong>Add Plugin</strong> and then <strong>+Create custom plugin</strong>.</li> \n <li>Enter a name and description for your plugin.</li> \n <li>Under API Schema, select <strong>Define with in-line OpenAPI schema editor </strong>and paste the edited email plugin YAML file with the new URLs in the text box. \n  <ol type=\"a\"> \n   <li>Make sure the YAML | JSON toggle is set to <strong>YAML</strong>.</li> \n  </ol> </li> \n <li>Under <strong>Authentication</strong>, select <strong>Authentication required</strong>.</li> \n <li>Under <strong>AWS Secrets Manager secret</strong> select<strong> Create and add new secret</strong>. \n  <ol type=\"a\"> \n   <li>Enter a name for your secret.</li> \n   <li>Enter a short description (for example : Custom email plugin for Amazon Q Business application)</li> \n   <li>To populate the other details, navigate to the Amazon Cognito console and select your created user pool. \n    <ol type=\"i\"> \n     <li>Under <strong>Recommendations / Set up your app</strong>, choose <strong>EmailSenderPoolClient</strong></li> \n     <li>Copy <strong>Client ID</strong> and <strong>Client secret </strong>into the QBusiness custom plugin screen.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-8-2.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113376\" height=\"450\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-8-2.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1430\" /></a></li> \n     <li>For the OAuth callback URL, copy the Amazon Q Business web application deployed URL and add <code>/oauth/callback.</code> For example: <code>https://xxxxxx.chat.qbusiness.us-east-1.on.aws/oauth/callback</code><br /> <img alt=\"\" class=\"alignnone wp-image-114667 size-full\" height=\"232\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/19/Screenshot-2025-08-19-at-9.31.24 AM-1.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1678\" /></li> \n     <li>Save this URL for a later step.</li> \n    </ol> </li> \n  </ol> </li> \n <li>Select <strong>Create and use a new service role </strong>for authorization.</li> \n <li>Choose <strong>Add plugin</strong>.</li> \n</ol> \n<h3>Step 6: Add a callback URL to Amazon Cognito</h3> \n<p>The following steps help ensure that the callback URL is configured correctly. The callback URL is a user-configured URL where your application receives the authorization code after a user successfully signs in or signs out through the Amazon Cognito hosted UI.</p> \n<ol> \n <li>Navigate to the Amazon Cognito console.</li> \n <li>Choose <strong>App clients </strong>under <strong>Applications </strong>in the navigation pane.</li> \n <li>Choose your app client.</li> \n <li>Choose the <strong>Login pages</strong> tab.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-10-2.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113378\" height=\"387\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-10-2.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1430\" /></a></li> \n</ol> \n<ol start=\"5\"> \n <li>Choose <strong>Edit </strong>under the<strong> Managed login pages configuration</strong>.</li> \n <li>Paste the OAuth callback URL you saved from the previous step under <strong>Allowed Callback URLs</strong>. \n  <ol type=\"a\"> \n   <li>Example: <code>https://xxxxxx.chat.qbusiness.us-east-1.on.aws/oauth/callback</code></li> \n  </ol> </li> \n <li>Choose <strong>Save changes</strong>.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-11-2.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113379\" height=\"1164\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-11-2.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1684\" /></a></li> \n</ol> \n<h3>Step 7: Sync data source</h3> \n<p>The following steps are to make sure that the data source has the most recent updates</p> \n<ol> \n <li>Navigate to the Amazon Q Business console.</li> \n <li>Find your application (should be named <strong>qbusiness-example-app</strong>).</li> \n <li>Select your application and go to the <strong>Data sources</strong> section in the navigation pane.</li> \n <li>Select your data source and initiate a sync by choosing <strong>Sync now</strong> to index the uploaded documents.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-12-2.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113381\" height=\"239\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-12-2.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1430\" /></a></li> \n</ol> \n<ol start=\"5\"> \n <li><strong>Sync history </strong>should show a status of completed.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-13.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113380\" height=\"280\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-13.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1431\" /></a></li> \n</ol> \n<h3>Step 8: Set up user access for the Amazon Q Business chatbot</h3> \n<p>The following steps allow users to be added to access the chatbot</p> \n<ol> \n <li>Navigate back to the application in the Amazon Q Business console.</li> \n <li>Choose <strong>Manage User Access</strong>.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-14-1.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113385\" height=\"222\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-14-1.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1428\" /></a></li> \n</ol> \n<ol start=\"3\"> \n <li>Choose <strong>Add Groups or Users</strong>.</li> \n <li>If you already have an existing user you want to give access to, choose <strong>Assign existing users and groups</strong>. \n  <ol type=\"a\"> \n   <li>Start entering the user’s name and <strong>Assign</strong> an existing user.</li> \n  </ol> </li> \n <li>To add a new user, choose <strong>Add and Assign New Users</strong>. \n  <ol type=\"a\"> \n   <li>Choose <strong>Add and then Assign</strong> and then choose <strong>Confirm</strong>. \n    <ol type=\"i\"> \n     <li>Each user should get an email asking them to accept the invitation.</li> \n     <li>After they choose <strong>Accept Invitation</strong>, they will be taken to a page to set up their password. They will use this username and password to sign in to the chatbot.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.48.41 PM.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113395\" height=\"112\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.48.41 PM.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1148\" /></a></li> \n    </ol> </li> \n  </ol> </li> \n</ol> \n<h3>Step 9: Query the chatbot</h3> \n<p>The following steps walk you through how to best use the application.</p> \n<ol> \n <li>Navigate to the application on the Amazon Q Business console again.</li> \n <li>Select the <strong>Deployed URL</strong>.<br /> <img alt=\"\" class=\"alignnone wp-image-114666 size-full\" height=\"232\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/19/Screenshot-2025-08-19-at-9.31.24 AM.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1678\" /></li> \n</ol> \n<ol start=\"3\"> \n <li>Sign in using the username and password set in the previous step. Follow the steps in the console to register a built-in authenticator.</li> \n <li>You can now: \n  <ol type=\"a\"> \n   <li>Query the chatbot about your training materials. For example, ask it: <code>Tell me about my dental insurance.</code></li> \n   <li>Request to send emails through the chatbot. Make sure to choose the custom plugin button to use this functionality. \n    <ol type=\"i\"> \n     <li>Example: <code>Can you send an email asking for more information about my dental insurance?</code></li> \n     <li>If asked for your email ID, enter the email ID that you verified in previous steps.</li> \n     <li>The application will ask you to authorize your access. Choose <strong>Authorize</strong>.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-17-1.png\"><img alt=\"\" class=\"alignnone size-full wp-image-113383\" height=\"186\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-17-1.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"1432\" /></a></li> \n    </ol> </li> \n  </ol> </li> \n <li>Enter your <code>CognitoUserEmail</code> and password defined in Step 3 in the window and choose <strong>Sign in</strong>.<br /> <a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-18-1.png\"><img alt=\"\" class=\"wp-image-114000 size-full alignnone\" height=\"473\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/11/ML-17015-image-18-1.png\" style=\"margin: 10px 0px 10px 0px;\" width=\"500\" /></a></li> \n <li>Emails will be sent to the email address verified in Step 1.</li> \n</ol> \n<h2>Troubleshooting</h2> \n<p>The following issues might occur during deployment or usage of your chatbot solution. Use these solutions to resolve common problems.</p> \n<p>Data sync failures typically result from incorrect S3 bucket permissions. Check your bucket policy and access settings to facilitate proper configuration.</p> \n<ul> \n <li>User access issues often occur when invitations aren’t accepted or passwords aren’t set up. Verify that users have completed both steps in the access setup process.</li> \n <li>When the bot provides incomplete answers, try refreshing your content by initiating a new data source sync in the Amazon Q Business console.</li> \n <li>Amazon Cognito authorization issues can occur in the Q Business console – to mitigate them:</li> \n <li>Make sure that the callback URL matches your Q Business deployed URL</li> \n <li>The callback URL in the managed sign in pages configuration matches your Q Business deployed URL as seen in Step 6.</li> \n <li>Your Amazon Cognito URLs are copied into the <code>email-plugin.yaml</code> correctly from the CloudFormation outputs.</li> \n <li>Email delivery problems usually stem from email configuration. Verify that:</li> \n <li>The <strong>Custom Plugin</strong> button shows an active (blue) status.</li> \n <li>Your Amazon SES email address is verified.</li> \n <li>You’re using the correct email address in your configuration.</li> \n</ul> \n<p>The BlueprintRole section is currently commented out because this is a proof-of-concept deployment. When deploying to production environments, especially those involving multiple AWS accounts or organizations, you should uncomment this section. The BlueprintRole provides necessary permissions for cross-account access and advanced management features of Amazon Q Business applications.</p> \n<h2>Service versions</h2> \n<p>AWS Lambda runtime: Python 3.11</p> \n<h2>Regional availability</h2> \n<p>Before deploying this solution, note that Amazon Q Business is not available in all AWS Regions. This solution can only be deployed in Regions where Amazon Q Business is supported. For the most up-to-date information on Regional availability, check the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Regional Services</a> list.</p> \n<h2>Real-world use case</h2> \n<p>Amazon Q Business is transforming how businesses handle internal knowledge management and support. By securely connecting to company data sources and systems, Amazon Q Business helps organizations make their institutional knowledge more accessible and actionable. Here are two real-world examples demonstrating how Amazon Q Business enhances workplace productivity and knowledge sharing using the custom plugin feature.</p> \n<p>Scenario 1: New employee Sarah uses a chatbot to learn about the organization’s leave policy. The chatbot efficiently retrieves relevant information from indexed training materials to answer her initial question. When Sarah later asks a specific question beyond the scope of the chatbot’s knowledge base, it promptly offers to connect her with the training support team through email. Sarah takes advantage of this option, making sure her complex query receives proper attention without delay. This interaction demonstrates the chatbot’s effectiveness in providing immediate access to information while maintaining appropriate escalation channels for questions requiring human expertise.</p> \n<p>Scenario 2: Alex, a field technician at a manufacturing company, needs to complete an urgent maintenance procedure on specialized equipment while at a client site. He accesses the company’s Amazon Q-powered knowledge assistant.</p> \n<ul> \n <li>Alex asks, “How do I recalibrate the XB-2000 sensor array after firmware update?” The chatbot immediately retrieves the relevant technical documentation from indexed maintenance manuals and presents a step-by-step procedure with details.</li> \n <li>During the calibration, Alex encounters an unexpected error code not covered in the standard documentation. He uses the custom plugin to request immediate assistance, typing “I need help with error code E-457 on the XB-2000.” The chatbot offers to email the technical support team, including his location details and equipment specifications automatically gathered from his user profile.</li> \n</ul> \n<p>This scenario demonstrates how the Amazon Q Business solution delivers critical technical knowledge in field situations while providing seamless escalation paths for edge cases that require specialized expertise, ultimately reducing equipment downtime and improving customer satisfaction.</p> \n<p>Scenario 3: A global manufacturing company with more than 5,000 employees implements Amazon Q Business to streamline their equipment maintenance support system across multiple facilities. Maintenance teams use Amazon Q Business to access equipment documentation and when encountering situations requiring vendor support or parts ordering, they use the custom plugin’s email feature.</p> \n<ul> \n <li>Example interaction: A maintenance supervisor in Singapore enters “Need to escalate XB-2000 production line shutdown to vendor support team.”</li> \n</ul> \n<p>The Amazon Q custom plugin automatically:</p> \n<ul> \n <li>Generates a structured email containing facility location, equipment history, and maintenance logs.</li> \n <li>Routes communications to vendor support, maintenance management, and procurement teams.</li> \n</ul> \n<p>This implementation demonstrates how the custom plugin feature standardizes emergency communications across global facilities while making sure critical information is automatically included in escalations.</p> \n<h2>Clean up</h2> \n<p>To remove the solution, delete the CloudFormation stack you created to test this solution. This action will automatically deprovision associated AWS resources, including Lambda functions, S3 buckets, and Amazon OpenSearch Service domains set up by Amazon Q Business. This solution uses multiple AWS services with costs varying based on usage patterns. Amazon Q Business pricing is determined by the number of users and queries processed, with additional charges applying for custom plugin usage. Lambda costs are calculated based on the number of requests and compute time, though a free tier allowance of 1 million requests per month is available. Storage and data transfer costs will apply for Amazon S3, which hosts your training materials. Email communications through Amazon SES incur standard sending charges, though you can benefit from a free tier that includes 62,000 outbound messages per month. For detailed pricing information, we recommend consulting the official pricing pages for each service.</p> \n<h2>Conclusion</h2> \n<p>This intelligent chatbot solution harnesses the capabilities of Amazon Q to revolutionize employee training by providing instant access to organizational knowledge while maintaining human escalation paths for complex inquiries. Queries the system is designed to handle include multi-turn conversations requiring context from previous interactions, questions that need information synthesis across multiple documents, and technical troubleshooting scenarios requiring step-by-step guidance. By implementing this CloudFormation-automated deployment, organizations can significantly reduce support costs up to 85%, improve knowledge accessibility, and create a training environment that is designed to scale with their needs. It supports enterprise-wide deployments by integrating with your existing identity and access management systems, so you can quickly add or remove users and manage permissions at scale. As your organization expands, you can connect additional data sources such as Dropbox and Google Drive, making sure the system grows alongside your business needs.</p> \n<p>This Amazon Q Business training solution is worth building because it dramatically reduces training support costs while providing employees with continuous access to accurate information. The automated deployment and seamless human escalation path make it an ideal solution for organizations looking to scale knowledge delivery without expanding support staff.</p> \n<p>Empower your organization with this cutting-edge chatbot solution today and share your experiences and insights in the comments section below.</p> \n<p>Explore more about Amazon Q Business capabilities in our <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/what-is.html\" rel=\"noopener noreferrer\" target=\"_blank\">comprehensive documentation</a> or join our AWS Community forum to connect with others implementing similar solutions. Don’t forget to follow #AmazonQBusiness on social media to share your implementation journey!</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.55.09 PM.png\"><img alt=\"\" class=\"wp-image-113398 size-thumbnail alignleft\" height=\"126\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.55.09 PM-e1754427435607-100x126.png\" width=\"100\" /></a>Neha Bhupatiraju</strong> is a Data and ML Engineer at AWS Professional Services. With expertise in data engineering and machine learning, she helps enterprise customers leverage both traditional data analytics and machine learning. She specializes in implementing intelligent chatbots, developing predictive analytics models and building generative AI applications.</p> \n<p style=\"clear: both;\"><b><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.55.26 PM.png\"><img alt=\"\" class=\"size-thumbnail wp-image-113399 alignleft\" height=\"119\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.55.26 PM-100x119.png\" width=\"100\" /></a>Charishma Ravoori</b> is a Data and ML Engineer at AWS Professional Services. Charishma works with AWS customers and partners to help them build solutions in predictive/data analytics, data engineering and generative AI using AWS services.</p> \n<p style=\"clear: both;\"><b><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.56.05 PM.png\"><img alt=\"\" class=\"size-thumbnail wp-image-113401 alignleft\" height=\"132\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.56.05 PM-100x132.png\" width=\"100\" /></a>Ujwala Bitla</b> is a Deep Learning Architect at AWS Generative AI Innovation Center, where she designs and delivers cutting-edge GenAI solutions for&nbsp; customers across industries. With extensive experience in Data Science and Analysis, she specializes in Large Language Models, Retrieval-Augmented Generation (RAG), Agents and responsible AI implementation.</p> \n<p style=\"clear: both;\"><b><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.55.48 PM.png\"><img alt=\"\" class=\"size-thumbnail wp-image-113400 alignleft\" height=\"131\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/Screenshot-2025-08-05-at-4.55.48 PM-100x131.png\" width=\"100\" /></a>Raju Patil</b> is a Senior Data Scientist in AWS Professional Services, where he builds and deploys AI/ML solutions to help AWS customers overcome business challenges. His work spans across various use cases, including Generative AI, Computer Vision, Time-Series Forecasting, and Predictive Analytics.</p>"
        }
      ]
    },
    {
      "title": "Create a travel planning agentic workflow with Amazon Nova",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Create a travel planning agentic workflow with Amazon Nova"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/create-a-travel-planning-agentic-workflow-with-amazon-nova/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/create-a-travel-planning-agentic-workflow-with-amazon-nova/",
      "authors": [
        {
          "name": "Isaac Privitera"
        }
      ],
      "author": "Isaac Privitera",
      "author_detail": {
        "name": "Isaac Privitera"
      },
      "published": "Mon, 18 Aug 2025 20:30:15 +0000",
      "published_parsed": [
        2025,
        8,
        18,
        20,
        30,
        15,
        0,
        230,
        0
      ],
      "tags": [
        {
          "term": "Advanced (300)",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Nova",
          "scheme": null,
          "label": null
        },
        {
          "term": "Generative AI",
          "scheme": null,
          "label": null
        }
      ],
      "id": "76148e40b63152f8f90e52460fc904c981ac9008",
      "guidislink": false,
      "summary": "In this post, we explore how to build a travel planning solution using AI agents. The agent uses Amazon Nova, which offers an optimal balance of performance and cost compared to other commercial LLMs. By combining accurate but cost-efficient Amazon Nova models with LangGraph orchestration capabilities, we create a practical travel assistant that can handle complex planning tasks while keeping operational costs manageable for production deployments.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we explore how to build a travel planning solution using AI agents. The agent uses Amazon Nova, which offers an optimal balance of performance and cost compared to other commercial LLMs. By combining accurate but cost-efficient Amazon Nova models with LangGraph orchestration capabilities, we create a practical travel assistant that can handle complex planning tasks while keeping operational costs manageable for production deployments."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>Traveling is enjoyable, but travel planning can be complex to navigate and a hassle. Travelers must book accommodations, plan activities, and arrange local transportation. All these decisions can feel overwhelming. Although travel professionals have long helped manage these complexities, recent breakthroughs in <a href=\"https://aws.amazon.com/what-is/generative-ai/\" rel=\"noopener noreferrer\" target=\"_blank\">generative AI</a> have made something entirely new possible—intelligent assistants that can understand natural conversation, access real-time data, and directly interface with booking systems and travel tools. <a href=\"https://aws.amazon.com/what-is/ai-agents/\" rel=\"noopener noreferrer\" target=\"_blank\">Agentic workflows</a>, which use large language models (LLMs) with access to external tools, are particularly promising for simplifying dynamic, multi-step processes like travel planning.</p> \n<p>In this post, we explore how to build a travel planning solution using AI agents. The agent uses <a href=\"https://aws.amazon.com/ai/generative-ai/nova/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Nova</a>, which offers an optimal balance of performance and cost compared to other commercial LLMs. By combining accurate but cost-efficient Amazon Nova models with <a href=\"https://www.langchain.com/langgraph\" rel=\"noopener noreferrer\" target=\"_blank\">LangGraph</a> orchestration capabilities, we create a practical travel assistant that can handle complex planning tasks while keeping operational costs manageable for production deployments.</p> \n<h2>Solution overview</h2> \n<p>Our solution is built on a serverless <a href=\"http://aws.amazon.com/lambda\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Lambda</a> architecture using <a href=\"https://www.docker.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Docker</a> containers and implements a comprehensive three-layer approach: frontend interaction, core processing, and integration services. In the core processing layer, we use LangGraph, a stateful orchestration framework, to create a sophisticated yet flexible agent-based system that manages the complex interactions required for travel planning.</p> \n<p>The core of our system is a graph architecture where components (nodes) handle distinct aspects of travel planning, with the router node orchestrating the flow of information between them. We use Amazon Nova, a new generation of state-of-the-art foundation models (FMs) available exclusively on <a href=\"https://aws.amazon.com/bedrock/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a> that delivers frontier intelligence with industry-leading price-performance. The router node uses an LLM to analyze each user query and, with access to the description of our 14 action nodes, decides which ones need to be executed. The action nodes, each with their own LLM chain, powered by either Amazon Nova Pro or Amazon Nova Lite models, manage various functions, including web research, personalized recommendations, weather lookups, product searches, and shopping cart management.</p> \n<p>We use Amazon Nova Lite for the router and simpler action nodes. It can handle query analysis and basic content generation with its lightning-fast processing while maintaining strong accuracy at a low cost. Five complex nodes use Amazon Nova Pro for tasks requiring advanced instruction following and multi-step operations, such as detailed travel planning and recommendations. Both models support a 300,000-token context window and can process text, image, and video inputs. The models support text processing across more than 200 languages, helping our travel assistant serve a global audience.The integration layer unifies multiple data sources and services through an interface:</p> \n<ul> \n <li><a href=\"https://webservices.amazon.com/paapi5/documentation/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Product Advertising API</a> for travel-related product recommendations</li> \n <li>Google Custom Search API for real-time travel information</li> \n <li>OpenWeather API for accurate weather forecasts</li> \n <li><a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Knowledge Bases</a> for travel destination insights</li> \n <li><a href=\"https://aws.amazon.com/dynamodb/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon DynamoDB</a> for persistent storage of user profiles and chat history</li> \n</ul> \n<p>These integrations serve as examples, and the architecture is designed to be extensible, so organizations can quickly incorporate their own APIs and data sources based on specific requirements.</p> \n<p>The agent keeps track of the conversation state using AgentState (<a href=\"https://mypy.readthedocs.io/en/stable/typed_dict.html\" rel=\"noopener noreferrer\" target=\"_blank\">TypedDict</a>), a special Python dictionary that helps prevent data errors by enforcing specific data types. It stores the information we need to know about each user’s session: their conversation history, profile information, processing status, and final outputs. This makes sure the different action nodes can access and update information reliably.</p> \n<p>The following diagram illustrates the solution architecture.</p> \n<p><img alt=\"\" class=\"alignnone wp-image-112812 size-large\" height=\"557\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18031-image-4-1024x557.png\" width=\"1024\" /></p> \n<p>The travel assistant processes user interactions from end to end:</p> \n<ol> \n <li>Users interact with a React.js web application through a chat interface.</li> \n <li>Their requests are authenticated using <a href=\"https://aws.amazon.com/cognito/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Cognito</a> and routed through <a href=\"https://aws.amazon.com/api-gateway/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon API Gateway</a>.</li> \n <li>Authenticated requests are sent to our backend <a href=\"https://aws.amazon.com/lambda/\" rel=\"noopener noreferrer\" target=\"_blank\">Lambda functions</a>, which host the core agent workflow.</li> \n <li>API credentials are securely stored using <a href=\"https://aws.amazon.com/secrets-manager/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Secrets Manager</a>, following best practices to make sure these sensitive keys are never exposed in code or configuration files, with appropriate access controls and rotation policies implemented.</li> \n <li>The Travel Assistant Agent itself consists of several interconnected components. At the center, the agent router analyzes incoming queries and orchestrates the workflow.</li> \n <li>The agent maintains state through three DynamoDB tables that store conversation history, shopping wishlists, and user profiles, making sure context is preserved across interactions.</li> \n <li>For travel-specific knowledge, the system uses a combination of Amazon Bedrock Knowledge Bases, <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon OpenSearch Serverless</a>, and a document store in <a href=\"http://aws.amazon.com/s3\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Simple Storage Service</a> (Amazon S3). These components work together to provide accurate, relevant travel information when needed.</li> \n <li>The agent’s action nodes handle specialized tasks by combining LLM chains with external APIs. When users need product recommendations, the system connects to the Amazon Product Advertising API. For general travel information, it uses the Google Custom Search API, and for weather-related queries, it consults the OpenWeather API. API credentials are securely managed through Secrets Manager.</li> \n <li>The system formulates comprehensive responses based on collected information, and the final responses are returned to the user through the chat interface.</li> \n</ol> \n<p>This architecture supports both simple queries that can be handled by a single node and complex multi-step interactions that require coordination across multiple components. The system can scale horizontally, and new capabilities can be added by introducing additional action nodes and API integrations.</p> \n<p>You can deploy this solution using the <a href=\"https://aws.amazon.com/cdk/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Cloud Development Kit</a> (AWS CDK), which generates an <a href=\"http://aws.amazon.com/cloudformation\" rel=\"noopener noreferrer\" target=\"_blank\">AWS CloudFormation</a> template that handles the necessary resources, including Lambda functions, DynamoDB tables, and API configurations. The deployment creates the required AWS resources and outputs the API endpoint URL for your frontend application.</p> \n<h2>Prerequisites</h2> \n<p>For this walkthrough, you must have the following prerequisites:</p> \n<ul> \n <li>An active <a href=\"https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&amp;client_id=signup\" rel=\"noopener noreferrer\" target=\"_blank\">AWS account</a> and familiarity with FMs, Amazon Bedrock, and <a href=\"https://aws.amazon.com/opensearch-service/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon OpenSearch Service</a></li> \n <li><a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html\" rel=\"noopener noreferrer\" target=\"_blank\">Access</a> to the Amazon Nova FMs on Amazon Bedrock</li> \n <li><a href=\"https://nodejs.org/en\" rel=\"noopener noreferrer\" target=\"_blank\">Node.js</a> v16.x or later</li> \n <li>Python 3.9 or later</li> \n <li>Access to the Product Advertising API (PAAPI)</li> \n</ul> \n<h2>Clone the repository</h2> \n<p>Start by cloning the GitHub repository containing the solution files:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">git clone https://github.com/aws-samples/sample-travel-assistant-agent.git</code></pre> \n</div> \n<h2>Obtain API keys</h2> \n<p>The solution requires API keys from three services to enable its core functionalities:</p> \n<ul> \n <li><strong>OpenWeather API</strong> – Create a Free Access account at <a href=\"https://openweathermap.org/\" rel=\"noopener noreferrer\" target=\"_blank\">OpenWeather</a> to obtain your API key. The free tier (60 calls per minute) is sufficient for testing and development.</li> \n <li><strong>Google Custom Search API</strong> – Set up the search functionality through <a href=\"https://console.cloud.google.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Google Cloud Console</a>. Create or select a project and enable the Custom Search API. Then, generate an API key from the credentials section. Create a search engine at <a href=\"https://programmablesearch.google.com/controlpanel/all\" rel=\"noopener noreferrer\" target=\"_blank\">Programmable Search</a> and note your Search Engine ID. The free tier includes 100 queries per day.</li> \n <li><strong>(Optional) Amazon Product Advertising API (PAAPI) </strong>– If you want to enable product recommendations, access the <a href=\"https://webservices.amazon.com/paapi5/documentation/\" rel=\"noopener noreferrer\" target=\"_blank\">PAAPI Documentation Portal</a> to generate your API keys. You will receive both a public key and a secret key. You must have an Amazon Associates account to access these credentials. If you’re new to the Amazon Associates Program, complete the application process first. Skip this step if you don’t want to use PAAPI features.</li> \n</ul> \n<h2>Add API keys to Secrets Manager</h2> \n<p>Before deploying the solution, you must securely store your API keys in Secrets Manager. The following table lists the secrets to create and their JSON structure. For instructions to create a secret, refer to <a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_secret.html\" rel=\"noopener noreferrer\" target=\"_blank\">Create an AWS Secrets Manager secret</a>.</p> \n<table border=\"1px\" cellpadding=\"10px\" class=\"styled-table\"> \n <tbody> \n  <tr> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><strong>Secret Name</strong></td> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><strong>JSON Structure</strong></td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><code>openweather_maps_keys</code></td> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><code>{\" openweather_key\": \"YOUR_API_KEY\"}</code></td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><code>google_search_keys</code></td> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><code>{\"cse_id\": \"YOUR_SEARCH_ENGINE_ID\", \"google_api_key\": \"YOUR_API_KEY\"}</code></td> \n  </tr> \n  <tr> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><code>paapi_keys</code></td> \n   <td style=\"padding: 10px; border: 1px solid #dddddd;\"><code>{\"paapi_public\": \"YOUR_PUBLIC_KEY\", \"paapi_secret\": \"YOUR_SECRET_KEY\"}</code></td> \n  </tr> \n </tbody> \n</table> \n<h2>Configure environment variables</h2> \n<p>Create a .env file in the project root with your configuration:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">STACK_NAME=TravelAssistantAgent\n\n# Optional: Create Bedrock Knowledge Base with documents\nKB_DOCS_PATH = Path/to/your/documents/folder\n# Optional: Enable/disable Product Search features with PAAPI\nUSE_PAAPI=false</code></pre> \n</div> \n<h2>Deploy the stack</h2> \n<p>If this is your first time using the AWS CDK in your AWS account and AWS Region, bootstrap your environment:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">cdk bootstrap</code></pre> \n</div> \n<p>Deploy the solution using the provided script, which creates the required AWS resources, including Lambda functions, DynamoDB tables, and API configurations:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">sh deploy.sh</code></pre> \n</div> \n<h2>Access your application</h2> \n<p>When the deployment is complete, open the AWS CloudFormation console and open your stack. On the <strong>Outputs</strong> tab, note the following values:</p> \n<ul> \n <li><strong>WebAppDomain</strong> – Your application’s URL</li> \n <li><strong>UserPoolId</strong> – Required for user management</li> \n <li><strong>UserPoolClientId</strong> – Used for authentication</li> \n</ul> \n<p><img alt=\"\" class=\"alignnone wp-image-112809 size-large\" height=\"708\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18031-IMAGE-1-1024x708.png\" style=\"width: 1024; height: auto;\" width=\"1024\" /></p> \n<h2>Create an Amazon Cognito user</h2> \n<p>Complete the following steps to create an Amazon Cognito user:</p> \n<ol> \n <li>On the Amazon Cognito console, choose <strong>User pools</strong> in the navigation pane.</li> \n <li>Choose your user pool.</li> \n <li>Choose <strong>Users </strong>in the navigation pane, then choose <strong>Create user</strong>.</li> \n</ol> \n<p><img alt=\"\" class=\"alignnone wp-image-112810 size-large\" height=\"567\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18031-IMAGE-2-1024x567.png\" style=\"width: 1024; height: auto;\" width=\"1024\" /></p> \n<ol start=\"4\"> \n <li>For <strong>Email address</strong>, enter an email address, and select <strong>Mark email address as verified</strong>.</li> \n <li>For <strong>Password</strong>, enter a temporary password.</li> \n <li>Choose <strong>Create user</strong>.</li> \n</ol> \n<p><img alt=\"\" class=\"alignnone wp-image-112811 size-large\" height=\"728\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18031-IMAGE-3-1024x728.png\" style=\"width: 1024; height: auto;\" width=\"1024\" /></p> \n<p>You can use these credentials to access your application at the WebAppDomain URL.</p> \n<h2>Test the solution</h2> \n<p>To test the agent’s capabilities, we created a business traveler persona and simulated a typical travel planning conversation flow. We focused on routing, function calling accuracy, response quality, and latency metrics. The agent’s routing system directs the user questions to the appropriate specialized node (for example, searching for accommodations, checking weather conditions, or suggesting travel products). Throughout the conversation, the agent maintains the context of previously discussed details, so it can build upon earlier responses while providing relevant new information. For example, after discussing travel destination, the agent can naturally incorporate this into subsequent weather and packing list recommendations.</p> \n<p>The following screenshots demonstrate the end-user experience, while the underlying API interactions are handled seamlessly on the backend. The complete implementation details, including Lambda function code and API integration patterns, are available in our <a href=\"https://github.com/aws-samples/sample-travel-assistant-agent/tree/main\" rel=\"noopener\" target=\"_blank\">GitHub repository</a>.</p> \n<div style=\"margin: 20px 0;\">\n <img alt=\"\" class=\"alignnone wp-image-112813 size-large\" height=\"385\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18031-image-5-1024x385.png\" style=\"width: 80%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" width=\"1024\" />\n</div> \n<p>The solution demonstrates personalization capabilities using sample user profiles stored in DynamoDB, containing upcoming trips and travel preferences. In production deployments, these profiles can be integrated with existing customer databases and reservation systems to provide a personalized assistance.</p> \n<div style=\"margin: 20px 0;\">\n <img alt=\"\" class=\"alignnone wp-image-112814 size-large\" height=\"992\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18031-image-6-1024x992.png\" style=\"width: 80%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" width=\"1024\" />\n</div> \n<div style=\"margin: 20px 0;\">\n <img alt=\"\" class=\"alignnone wp-image-112815 size-large\" height=\"571\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18031-image-7-1024x571.png\" style=\"width: 80%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" width=\"1024\" />\n</div> \n<div style=\"margin: 20px 0;\">\n <img alt=\"\" class=\"alignnone wp-image-112816 size-large\" height=\"623\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18031-image-8-1024x623.png\" style=\"width: 80%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" width=\"1024\" />\n</div> \n<p>The product recommendations shown are live links to actual items available on Amazon.com, so the user can explore or purchase these products directly. The user can choose a link to check out the product, or choose <strong>Add to Amazon Cart</strong> to see the items in their shopping cart.</p> \n<div style=\"margin: 20px 0;\">\n <img alt=\"\" class=\"alignnone wp-image-112816 size-large\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/06/ML-18031-Image-9-V3.jpeg\" style=\"width: 80%; height: auto; border: 1px solid #ddd; border-radius: 4px;\" />\n</div> \n<h3>Clean up</h3> \n<p>After you are done experimenting with the travel assistant, you can locate the CloudFormation stack on the AWS CloudFormation console and delete it. This will delete the resources you created.</p> \n<h2>Conclusion</h2> \n<p>Our travel planning assistant agent demonstrates a practical application built by Amazon Nova and LangGraph for solving real-world business challenges. The system streamlines complex travel planning while naturally integrating product recommendations through specialized processing nodes and real-time data integration. Amazon Nova Lite models showed reasonable performance at task orchestration, and Amazon Nova Pro performed well for more complex function calling operations. Looking ahead, this framework could be implemented with more dynamic orchestration systems such as ReAct. To build your own implementation, explore our code samples in the <a href=\"https://github.com/aws-samples/sample-travel-assistant-agent/tree/main\" rel=\"noopener\" target=\"_blank\">GitHub repository</a>.</p> \n<p>For those looking to deepen their understanding of LLM-powered agents, AWS provides extensive resources on building intelligent systems. The <a href=\"https://aws.amazon.com/bedrock/agents/?utm_source=chatgpt.com\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock Agents documentation</a> offers insights into automating multistep tasks with FMs, and the <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/tree/main/agents-and-function-calling\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Bedrock Agent Samples GitHub repo</a> provides guidance for implementing multiple agent applications using Amazon Bedrock.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-medium alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/01/ML-18031-image-10.jpeg\" width=\"100\" /><strong>Isaac Privitera</strong> is a Principal Data Scientist with the AWS Generative AI Innovation Center, where he develops bespoke generative AI-based solutions to address customers’ business problems. His primary focus lies in building responsible AI systems, using techniques such as RAG, multi-agent systems, and model fine-tuning. When not immersed in the world of AI, Isaac can be found on the golf course, enjoying a football game, or hiking trails with his loyal canine companion, Barry.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-medium alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/ML-18031-image-11.jpeg\" width=\"120\" /><strong>Ryan Razkenari</strong> is a Deep Learning Architect at the AWS Generative AI Innovation Center, where he uses his expertise to create cutting-edge AI solutions. With a strong background in AI and analytics, he is passionate about building innovative technologies that address real-world challenges for AWS customers.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"size-medium alignleft\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/01/ML-18031-image-12.jpeg\" width=\"100\" /><strong>Sungmin Hong</strong> is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he helps expedite a variety of use cases for AWS customers. Before joining Amazon, Sungmin was a postdoctoral research fellow at Harvard Medical School. He holds a PhD in Computer Science from New York University. Outside of work, Sungmin enjoys hiking, reading, and cooking.</p>"
        }
      ]
    },
    {
      "title": "Introducing Amazon Bedrock AgentCore Gateway: Transforming enterprise AI agent tool development",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Introducing Amazon Bedrock AgentCore Gateway: Transforming enterprise AI agent tool development"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-agentcore-gateway-transforming-enterprise-ai-agent-tool-development/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-agentcore-gateway-transforming-enterprise-ai-agent-tool-development/",
      "authors": [
        {
          "name": "Dhawalkumar Patel"
        }
      ],
      "author": "Dhawalkumar Patel",
      "author_detail": {
        "name": "Dhawalkumar Patel"
      },
      "published": "Fri, 15 Aug 2025 18:04:57 +0000",
      "published_parsed": [
        2025,
        8,
        15,
        18,
        4,
        57,
        4,
        227,
        0
      ],
      "tags": [
        {
          "term": "Amazon Bedrock",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Machine Learning",
          "scheme": null,
          "label": null
        },
        {
          "term": "Artificial Intelligence",
          "scheme": null,
          "label": null
        },
        {
          "term": "Generative AI",
          "scheme": null,
          "label": null
        },
        {
          "term": "Intermediate (200)",
          "scheme": null,
          "label": null
        }
      ],
      "id": "083fb26f5829b5108551a3ffd20466da52a176c1",
      "guidislink": false,
      "summary": "In this post, we discuss Amazon Bedrock AgentCore Gateway, a fully managed service that revolutionizes how enterprises connect AI agents with tools and services by providing a centralized tool server with unified interface for agent-tool communication. The service offers key capabilities including Security Guard, Translation, Composition, Target extensibility, Infrastructure Manager, and Semantic Tool Selection, while implementing sophisticated dual-sided security architecture for both inbound and outbound connections.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In this post, we discuss Amazon Bedrock AgentCore Gateway, a fully managed service that revolutionizes how enterprises connect AI agents with tools and services by providing a centralized tool server with unified interface for agent-tool communication. The service offers key capabilities including Security Guard, Translation, Composition, Target extensibility, Infrastructure Manager, and Semantic Tool Selection, while implementing sophisticated dual-sided security architecture for both inbound and outbound connections."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>To fulfill their tasks, AI Agents need access to various capabilities including tools, data stores, prompt templates, and other agents. As organizations scale their AI initiatives, they face an exponentially growing challenge of connecting each agent to multiple tools, creating an M×N integration problem that significantly slows development and increases complexity.</p> \n<p>Although protocols such as <a href=\"https://modelcontextprotocol.io/docs/getting-started/intro\" rel=\"noopener noreferrer\" target=\"_blank\">Model Context Protocol</a> (MCP) and Agent2Agent (A2A) have emerged to address interoperability, implementing these solutions requires substantial engineering effort. Organizations must build MCP servers, convert existing APIs, manage infrastructure, build intelligent tools discovery, and implement security controls, all that while maintaining these integrations over time as protocols rapidly evolve and new major versions are released. As deployments grow to hundreds of agents and thousands of tools, enterprises need a more scalable and manageable solution.</p> \n<h2>Introducing Amazon Bedrock AgentCore Gateway</h2> \n<p>We’re excited to announce <a href=\"https://aws.amazon.com/bedrock/agentcore/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock AgentCore</a> Gateway, a fully managed service that revolutionizes how enterprises connect AI agents with tools and services. <a href=\"https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html\" rel=\"noopener noreferrer\" target=\"_blank\">AgentCore Gateway</a> serves as a centralized tool server, providing a unified interface where agents can discover, access, and invoke tools.</p> \n<p>Built with native support for the MCP, Gateway enables seamless agent-to-tool communication while abstracting away security, infrastructure, and protocol-level complexities. This service provides zero-code MCP tool creation from APIs and <a href=\"https://aws.amazon.com/lambda/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Lambda</a> functions, intelligent tool discovery, built-in inbound and outbound authorization, and serverless infrastructure for MCP servers. You can focus on building intelligent agent experiences rather than managing connectivity with tools and services. The following diagram illustrates the AgentCore Gateway workflow.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114365\" height=\"1446\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-19425-image-1.png\" width=\"2584\" /></p> \n<h3>Key capabilities of Amazon Bedrock AgentCore Gateway</h3> \n<p>The Amazon Bedrock AgentCore Gateway introduces a comprehensive set of capabilities designed to revolutionize tool integration for AI agents. At its core, Gateway offers powerful and secure API integration functionality that transforms existing REST APIs into MCP servers. This integration supports both OpenAPI specifications and Smithy models, so organizations can seamlessly convert their enterprise APIs into MCP-compatible tools. Beyond API integration, Gateway provides built-in support for Lambda functions so developers can connect their serverless computing resources as tools with defined schemas. Gateway provides the following key capabilities:</p> \n<ul> \n <li><strong>Security Guard</strong> – Manages OAuth authorization so only valid users and agents can access tools and resources. We will dive deeper into security in the following section.</li> \n <li><strong>Translation</strong> – Converts agent requests using protocols such as MCP into API requests and Lambda invocations, alleviating the need to manage protocol integration or version support.</li> \n <li><strong>Composition</strong> – Combines multiple APIs, functions, and tools into a single MCP endpoint for streamlined agent access.</li> \n <li><strong>Target extensibility</strong> – An AgentCore gateway is a central access point that serves as a unified interface for AI agents to discover and interact with tools. It handles authentication, request routing, and protocol translation between MCP and your APIs. Each gateway can manage multiple targets. A target represents a backend service or group of APIs that you want to expose as tools to AI agents. Targets can be AWS Lambda functions, OpenAPI specifications, or Smithy models. Each target can expose multiple tools, and Gateway automatically handles the conversion between MCP and the target’s built-in protocol. Gateway supports streamable http transport.</li> \n <li><strong>Infrastructure Manager</strong> – As a fully managed service, Gateway removes the burden of infrastructure management from organizations. It provides comprehensive infrastructure with built-in security features and robust observability capabilities. Teams no longer need to worry about hosting concerns, scaling issues, or maintaining the underlying infrastructure. The service automatically handles these aspects, providing reliable performance and seamless scaling as demand grows.</li> \n <li><strong>Semantic Tool Selection</strong> – Intelligent tool discovery represents another core capability of Gateway. As organizations scale to hundreds or thousands of tools, discovering the right tool becomes increasingly challenging for AI agents. Moreover, when agents are presented with too many tools simultaneously, they can experience something called “tool overload,” leading to hallucinations, incorrect tool selections, or inefficient execution paths that significantly impact performance. Gateway addresses these challenges by providing a special built-in tool named <code>'x_amz_bedrock_agentcore_search'</code> that can be accessed using the standard MCP tools and call operation.</li> \n</ul> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114366\" height=\"666\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-19425-image-2.jpeg\" width=\"1287\" /></p> \n<h2>Security and authentication</h2> \n<p>Gateway implements a sophisticated dual-sided security architecture that handles both inbound access to Gateway itself and outbound connections to target services.</p> \n<p>For inbound requests, Gateway follows the MCP <a href=\"https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization\" rel=\"noopener noreferrer\" target=\"_blank\">authorization specification</a>, using OAuth-based authorization to validate and authorize incoming tool calls. Gateway functions as an OAuth resource server. This means it can work with the OAuth Identity Provider your organization might use–whether that’s <a href=\"https://aws.amazon.com/cognito/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Cognito</a>, Okta, Auth0, or your own OAuth provider. When you create a gateway, you can specify multiple approved client IDs and audiences, giving you granular control over which applications and agents can access your tools. The Gateway validates incoming requests against your OAuth provider, supporting both authorization code flow (3LO) and client credentials flow (2LO, commonly used for service-to-service communication).</p> \n<p>The outbound security model is equally flexible but varies by target type:</p> \n<p>For AWS Lambda and Smithy model targets, AgentCore Gateway uses <a href=\"https://aws.amazon.com/iam/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Identity and Access Management</a> (IAM) based authorization. The gateway assumes an IAM role you configure, which can have precisely scoped permissions for each target service. This integrates smoothly with existing AWS security practices and IAM policies.</p> \n<p>For OpenAPI targets (REST APIs), Gateway supports two authentication methods:</p> \n<ol> \n <li><strong>API key</strong> – You can configure the key to be sent in either headers or query parameters with customizable parameter names</li> \n <li><strong>OAuth token for 2LO</strong> – For outbound OAuth authentication to target APIs, Gateway supports two-legged OAuth (2LO) client credentials grant type, enabling secure machine-to-machine communications without user interaction</li> \n</ol> \n<p>Credentials are securely managed through AgentCore Identity’s resource credentials provider. Each target is associated with exactly one authentication configuration, facilitating clear security boundaries and audit trails. AgentCore Identity handles the complex security machinery while presenting a clean, simple interface to developers. You configure security one time during setup, and Gateway handles the token validation, outbound token caching (through AgentCore Identity), and secure communication from there.</p> \n<h2>Get started with Amazon Bedrock AgentCore Gateway</h2> \n<p>You can create gateways and add targets through multiple interfaces:</p> \n<ul> \n <li><a href=\"https://aws.amazon.com/sdk-for-python/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS SDK for Python (Boto3)</a></li> \n <li><a href=\"https://aws.amazon.com/console/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Management Console</a></li> \n <li><a href=\"https://aws.amazon.com/cli/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Command Line Interface</a> (AWS CLI)</li> \n <li><a href=\"https://github.com/aws/bedrock-agentcore-starter-toolkit/tree/main/src/bedrock_agentcore_starter_toolkit/operations/gateway\" rel=\"noopener noreferrer\" target=\"_blank\">AgentCore starter toolkit for fast and straightforward setup</a></li> \n</ul> \n<p>The following practical examples and code snippets demonstrate the process of setting up and using Amazon Bedrock AgentCore Gateway.</p> \n<h3>Create a gateway</h3> \n<p>To create a gateway, use <a href=\"https://aws.amazon.com/cognito/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Cognito</a> for inbound auth using the AWS Boto3:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\">gateway_client = boto3.client('bedrock-agentcore-control')\nauth_config = {\n&nbsp;&nbsp; &nbsp;\"customJWTAuthorizer\": { \n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"allowedClients\": '&lt;cognito_client_id&gt;‘, # Client MUST match with the ClientId configured in Cognito.\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"discoveryUrl\": '&lt;cognito_oauth_discovery_url&gt;'\n&nbsp;&nbsp; &nbsp;}\n}\ncreate_response = gateway_client.create_gateway(name='DemoGateway',\n&nbsp;&nbsp; &nbsp;roleArn = '&lt;IAM Role&gt;' # The IAM Role must have permissions to create/list/get/delete Gateway \n&nbsp;&nbsp; &nbsp;protocolType='MCP',\n&nbsp;&nbsp; &nbsp;authorizerType='CUSTOM_JWT',\n&nbsp;&nbsp; &nbsp;authorizerConfiguration=auth_config, \n&nbsp;&nbsp; &nbsp;description='Demo AgentCore Gateway'\n)\n# Values with &lt; &gt; needs to be replaced with real values</code></pre> \n</div> \n<p>Here is the reference to <a href=\"https://docs.aws.amazon.com/bedrock-agentcore-control/latest/APIReference/Welcome.html\" rel=\"noopener noreferrer\" target=\"_blank\">control plane</a> and <a href=\"https://docs.aws.amazon.com/Bedrock-AgentCore/latest/APIReference/Welcome.html\" rel=\"noopener noreferrer\" target=\"_blank\">data plane</a> APIs for Amazon Bedrock AgentCore.</p> \n<h3>Create gateway targets</h3> \n<p>Create a target for an existing API using OpenAPI specification with API key as an outbound auth:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-css\"># Create outbound credentials provider in AgentCore Identity\nacps&nbsp;&nbsp;boto3client(service_name\"bedrock-agentcore-control\")\n\nresponseacpscreate_api_key_credential_provider(\nname\"APIKey\",\napiKey\"&lt;your secret API key\"\n)\n\ncredentialProviderARN&nbsp;&nbsp;response['credentialProviderArn']\n\n# Specify OpenAPI spec file via S3 or inline\nopenapi_s3_target_config = {\n&nbsp;&nbsp; &nbsp;\"mcp\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"openApiSchema\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"s3\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"uri\": openapi_s3_uri\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp;}\n}\n\n# API Key credentials provider configuration\napi_key_credential_config = [\n&nbsp;&nbsp; &nbsp;{\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"credentialProviderType\" : \"API_KEY\", \n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"credentialProvider\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"apiKeyCredentialProvider\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"credentialParameterName\": \"api_key\", # Replace this with the name of the api key name expected by the respective API provider. For passing token in the header, use \"Authorization\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"providerArn\": credentialProviderARN,\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"credentialLocation\":\"QUERY_PARAMETER\", # Location of api key. Possible values are \"HEADER\" and \"QUERY_PARAMETER\".\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#\"credentialPrefix\": \" \" # Prefix for the token. Valid values are \"Basic\". Applies only for tokens.\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp;}\n&nbsp;&nbsp;]\n\n# Add the OpenAPI target to the gateway\ntargetname='DemoOpenAPITarget'\nresponse = gateway_client.create_gateway_target(\n&nbsp;&nbsp; &nbsp;gatewayIdentifier=gatewayID,\n&nbsp;&nbsp; &nbsp;name=targetname,\n&nbsp;&nbsp; &nbsp;description='OpenAPI Target with S3Uri using SDK',\n&nbsp;&nbsp; &nbsp;targetConfiguration=openapi_s3_target_config,\n&nbsp;&nbsp; &nbsp;credentialProviderConfigurations=api_key_credential_config)</code></pre> \n</div> \n<p>Create a target for a Lambda function:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-javascript\"># Define the lambda target with tool schema. Replace the AWS Lambda function ARN below\nlambda_target_config = {\n&nbsp;&nbsp;\"mcp\": {\n&nbsp;&nbsp; &nbsp;\"lambda\": {\n&nbsp;&nbsp; &nbsp; &nbsp;\"lambdaArn\": \"&lt;Your AWS Lambda function ARN&gt;\",\n&nbsp;&nbsp; &nbsp; &nbsp;\"toolSchema\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"inlinePayload\": [\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"name\": \"get_order_tool\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"description\": \"tool to get the order\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"inputSchema\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"type\": \"object\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"properties\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"orderId\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"type\": \"string\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"required\": [\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"orderId\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]}}]}}}}\n\n# Create outbound auth config. For AWS Lambda function, its always IAM.\ncredential_config = [ \n&nbsp;&nbsp; &nbsp;{\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"credentialProviderType\" : \"GATEWAY_IAM_ROLE\"\n&nbsp;&nbsp; &nbsp;}\n]\n\n# Add AWS Lambda target to the gateway\ntargetname='LambdaUsingSDK'\nresponse = gateway_client.create_gateway_target(\n&nbsp;&nbsp; &nbsp;gatewayIdentifier=gatewayID,\n&nbsp;&nbsp; &nbsp;name=targetname,\n&nbsp;&nbsp; &nbsp;description='Lambda Target using SDK',\n&nbsp;&nbsp; &nbsp;targetConfiguration=lambda_target_config,\n&nbsp;&nbsp; &nbsp;credentialProviderConfigurations=credential_config)</code></pre> \n</div> \n<h3>Use Gateway with different agent frameworks</h3> \n<p>Use Gateway with <a href=\"https://strandsagents.com/latest/\" rel=\"noopener noreferrer\" target=\"_blank\">Strands Agents</a> integration:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">\nfrom strands import Agent\nimport logging\n\ndef create_streamable_http_transport():\n&nbsp;&nbsp; &nbsp;return streamablehttp_client(gatewayURL,headers={\"Authorization\": f\"Bearer {token}\"})\n\nclient = MCPClient(create_streamable_http_transport)\n\nwith client:\n&nbsp;&nbsp; &nbsp;# Call the listTools \n&nbsp;&nbsp; &nbsp;tools = client.list_tools_sync()\n&nbsp;&nbsp; &nbsp;# Create an Agent with the model and tools\n&nbsp;&nbsp; &nbsp;agent = Agent(model=yourmodel,tools=tools) ## you can replace with any model you like\n&nbsp; &nbsp;&nbsp;# Invoke the agent with the sample prompt. This will only invoke &nbsp;MCP listTools and retrieve the list of tools the LLM has access to. The below does not actually call any tool.\n&nbsp;&nbsp; &nbsp;agent(\"Hi , can you list all tools available to you\")\n&nbsp;&nbsp; &nbsp;# Invoke the agent with sample prompt, invoke the tool and display the response\n&nbsp;&nbsp; &nbsp;agent(\"Check the order status for order id 123 and show me the exact response from the tool\")</code></pre> \n</div> \n<p>Use Gateway with LangChain integration:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">from langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain.chat_models import init_chat_model\n\nclient = MultiServerMCPClient(\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"healthcare\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"url\": gateway_endpoint,\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"transport\": \"streamable_http\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"headers\":{\"Authorization\": f\"Bearer {jwt_token}\"}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp;)\n&nbsp;agent = create_react_agent(\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;LLM, \n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;tools, \n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;prompt=systemPrompt\n&nbsp;)</code></pre> \n</div> \n<h3>Implement semantic search</h3> \n<p>You can opt in to semantic search when creating a gateway. It automatically provisions a powerful built-in tool called <code>x_amz_bedrock_agentcore_search</code> that enables intelligent tool discovery through natural language queries. Use the output of the search tool in place of MCP’s list operation for scalable and performant tool discovery. The following diagram illustrates how you can use the MCP search tool.</p> \n<p><img alt=\"\" class=\"alignnone size-full wp-image-114367\" height=\"329\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/ML-19425-image-3.jpeg\" width=\"1287\" /></p> \n<p>To enable semantic search, use the following code:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-python\">&nbsp;# Enable semantic search of tools\n&nbsp;&nbsp; &nbsp;search_config = {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"mcp\": {\"searchType\": \"SEMANTIC\", \"supportedVersions\": [\"2025-03-26\"]}\n&nbsp;&nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp;# Create the gateway\n&nbsp;&nbsp; &nbsp;response = agentcore_client.create_gateway(\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name=gateway_name,\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;roleArn=gateway_role_arn,\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;authorizerType=\"CUSTOM_JWT\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;description=gateway_desc,\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;protocolType=\"MCP\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;authorizerConfiguration=auth_config,\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;protocolConfiguration=search_config,\n&nbsp;&nbsp; &nbsp;)\ndef tool_search(gateway_endpoint, jwt_token, query):\n&nbsp;&nbsp; &nbsp;toolParams = {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"name\": \"x_amz_bedrock_agentcore_search\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"arguments\": {\"query\": query},\n&nbsp;&nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp;toolResp = invoke_gateway_tool(\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;gateway_endpoint=gateway_endpoint, jwt_token=jwt_token, tool_params=toolParams\n&nbsp;&nbsp; &nbsp;)\n&nbsp;&nbsp; &nbsp;tools = toolResp[\"result\"][\"structuredContent\"][\"tools\"]\n&nbsp;&nbsp; &nbsp;return tools</code></pre> \n</div> \n<p>To find the entire code sample, visit the <a href=\"https://github.com/awslabs/amazon-bedrock-agentcore-samples/blob/main/01-tutorials/02-AgentCore-gateway/03-search-tools/01-gateway-search.ipynb\" rel=\"noopener noreferrer\" target=\"_blank\">Semantic search tutorial</a> in the <a href=\"https://github.com/awslabs/amazon-bedrock-agentcore-samples/\" rel=\"noopener noreferrer\" target=\"_blank\">amazon-bedrock-agentcore-samples</a> GitHub repository.</p> \n<h2>Assess Gateway performance using monitoring and observability</h2> \n<p>Amazon Bedrock AgentCore Gateway provides observability through integration with <a href=\"https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-advanced-observability-metrics.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon CloudWatch</a> and <a href=\"https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-cloudtrail.html\" rel=\"noopener noreferrer\" target=\"_blank\">AWS CloudTrail</a>, for detailed monitoring and troubleshooting of your tool integrations. The observability features include multiple dimensions of gateway operations through detailed metrics: <a href=\"https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-advanced-observability-metrics.html#gateway-metrics-usage\" rel=\"noopener noreferrer\" target=\"_blank\">usage metrics</a> (<code>TargetType</code>, <code>IngressAuthType</code>, <code>EgressAuthType</code>, <code>RequestsPerSession</code>), <a href=\"https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-advanced-observability-metrics.html#gateway-metrics-invocation\" rel=\"noopener noreferrer\" target=\"_blank\">invocation metrics</a> (<code>Invocations</code>, <code>ConcurrentExecutions</code>, <code>Sessions</code>), <a href=\"https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-advanced-observability-metrics.html#gateway-metrics-invocation\" rel=\"noopener noreferrer\" target=\"_blank\">performance metrics</a> (<code>Latency</code>, <code>Duration</code>, <code>TargetExecutionTime</code>), and error rates (<code>Throttles</code>, <code>SystemErrors</code>, <code>UserErrors</code>). The performance metrics can be analyzed using various statistical methods (Average, Minimum, Maximum, p50, p90, p99) and are tagged with relevant dimensions for granular analysis, including Operation, Resource, and Name . For operational logging, Gateway integrates with CloudTrail to capture both management and <a href=\"https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/understanding-gateway-cloudtrail-log-entries.html\" rel=\"noopener noreferrer\" target=\"_blank\">data events</a>, providing a complete audit trail of API interactions. The metrics are accessible through both the Amazon Bedrock AgentCore console and CloudWatch console, where you can create custom dashboards, set up automated <a href=\"https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway-advanced-observability-metrics.html#gateway-advanced-observability-alarms\" rel=\"noopener noreferrer\" target=\"_blank\">alerts</a>, and perform detailed performance analysis.</p> \n<h2>Best practices</h2> \n<p>Gateway offers an enhanced debugging option through the <code>exceptionLevel</code> property, which can be enabled during Gateway creation or updated as shown in the following code example:</p> \n<div class=\"hide-language\"> \n <pre><code class=\"lang-code\">create_response = gateway_client.create_gateway(name='DemoGateway',\n&nbsp;&nbsp; &nbsp;roleArn = '&lt;IAM Role&gt;' # The IAM Role must have permissions to create/list/get/delete Gateway \n&nbsp;&nbsp; &nbsp;protocolType='MCP',\n&nbsp;&nbsp; &nbsp;authorizerType='CUSTOM_JWT',\n&nbsp;&nbsp; &nbsp;authorizerConfiguration=auth_config, \n&nbsp;&nbsp; &nbsp;description='Demo AgentCore Gateway',\n    exceptionLevel=\"DEBUG\"   # Debug mode for granular error messages\n)</code></pre> \n</div> \n<p>When activated, this feature provides more granular error messages in the content text block (with <code>isError:true</code>) during Gateway testing, facilitating quicker troubleshooting and integration. When documenting and extracting Open APIs for Gateway, focus on clear, natural language descriptions that explain real-world use cases. Include detailed field descriptions, validation rules, and examples for complex data structures while maintaining consistent terminology throughout. For optimal tool discovery, incorporate relevant business domain keywords naturally in descriptions and provide context about when to use each API. Finally, test semantic search effectiveness so tools are discoverable through natural language queries. Regular reviews and updates are essential to maintain documentation quality as APIs evolve.When extracting APIs from larger specifications, identify the core functionality needed for agent tasks, maintain semantic relationships between components, and preserve security definitions. Follow a systematic extraction process: review the full specification, map agent use cases to specific endpoints, extract relevant paths and schemas while maintaining dependencies, and validate the extracted specification.The following are the best practices on grouping your APIs into a Gateway target:</p> \n<ul> \n <li>Start with the use case and group your MCP tools based on the agentic application’s business domain similar to domain-driven design principles applicable to the microservices paradigm.</li> \n <li>You can attach only one resource credentials provider for outbound authorization for the Gateway target. Group the tools based on the outbound authorizer.</li> \n <li>Group your APIs based on the type of the APIs, that is, OpenAPI, Smithy, or AWS Lambda, serving as a bridge to other enterprise APIs.</li> \n</ul> \n<p>When onboarding tools to Gateway, organizations should follow a structured process that includes security and vulnerability checks. Implement a review pipeline that scans API specifications for potential security risks, maintains proper authentication mechanisms, and validates data handling practices. For runtime tool discovery, use the semantic search capabilities in Gateway, but also consider design-time agent-tool mapping for critical workflows to provide predictable behavior.</p> \n<p>Enrich tool metadata with detailed descriptions, usage examples, and performance characteristics to improve discoverability and aid in appropriate tool selection by agents. To maintain consistency across your enterprise, integrate Gateway with a centralized tool registry that serves as a single source of truth. This can be achieved using open source solutions such as the <a href=\"https://github.com/modelcontextprotocol/registry/tree/main/tools/publisher\" rel=\"noopener noreferrer\" target=\"_blank\">MCP Registry Publisher Tool</a>, which publishes MCP server details to an MCP registry. Regularly synchronize Gateway’s tool inventory with this central registry for up-to-date and consistent tool availability across your AI landscape. These practices can help maintain a secure, well-organized, and efficiently discoverable tool solution within Gateway, facilitating seamless agent-tool interactions while can align with enterprise governance standards.</p> \n<h2>What customers are saying</h2> \n<p><a href=\"https://innovaccer.com/\" rel=\"noopener\" target=\"_blank\">Innovaccer</a>, a leading healthcare technology company, shares their experience:</p> \n<blockquote>\n <p><em>“AI has massive potential in healthcare, but getting the foundation right is key. That’s why we’re building HMCP (Healthcare Model Context Protocol) on Amazon Bedrock AgentCore Gateway, which has been a game-changer, automatically converting our existing APIs into MCP-compatible tools and scaling seamlessly as we grow. It gives us the secure, flexible base we need to make sure AI agents can safely and responsibly interact with healthcare data, tools, and workflows. With this partnership, we’re accelerating AI innovation with trust, compliance, and real-world impact at the core.” </em></p> \n <p>—Abhinav Shashank, CEO &amp; Co-founder, Innovaccer</p>\n</blockquote> \n<h2>Conclusion</h2> \n<p>Amazon Bedrock AgentCore Gateway represents a significant advancement in enterprise AI agent development. By providing a fully managed, secure, and scalable solution for tool integration, Gateway enables organizations to accelerate their AI initiatives while maintaining enterprise-grade security and governance. As part of the broader Amazon Bedrock AgentCore suite, Gateway works seamlessly with other capabilities including Runtime, Identity, Code Interpreter, Memory, Browser, and Observability to provide a comprehensive domain for building and scaling AI agent applications.</p> \n<p>For more detailed information and advanced configurations, refer to the <a href=\"https://github.com/awslabs/amazon-bedrock-agentcore-samples/tree/main/01-tutorials/02-AgentCore-gateway\" rel=\"noopener noreferrer\" target=\"_blank\">code samples on GitHub</a>, the <a href=\"https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/gateway.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock AgentCore Gateway Developer Guide</a> and <a href=\"https://aws.amazon.com/bedrock/agentcore/pricing/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon AgentCore Gateway pricing</a>.</p> \n<hr /> \n<h3>About the authors</h3> \n<p style=\"clear: both;\"><strong><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/dhawalkp.jpg\"><img alt=\"\" class=\"alignleft size-thumbnail wp-image-112777\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/07/31/dhawalkp-100x133.jpg\" width=\"100\" /></a>Dhawal Patel</strong> is a Principal Machine Learning Architect at Amazon Web Services (AWS). He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing and AI. He focuses on deep learning, including natural language processing (NLP) and computer vision domains. He helps customers achieve high-performance model inference on Amazon SageMaker.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"wp-image-114371 size-thumbnail alignleft\" height=\"133\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/14/sfmike-100x133.jpeg\" width=\"100\" /><strong>Mike Liu</strong> is a Principal Product Manager at Amazon, where he works at the intersection of agentic AI and foundational model development. He led the product roadmap for Amazon Bedrock Agents and is now helping customers achieve superior performance using model customization on Amazon Nova models. Prior to Amazon, he worked on AI/ML software in Google Cloud and ML accelerators at Intel.</p> \n<p style=\"clear: both;\"><img alt=\"\" class=\"alignleft wp-image-9807 size-full\" height=\"131\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/10/03/kartik-rustagi-100.jpg\" width=\"100\" /><strong>Kartik Rustagi</strong> works as a Software Development Manager in Amazon AI. He and his team focus on enhancing the conversation capability of chat bots powered by Amazon Lex. When not at work, he enjoys exploring the outdoors and savoring different cuisines.</p>"
        }
      ]
    },
    {
      "title": "Build a scalable containerized web application on AWS using the MERN stack with Amazon Q Developer – Part 1",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "Build a scalable containerized web application on AWS using the MERN stack with Amazon Q Developer – Part 1"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://aws.amazon.com/blogs/machine-learning/build-a-scalable-containerized-web-application-on-aws-using-the-mern-stack-with-amazon-q-developer-part-1/"
        }
      ],
      "link": "https://aws.amazon.com/blogs/machine-learning/build-a-scalable-containerized-web-application-on-aws-using-the-mern-stack-with-amazon-q-developer-part-1/",
      "authors": [
        {
          "name": "Bill Chan"
        }
      ],
      "author": "Bill Chan",
      "author_detail": {
        "name": "Bill Chan"
      },
      "published": "Fri, 15 Aug 2025 16:45:40 +0000",
      "published_parsed": [
        2025,
        8,
        15,
        16,
        45,
        40,
        4,
        227,
        0
      ],
      "tags": [
        {
          "term": "Amazon Cognito",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon DocumentDB",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Elastic Container Service",
          "scheme": null,
          "label": null
        },
        {
          "term": "Amazon Q Developer",
          "scheme": null,
          "label": null
        },
        {
          "term": "Technical How-to",
          "scheme": null,
          "label": null
        }
      ],
      "id": "5bcec91ee32cdcc8cb5bedc7e9de1687a85a73c1",
      "guidislink": false,
      "summary": "In a traditional SDLC, a lot of time is spent in the different phases researching approaches that can deliver on requirements: iterating over design changes, writing, testing and reviewing code, and configuring infrastructure. In this post, you learned about the experience and saw productivity gains you can realize by using Amazon Q Developer as a coding assistant to build a scalable MERN stack web application on AWS.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
        "value": "In a traditional SDLC, a lot of time is spent in the different phases researching approaches that can deliver on requirements: iterating over design changes, writing, testing and reviewing code, and configuring infrastructure. In this post, you learned about the experience and saw productivity gains you can realize by using Amazon Q Developer as a coding assistant to build a scalable MERN stack web application on AWS."
      },
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://aws.amazon.com/blogs/machine-learning/feed/",
          "value": "<p>The MERN (MongoDB, Express, React, Node.js) stack is a popular JavaScript web development framework. The combination of technologies is well-suited for building scalable, modern web applications, especially those requiring real-time updates and dynamic user interfaces. <a href=\"https://aws.amazon.com/q/developer\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Q Developer</a> is a generative AI-powered assistant that improves developer efficiency across the different phases of the software development lifecycle (SDLC). In this two-part blog series, I capture the experience and demonstrate the productivity gains you can achieve by using Amazon Q Developer as a coding assistant to build a scalable MERN stack web application on AWS. The solution forms a solid foundation for you to build a feature rich web application. In my case, using the process outlined in this blog, I extended the MERN stack web application to include real-time video conferencing (using <a href=\"https://aws.amazon.com/chime/chime-sdk/\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Chime SDK</a>) and an AI chatbot (invoking <a href=\"https://aws.amazon.com/bedrock\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Bedrock</a> foundation models).</p> \n<p>Typically, in the plan phase of the SDLC, time is spent researching approaches and identifying common solution patterns that can deliver on requirements. Using Amazon Q Developer, you can speed up this process by prompting for an approach to deploy a scalable MERN stack web application on AWS. Trained on over 17 years of AWS experience building in the cloud, Amazon Q Developer responses are based on AWS well-architected patterns and best practices. In the design phase, I use the responses from Amazon Q Developer to craft a detailed requirements prompt to generate the code for your MERN stack web application. Then in the build phase, I extend the code to implement a working solution, generate unit tests and conduct an automated code review.</p> \n<p>In part 2 of this blog series, I will use Amazon Q Developer to extend the base MERN stack web application to include a chat user interface (which invokes an agentic workflow based on the <a href=\"https://strandsagents.com/latest/\" rel=\"noopener noreferrer\" target=\"_blank\">Strands Agent SDK</a> and Amazon Bedrock), deploy the solution to AWS using infrastructure as code (IaC), troubleshoot issues and generate the documentation for our solution.</p> \n<h2>Walkthrough</h2> \n<h3>Prerequisites</h3> \n<p>To complete the walkthrough in this post, you must have the following:</p> \n<ul> \n <li>An <a href=\"https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?refid=em_127222&amp;p=free&amp;c=hp&amp;z=1\" rel=\"noopener noreferrer\" target=\"_blank\">AWS account</a> to deploy the solution components to AWS.</li> \n <li><a href=\"https://aws.amazon.com/cli/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Command Line Interface (AWS CLI)</a> installed and configured.</li> \n <li><a href=\"https://www.docker.com/products/docker-desktop/\" rel=\"noopener noreferrer\" target=\"_blank\">Docker Desktop</a> installed.</li> \n <li>Set up access to Amazon Q Developer by using one of the following two options: \n  <ul> \n   <li><a href=\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-free-tier.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Q Developer Free tier</a> – Provides access to explore capabilities before opting for a paid tier and requires an <a href=\"https://profile.aws.amazon.com/\" rel=\"noopener noreferrer\" target=\"_blank\">AWS Builder ID profile</a>.</li> \n   <li><a href=\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-pro-tier.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Q Developer Pro tier</a> – Paid subscription with access to additional features. Set up through <a href=\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-pro-tier-setting-up-access.html\" rel=\"noopener noreferrer\" target=\"_blank\">IAM Identity Center</a>.</li> \n  </ul> </li> \n <li>A supported integrated development environment (IDE) including Visual Studio Code and JetBrains IDEs. For more information, follow the instructions for <a href=\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-in-IDE-setup.html#setup-vscode\" rel=\"noopener noreferrer\" target=\"_blank\">installing the Amazon Q Developer extension or plugin</a> in your IDE.</li> \n</ul> \n<h3>Sign in to Amazon Q Developer (in your IDE)</h3> \n<p>After setting up Amazon Q Developer access tier and installing the Amazon Q extension for your IDE, you can sign in to Amazon Q Developer by using the IDE.</p> \n<ol> \n <li>The first sign-in flow shows the authentication process for the Free tier using an AWS Builder ID.</li> \n</ol> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-1-4.png\"><img alt=\"Diagram showing Amazon Q Developer sign-in process for Visual Studio Code using Free tier\" class=\"alignnone wp-image-113230 size-full\" height=\"590\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-1-4.png\" width=\"925\" /></a></p> \n<ol start=\"2\"> \n <li>The second sign-in flow shows the authentication process for the Pro tier using a sign-in URL to the AWS access portal (provided by your AWS administrator).</li> \n</ol> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-2-1.png\"><img alt=\"Diagram showing Amazon Q Developer sign-in process for Visual Studio Code using Pro tier\" class=\"alignnone wp-image-113229 size-full\" height=\"501\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-2-1.png\" width=\"1312\" /></a></p> \n<ol start=\"3\"> \n <li>After successful authentication, you’ll be presented with an initial chat window to start a conversation with Amazon Q Developer. In the chat input at the bottom, you have options to add additional context for Amazon Q Developer to provide responses such as using the active file or the entire workspace, defining rules for Amazon Q Developer to follow when it generates responses, toggling agentic coding on and off, and selecting your preferred foundation model (Claude Sonnet 4 in our case).</li> \n</ol> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-1-q-chat.png\"><img alt=\"Diagram showing Amazon Q Developer chat window\" class=\"alignnone wp-image-114108 size-large\" height=\"1024\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-1-q-chat-916x1024.png\" width=\"916\" /></a></p> \n<p>With Free Tier, you have access to limited agentic requests per month, access to the latest Claude models and use of Amazon Q Developer in the IDE or CLI. In this post, I use the Pro Tier, which in addition to Free Tier features, also provides increased limits of agentic requests and app transformation, Identity center support and IP indemnity.</p> \n<h3>Plan</h3> \n<p>In the planning phase, you can prompt for a solution approach to better understand the different components that will make up the MERN stack web application. You would toggle agentic coding off in this phase as you research and understand the best approach. Example planning phase prompt:</p> \n<p><code>“Provide a high-level summary of a solution approach to deploying a scalable MERN stack application on AWS.”</code></p> \n<p>The response from Amazon Q Developer (also shown in the following screenshot) breaks down the solution into the following components:</p> \n<ul> \n <li>Frontend React application</li> \n <li>Backend NodeJS and Express containerized app running on <a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon ECS Fargate</a></li> \n <li>Database using MongoDB or <a href=\"https://aws.amazon.com/documentdb\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon DocumentDB</a></li> \n <li>Core network infrastructure</li> \n <li>Security</li> \n <li>Monitoring and operations</li> \n <li>Continuous integration and delivery (CI/CD) pipeline</li> \n <li>Performance</li> \n</ul> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-4-3.png\"><img alt=\"Diagram showing the Amazon Q Developer response to the solution approach prompt\" class=\"alignnone wp-image-113227 size-full\" height=\"606\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-4-3.png\" width=\"1188\" /></a></p> \n<h3>Design &amp; Build</h3> \n<p>After reviewing the solution approach, you can create a more detailed prompt about the web application requirements, which will be used in the feature development capability of Amazon Q Developer to generate the solution components. Turn agentic coding on before submitting the prompt. Example design phase prompt:</p> \n<p><code>“Build a scalable containerized web application using the MERN stack on AWS, with login and sign-up pages integrated with Amazon Cognito, a landing page that retrieves a list of shops from DocumentDB. I don’t intend to use AWS Amplify. It needs to be a modular design with components that can scale independently, running as containers using ECS and Fargate, highly available across two Availability Zones. I need to build, test and run the MERN stack locally before pushing the solution to AWS.” </code></p> \n<p>As shown in the following screenshots, Amazon Q Developer will provide an architecture overview of the solution before going through the build process step by step. I will provide a select number of screenshots for illustration but note that the steps generated by Amazon Q Developer will vary for your solution prompt.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-2-solution.png\"><img alt=\"Diagram showing the Amazon Q Developer response to the solution approach prompt\" class=\"alignnone size-full wp-image-114107\" height=\"1376\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-2-solution.png\" width=\"1688\" /></a></p> \n<p>For each file that it creates or updates, Amazon Q Developer gives you the option to review the difference and undo the changes. This is an important step to understand whether the generated code meets your requirements. For example, the snippet below shows an update the Navbar component.</p> \n<p><img alt=\"Diagram showing the update to the Navbar component.\" class=\"alignnone wp-image-114082 size-full\" height=\"272\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-2-solution-diff.png\" width=\"1684\" /></p> \n<p>When viewing the diff, you can see that Amazon Q Developer has added a new button class to fix a display issue.</p> \n<p><img alt=\"Diagram showing the diff of Amazon Q Developer adding a new button class to fix a display issue\" class=\"alignnone wp-image-114083 size-full\" height=\"994\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-2-solution-diff-1.png\" width=\"1676\" /></p> \n<p>Amazon Q Developer can also execute shell commands. In this case, create the backend and frontend directory. You have the option to ‘Reject’ or ‘Run’ the command.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-3-prompt-command.png\"><img alt=\"Diagram showing the option to ‘Reject’ or ‘Run’ the shell command to create the backend and frontend directories\" class=\"alignnone wp-image-114106 size-large\" height=\"215\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-3-prompt-command-1024x215.png\" width=\"1024\" /></a></p> \n<p>Here’s a snippet of Amazon Q Developer creating the authentication service, data model and Dockerfile for the solution.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-4-middleware.png\"><img alt=\"Diagram showing Amazon Q Developer creating the authentication service, data model and Dockerfile for the solution.\" class=\"alignnone wp-image-114105 size-full\" height=\"918\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-4-middleware.png\" width=\"1688\" /></a></p> \n<p>Another snippet of Amazon Q Developer creating the React frontend.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-5-frontend.png\"><img alt=\"Diagram showing Amazon Q Developer creating the React frontend.\" class=\"alignnone wp-image-114104 size-full\" height=\"888\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-5-frontend.png\" width=\"1682\" /></a></p> \n<p>A snippet of Amazon Q Developer creating the AWS infrastructure components.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-6-infra.png\"><img alt=\"Diagram showing Amazon Q Developer creating the AWS infrastructure components\" class=\"alignnone size-full wp-image-114103\" height=\"1068\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-6-infra.png\" width=\"1686\" /></a></p> \n<p>Amazon Q Developer then prompts to execute the deployment.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-7-deploy.png\"><img alt=\"Diagram showing the option to ‘Reject’ or ‘Run’ the shell command to make the deploy script executable.\" class=\"alignnone size-large wp-image-114102\" height=\"201\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-7-deploy-1024x201.png\" width=\"1024\" /></a></p> \n<p>But I noticed that it hasn’t followed my initial prompt to “build, test and run the MERN stack locally before pushing the solution to AWS”, so I provide the following prompt:</p> \n<p><code>“In my initial prompt, I asked to build, test and run the MERN stack locally before pushing the solution to AWS.</code></p> \n<p>Amazon Q Developer acknowledges my observation and makes the necessary changes for local deployment.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-8-revised.png\"><img alt=\"Diagram showing Amazon Q Developer creating files to run locally.\" class=\"alignnone size-large wp-image-114101\" height=\"679\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-8-revised-1024x679.png\" width=\"1024\" /></a></p> \n<p>Next, Amazon Q Developer will build, test and run the MERN stack locally as shown below.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-9-local-deploy.png\"><img alt=\"Diagram showing Amazon Q Developer setting up and running the application locally.\" class=\"alignnone size-large wp-image-114100\" height=\"637\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-9-local-deploy-1024x637.png\" width=\"1024\" /></a></p> \n<p>When reviewing the .env file changes, I noticed that the Amazon Cognito properties are not properly set, so provide the following prompt:</p> \n<p><code>“When reviewing your .env file changes, I noticed that setting to COGNITO_USER_POOL_ID and COGNITO_CLIENT_ID to local-development is incorrect, as I should be connecting to Amazon Cognito in AWS. And this hasn't been created yet. Additionally, the local deployment has been configured to connect to the local MongoDB container instead of DocumentDB.”</code></p> \n<p>Amazon Q Developer again acknowledges my observation and attempts to fix the issues. These two issues highlight that to effectively use Amazon Q Developer, it’s important to review and challenge the responses provided.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-10-local-cognito.png\"><img alt=\"Diagram showing Amazon Q Developer fixing Cognito and database issues.\" class=\"alignnone size-large wp-image-114099\" height=\"474\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-10-local-cognito-1024x474.png\" width=\"1024\" /></a></p> \n<p>After fixing the issues, Amazon Q Developer updates the README.md to reflect the updated approach and asks if I want to do a quick deployment with mocked authentication or an actual deployment with Amazon Cognito resources.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-11-deploy-options.png\"><img alt=\"Diagram showing Amazon Q Developer summarizing fixes made and options for next step.\" class=\"alignnone size-large wp-image-114098\" height=\"834\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-11-deploy-options-1024x834.png\" width=\"1024\" /></a></p> \n<p>I choose option B, with real Amazon Cognito resources, so Amazon Q Developer deploys the resources as shown below.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-12-cognito-deploy-blurred.png\"><img alt=\"Diagram showing Amazon Q Developer executing the setup-aws-cognito.sh script\" class=\"alignnone size-large wp-image-114128\" height=\"709\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-12-cognito-deploy-blurred-1024x709.png\" width=\"1024\" /></a></p> \n<p>Amazon Q Developer now checks that the frontend, backend and MongoDB containers are running.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-13-docker-status.png\"><img alt=\"Diagram showing the output of the container status\" class=\"alignnone size-large wp-image-114096\" height=\"578\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-13-docker-status-1024x578.png\" width=\"1024\" /></a></p> \n<p>Amazon Q Developer also tests that the application is running by executing curl commands to the application endpoints.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-14-curl.png\"><img alt=\"Diagram showing Amazon Q Developer testing application endpoints.\" class=\"alignnone size-large wp-image-114095\" height=\"689\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-14-curl-1024x689.png\" width=\"1024\" /></a></p> \n<p>After successfully running the commands, Amazon Q Developer provides a summary of the results, with details on how to access and test the application.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-15-summary-blurred.png\"><img alt=\"Diagram showing application testing results.\" class=\"alignnone size-large wp-image-114127\" height=\"764\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-15-summary-blurred-1024x764.png\" width=\"1024\" /></a></p> \n<p>Here’s a diagram showing the locally deployed solution.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-10.jpeg\"><img alt=\"Diagram showing the MERN stack solution components deployed locally.\" class=\"alignnone size-full wp-image-113221\" height=\"311\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/05/image-10.jpeg\" width=\"370\" /></a></p> \n<p>Now that the frontend, backend, and MongoDB containers are running, you can access the frontend application <strong>Sign In</strong> page on http://localhost:3000.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-16-login.png\"><img alt=\"Diagram showing the Sign In page for the application.\" class=\"alignnone wp-image-114093 size-large\" height=\"770\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-16-login-1024x770.png\" width=\"1024\" /></a></p> \n<p>Before logging in, you need to create a user. Choose the <strong>Sign Up</strong> link to enter an email and password.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-17-signup.png\"><img alt=\"Diagram showing the Sign Up page for the application.\" class=\"alignnone size-large wp-image-114092\" height=\"752\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-17-signup-1024x752.png\" width=\"1024\" /></a></p> \n<p>After attempting to sign up, I noticed that Amazon Q Developer hasn’t generated the corresponding frontend screen to enter the confirmation code, so I prompt it to fix the issue. Again, the generated code isn’t always perfect, but it’s a good starting point.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-18-confirm.png\"><img alt=\"Diagram showing the Sign Up verification code page\" class=\"alignnone size-large wp-image-114091\" height=\"733\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-18-confirm-1024x733.png\" width=\"1024\" /></a></p> \n<p>After authentication, you’ll be routed to the shops page as shown.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-19-shops.png\"><img alt=\"Diagram showing the authenticated landing page with available shops.\" class=\"alignnone size-large wp-image-114090\" height=\"409\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-19-shops-1024x409.png\" width=\"1024\" /></a></p> \n<h3>Test</h3> \n<p>Now that you’ve built and can run the MERN stack web application locally, you can use Amazon Q Developer to generate unit tests to find defects and improve code quality. I provide the following prompt:</p> \n<p><code>“Can you generate unit tests for the project?”</code></p> \n<p>Amazon Q Developer will then create comprehensive unit tests for the application.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-20-unit-test.png\"><img alt=\"Diagram showing Amazon Q Developer creating unit tests for the application\" class=\"alignnone size-large wp-image-114089\" height=\"894\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-20-unit-test-1024x894.png\" width=\"1024\" /></a></p> \n<p>At completion, Amazon Q Developer will provide a summary of the unit tests generated:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-21-unit-test-summary.png\"><img alt=\"Diagram showing a summary of the unit tests generated.\" class=\"alignnone size-large wp-image-114088\" height=\"684\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-21-unit-test-summary-1024x684.png\" width=\"1024\" /></a></p> \n<p>Amazon Q Developer also provides instructions for executing the tests:</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-23-unit-test-run.png\"><img alt=\"Diagram showing Amazon Q Developer instructions to run the tests.\" class=\"alignnone size-large wp-image-114087\" height=\"254\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-23-unit-test-run-1024x254.png\" width=\"1024\" /></a></p> \n<p>After executing the unit tests, Amazon Q Developer provides a summary of the results.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-22-unit-test-results.png\"><img alt=\"Diagram showing the Amazon Q Developer unit tests summary.\" class=\"alignnone size-large wp-image-114086\" height=\"720\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-22-unit-test-results-1024x720.png\" width=\"1024\" /></a></p> \n<h3>Review</h3> \n<p>We can now conduct a code review of the MERN stack application by prompting the following:</p> \n<p><code>“Can you do a code review of my project to identify and fix any code issues?”</code></p> \n<p>Amazon Q Developer will perform a code review and identify issues that require attention.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-24-code-review.png\"><img alt=\"Diagram showing Amazon Q Developer performing a comprehensive code review.\" class=\"alignnone size-large wp-image-114085\" height=\"921\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-24-code-review-1024x921.png\" width=\"1024\" /></a></p> \n<p>After completing the review, Amazon Q Developer will provide a summary of the critical issues fixed, along with next steps.</p> \n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-25-code-review-fixed.png\"><img alt=\"Diagram showing a summary of the critical issues fixed as part of the code review by Amazon Q Developer.\" class=\"alignnone size-large wp-image-114084\" height=\"802\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/12/ml-18115-25-code-review-fixed-1024x802.png\" width=\"1024\" /></a></p> \n<h2>Clean up</h2> \n<p>To avoid incurring future charges, remove the Amazon Cognito resources that you created.</p> \n<h2>Conclusion</h2> \n<p>In a traditional SDLC, a lot of time is spent in the different phases researching approaches that can deliver on requirements: iterating over design changes, writing, testing and reviewing code, and configuring infrastructure. <a href=\"https://aws.amazon.com/q/developer\" rel=\"noopener noreferrer\" target=\"_blank\">Amazon Q Developer</a> is a generative AI-powered assistant that improves developer efficiency across the phases of the SDLC. In this post, you learned about the experience and saw productivity gains you can realize by using Amazon Q Developer as a coding assistant to build a scalable MERN stack web application on AWS.</p> \n<p>In the plan phase, you used Amazon Q Developer to prompt for a solution approach to deploy a scalable MERN stack web application on AWS. Then in the design phase, you used the initial responses from Amazon Q Developer to craft a detailed requirements prompt and generated the code for your MERN stack web application. In the build phase, you customized the code and deployed a working solution locally. In the test phase, Amazon Q Developer generated the unit tests for you to identify bugs early to improve code quality. Finally, in the review phase, you conducted a code review and remediated issues identified.</p> \n<p>In part 2 of this blog series, you will use Amazon Q Developer to extend the base MERN stack web application to include a chat user interface (which invokes an agentic workflow based on the <a href=\"https://strandsagents.com/latest/\" rel=\"noopener noreferrer\" target=\"_blank\">Strands Agent SDK</a> and Amazon Bedrock), deploy the solution to AWS using infrastructure as code (IaC), troubleshoot issues and generate the documentation for our solution.</p> \n<hr /> \n<h3>About the Author</h3> \n<p><img alt=\"\" class=\"wp-image-113967 size-full alignleft\" height=\"102\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/08/10/bill-chan.png\" width=\"100\" /><strong>Bill Chan</strong> is an Enterprise Solutions Architect working with large enterprises to craft highly scalable, flexible, and resilient cloud architectures. He helps organizations understand best practices around advanced cloud-based solutions, and how to migrate existing workloads to the cloud. He enjoys relaxing with family and shooting hoops.</p>"
        }
      ]
    }
  ]
}