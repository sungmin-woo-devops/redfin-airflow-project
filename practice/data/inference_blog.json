{
  "feed": {
    "title": "inFERENCe",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://www.inference.vc/rss/",
      "value": "inFERENCe"
    },
    "subtitle": "posts on machine learning, statistics, opinions on things I'm reading in the space",
    "subtitle_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://www.inference.vc/rss/",
      "value": "posts on machine learning, statistics, opinions on things I'm reading in the space"
    },
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://www.inference.vc/"
      },
      {
        "href": "https://www.inference.vc/rss/",
        "rel": "self",
        "type": "application/rss+xml"
      }
    ],
    "link": "https://www.inference.vc/",
    "image": {
      "href": "https://www.inference.vc/favicon.png",
      "title": "inFERENCe",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "inFERENCe"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/"
        }
      ],
      "link": "https://www.inference.vc/"
    },
    "generator_detail": {
      "name": "Ghost 5.130"
    },
    "generator": "Ghost 5.130",
    "updated": "Tue, 26 Aug 2025 08:45:22 GMT",
    "updated_parsed": [
      2025,
      8,
      26,
      8,
      45,
      22,
      1,
      238,
      0
    ],
    "ttl": "60"
  },
  "entries": [
    {
      "title": "Discrete Diffusion: Continuous-Time Markov Chains",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Discrete Diffusion: Continuous-Time Markov Chains"
      },
      "summary": "A tutorial explaining some key intuitions behind continuous time Markov chains for machine learners interested in discrete diffusion models: alternative representations, connections to point processes, and the memoryless property.",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "A tutorial explaining some key intuitions behind continuous time Markov chains for machine learners interested in discrete diffusion models: alternative representations, connections to point processes, and the memoryless property."
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/discrete-diffusion-continuous-time-markov-chains/"
        }
      ],
      "link": "https://www.inference.vc/discrete-diffusion-continuous-time-markov-chains/",
      "id": "67d3f11fe23785000125c3db",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Thu, 22 May 2025 09:12:09 GMT",
      "published_parsed": [
        2025,
        5,
        22,
        9,
        12,
        9,
        3,
        142,
        0
      ],
      "media_content": [
        {
          "url": "https://www.inference.vc/content/images/2025/05/markov_chain_point_process-1-1.png",
          "medium": "image"
        }
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<img alt=\"Discrete Diffusion: Continuous-Time Markov Chains\" src=\"https://www.inference.vc/content/images/2025/05/markov_chain_point_process-1-1.png\" /><p><em>This post, intended to be the first in a series related to discrete diffusion models, has been sitting in my drafts for months. I thought that Google&apos;s release of Gemini Diffusion might be a good occasion to finally publish it.</em></p><p>While discrete time Markov chains - sequences of random variables in which the past and future are independent given the present - are rather well known in machine learning, fewer people ever come across their continuous cousins. Given that these models feature in work on discrete diffusion models (see e.g. <a href=\"https://arxiv.org/abs/2310.16834?ref=inference.vc\">Lou et al, 2023</a>, <a href=\"https://arxiv.org/pdf/2406.07524?ref=inference.vc\">Sahoo et al, 2024</a>, <a href=\"https://arxiv.org/abs/2406.04329?ref=inference.vc\">Shi et al, 2024</a>, obviously not a complete list), I thought it would be a good way to get back to blogging by writing about these models - and who knows, maybe carry on writing a series about discrete diffusion. For now, the goal of this post is to help you build some key intuition about how continuous-time MCs work.</p><p>A Markov chain is a stochastic process (infinite collection of random variables indexed by time $t$) defined by two properties:</p><ul><li>the random variables take discrete values (we call them states), and</li><li>the process is memory-less: What happens to the process in the future only depends on the state it is in at the moment. Mathematically, $X_{u} \\perp X_s \\vert X_t$ for all $s &gt; t &gt; u$, where $\\perp$ denotes conditional independence.</li></ul><p>We differentiate Markov chains on values the index $t$ can take: if $t$ is an integer, we call it a discrete-time Markov chain, and when $t$ is real, we call the process a continuous-time Markov chain.</p><p>Discrete Markov chains are fully described by a collection of state transition matrices $P_t = [P(X_{t+1} = i\\vert X_{t}=j)]{i,j}$. Note that this matrix $P_t$ is indexed by the time $t$, as it can, in general, change over time. If $P_t = P$ is constant, we call the Markov chain homogeneous.</p><h2 id=\"continuous-time-homogeneous-chains\">Continuous time homogeneous chains</h2><p>To extend the notion of MC to continuous time, we are first going to develop an alternative model of a discrete chains, by considering waiting times in a homogeneous discrete-time Markov chain. Waiting times are the time the chain spends in the same state before transitioning to another state. If the MC is homogeneous, then in every timestep it has a fixed probability $p_{i,i}$ of staying there. The waiting time therefore follows a <strong>geometric distribution</strong> with parameter $p_{i,i}$. </p><p>The geometric distribution is the only discrete distribution with the memory-less property, stated as $\\mathbb{P}[T=s+t\\vert T&gt;s] = \\mathbb{P}[T = t]$. What this means is: if you are at time s, and you know the event hasn&apos;t happened yet, the distribution of the remaining waiting time is the same, irrespective of how long you&apos;ve been waiting. It turns out, the geometric distribution is the only discrete distribution with this property. </p><p>With this observation, we can alternatively describe a homogeneous discrete-time Markov chain in terms of waiting times, and jump probabilities as follows. Starting from state $i$ the Markov chain:</p><ul><li>stays in the same state for a time drawn from a Geometric distribution with parameter $p_{i,i}$</li><li>when the waiting time expires, we sample a new state $i \\neq j$ with probability $\\frac{p_{i,j}}{\\sum_{k\\neq i} p_{i,k}}$</li></ul><p>In other words, we have decomposed the description of the Markov chain in terms of when the next jump happens, and how the state changes when the jump happens. In this representation, if we want something like a Markov chain in continuous time, we can do that by allowing the waiting time to take real, not just integer, values. We can do this by replacing the geometric distribution by a continuous probability distribution. To preserve the Markov property, however, it is important that we preserve the memoryless property $\\mathbb{P}[T=s+t\\vert T&gt;s] = \\mathbb{P}[T = t]$. There is only one such continuous distribution: <strong>the exponential distribution</strong>. A homogeneneous continuous-time Markov chain is thus described as follows. Starting from state $i$ at time $t$:</p><ul><li>stay in the same state for a time drawn from an exponential distribution with some parameter $\\lambda_{i,i}$</li><li>when the waiting time expires, sample a new state $i \\neq j$ with probability $\\frac{\\lambda_{i,j}}{\\sum_{k\\neq i} \\lambda_{i,k}}$</li></ul><p>Notice that I introduced a new set of parameters $\\lambda_{i,j}$ which now replaced the transition probabilities $p_{i,j}$. These no longer have to be probability distributions, they just have to be all positive real values. The matrix containing these parameters will be called the rate matrix, which I will denote by $\\Lambda$, but I&apos;ll note that in recent machine learning papers, they often use the notation $Q$ for the rate matrix.</p><h2 id=\"non-homogeneous-markov-chains-and-point-processes\">Non-homogeneous Markov chains and point processes</h2><p>The above description only really works for homogeneous Markov chains, in which the waiting time distribution does not change over time. When the transition probabilities can change over time, the wait time can no longer be described as exponential, and it&apos;s actually not so trivial to generalise to continuous time in this way. To do that, we need yet another alternative view on how Markov chains work: we instead consider their relationship to point processes.</p><h2 id=\"point-processes\">Point processes</h2><p>Consider the following scenario which will illustrate the relationship between the discrete uniform, the Bernoulli, the binomial and the geometric distributions. </p><p>The Easter Bunny is hiding eggs in 50 meter long (one dimensional) garden. In every 1 meter segment he hides at most one egg. He wants to make sure that on average, there is a 10% chance of an egg in each 1 meter segment, and he also wants the eggs to appear random.</p><p>Bunny is considering a few different ways of achieving this:</p><p><strong>Process 1:</strong> Bunny steps from square to square. At every square, he hides an egg with probability 0.1, and passes to the next square.</p><figure class=\"kg-card kg-code-card\"><pre><code>bernoulli = partial(numpy.random.binomial, n=1)\ngarden = [&apos;_&apos;]*50\n\nfor i in range(50):\n  if bernoulli(0.1)\n    garden[i] = &apos;&#x1f95a;&apos;\n\nprint(&apos;&apos;.join(garden))\n\n&gt;&gt;&gt; __________&#x1f95a;__&#x1f95a;______&#x1f95a;_&#x1f95a;__________________&#x1f95a;________</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Simulating the egg-hiding process using Bernoulli distributions</span></p></figcaption></figure><p><strong>Process 2: </strong>At each step Bunny draws a random non-negative integer $T$ from a Geometric distribution with parameter 0.1. He then moves T cells to the right. If he is still within bounds of the garden, he hides an egg where he is, and repeats the process until he is out of bounds.</p><figure class=\"kg-card kg-code-card\"><pre><code>garden = [&apos;_&apos;]*50\nbunny_pos = -1\n\nwhile bunny_pos&lt;50: \n  bunny_pos += geometric(0.1)\n  if bunny_pos&lt;50:\n    garden[bunny_pos] = &apos;&#x1f95a;&apos;\n\nprint(&apos;&apos;.join(garden))\n\n&gt;&gt;&gt; &#x1f95a;______&#x1f95a;_________________&#x1f95a;______________&#x1f95a;&#x1f95a;&#x1f95a;___&#x1f95a;_&#x1f95a;&#x1f95a;</code></pre><figcaption><p><span style=\"white-space: pre-wrap;\">Simulating the egg-hiding process using Geometric distributions</span></p></figcaption></figure><p><strong>Process 3: </strong>Bunny first decides how many eggs he&apos;s going to hide in total in the garden. Then he samples random locations (without replacement) from within the garden, and hides an egg at those locations:</p><pre><code>garden = [&apos;_&apos;]*50\nnumber_of_eggs = binomial(p=0.1, n=50)\nrandom_locations = permutation(np.arange(50))[:number_of_eggs]\n\nfor location in random_locations:\n  garden[location] = &apos;&#x1f95a;&apos;\n\nprint(&apos;&apos;.join(garden))\n\n&gt;&gt;&gt; ________&#x1f95a;_____&#x1f95a;_______________&#x1f95a;_&#x1f95a;______&#x1f95a;________&#x1f95a;&#x1f95a;</code></pre><p>It turns out it does not matter which of these processes Bunny follows, at the end of the day, the outcome (binary string representing presence or absence of eggs) follows the same distribution.</p><p>What Bunny does in each of these processes, is he simulated <strong>a discrete-time point process </strong>with parameter $p=0.1$. All these various processes are equivalent representations of the point process:</p><ul><li>a binary sequence where each digit is sampled from independent Bernoulli</li><li>a binary sequence where the gap between 1s follows a geometric distribution</li><li>a binary sequence where the number of 1s follows a binomial distribution, and the locations of 1s follow a uniform distribution (with the constraint that they are not equal).</li></ul><h2 id=\"continuous-point-processes\">Continuous point processes </h2><p>And this egg-hiding process does have a continuous limit, one where the garden is not subdivided into 50 segments, but is instead treated as a continuous segment along which eggs may appear anywhere. Like good physicists - and terrible parents - do, we will consider point-like Easter eggs that have size $0$ and can appear arbitrarily close to each other.</p><p>While process 1 seems inherently discrete (it loops over enumerable locations) both Process 2 and Process 3 can be made continuous, by replacing the discrete probability distributions with continuous cousins.</p><ul><li>In Process 2 we replace the geometric with the, also memoryless, exponential.</li><li>In Process 3 we replace the binomial with its limiting Poisson distribution, and the uniform sampling without replacement by a continuous uniform distribution over the space. We can drop the constraint that the locations have to be different, since this holds with probabilty one for continuous distributions.</li></ul><p>In both cases, the collection of random locations we end up with is what is known as a <strong>homogeneous Poisson point process</strong>. Instead of a probability $p$, this process will now have a positive parameter $\\lambda$ that controls the expected number of points the process is going to place in the interval.</p><h2 id=\"point-processes-and-markov-chains\">Point processes and Markov Chains</h2><p>Back to Markov chains. How are point processes and Markov chains related? The appearance of exponential and geometric distributions in Process 2 of generating point processes might be a giveaway, the waiting times in the Markov chains will be related to the waiting times between subsequent points in a random point process. Here is how we can use point processes to simulate a Markov chain. Starting from the discrete case let&apos;s denote by $p_{i,j}$ the transition probability from state $i$ to state $j$</p><p><strong>Step 1: </strong>For each of the $N(N-1)$ possible transitions $i \\rightarrow j$ between $N$ states, draw an independent point processes with parameter $p_{i,j}$. We now have a list of of event times for each transition. These can be visualised for a three-state Markov Chain as follows:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"Discrete Diffusion: Continuous-Time Markov Chains\" class=\"kg-image\" height=\"455\" src=\"https://www.inference.vc/content/images/2025/03/download-67.png\" width=\"589\" /><figcaption><span style=\"white-space: pre-wrap;\">For each transition i-&gt;j, we draw transition events from a point process with parameter corresponding to the transition probability</span></figcaption></figure><p>This plot has one line for each transition from a state $i$ to another state $j$. For each of these we drew transition points from a point process. Notice how some of the lines are denser - this is because the corresponding transition probabilities are higher. This is the transition matrix I used:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Discrete Diffusion: Continuous-Time Markov Chains\" class=\"kg-image\" height=\"522\" src=\"https://www.inference.vc/content/images/2025/03/download-68-1.png\" width=\"660\" /></figure><p>To turn this set of point processes into a Markov chain, we have to repeat the following steps starting from time 0.</p><p>Assume at time $t$ the Markov chain is in state $i$. We consider the point processes for all transitions out of state $i$, and look at the earliest event in any of these sequences that happen after time $t$. Suppose this earlier event is at time $s+t$ and happens in the point process we drew for transition $i \\rightarrow j$. Our process will then stay in state $i$ until time $s+t$, then we jump to to state $j$ and repeat the process.</p><p>This is illustrated in the following figure:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Discrete Diffusion: Continuous-Time Markov Chains\" class=\"kg-image\" height=\"455\" src=\"https://www.inference.vc/content/images/2025/03/download-69.png\" width=\"589\" /></figure><p>We start the Markov chain in state $1$. We therefore focus on the transitions $1\\rightarrow 0$ and $1 \\rightarrow 2$, as highlighted by the green area on the left side of this figure. We stay in state $1$ until we encounter a transition event (black dot) for either of these two transitions. This happens after about 5 timesteps, as illustrated by the red cross. At this point, we transition to state $2$, since the first transition event was observed in the $1 \\rightarrow 2$ row. Now we focus on transition events for lines $2\\rightarrow 1$ and $2\\rightarrow 0$, i.e. the top two rows. We don&apos;t have to wait too long until a transition event is observed, triggering a state update to state $0$, and so on. You can read off the Markov chain states from where the green focus area lies.</p><p>This reparametrisation of a CTMC in terms of underlying Poisson point processes now opens the door for us to simulate non-homogeneous CTMCs as well. But, as this post is already pretty long, I&apos;m not going to tell you here how to do it. Let that be your homework to think about: Go back to process 2 and process 3 of simulating the homogeneous Poisson process. Think about what makes them homogeneous (i.e. the same over time)? Which components would change if the rate matrix wasn&apos;t constant over time? Which representation can handle non-constant rates more easily?</p><h3 id=\"summary\">Summary</h3><p>In this post I attempted to convey some crucial, and I think cool, intuition about continuous time Markov chains. I focussed on exploring different representations of a Markov chain, each of which suggest a different way of simulating or sampling from the same process. Here is a recap of some key ideas discussed:</p><ul><li>You can reparametrise a homogeneous discrete Markov chain in terms of waiting times sampled from a geometric distribution, and transition probabilities.</li><li>The geometric distribution has a memory-less property which has a deep connection with the Markov property of Markov chains.</li><li>One way to go continuous is to consider a continuous waiting time distribution which preserves the memorylessness: the exponential distribution. This gave us our first representation of a continuous-time chain, but it&apos;s difficult to extend this to non-homogeneous case where the rate matrix may change over time.</li><li>We then discussed different representations of discrete point processes, and noted (although did not prove) their equivalence.</li><li>We could consider continuous versions of these point processes, by replacing discrete distributions by continuous ones, again preserving crucial features such as memorylessness.</li><li>We also discussed how a Markov chain can be described in terms of underlying point processes: each state pair has an associated point process, and transitions happen when a point process associated with the current state &quot;fires&quot;.</li><li>This was a redundant representation of the Markov chain, inasmuch as most points sampled from the underlying point processes did not contribute to the state evolution at all (in the last figures, most black dots lie outside the green area, so you can move their locations without effecting the states of the chain).</li></ul>"
        }
      ]
    },
    {
      "title": "We may finally crack Maths. But should we?",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "We may finally crack Maths. But should we?"
      },
      "summary": "<p>Automating mathematical theorem proving has been a long standing goal of artificial intelligence and indeed computer science. It&apos;s one of the areas I became very interested in recently. This is because I feel we may have the ingredients needed to make very, very significant progress:</p><ol><li>a structured search</li></ol>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>Automating mathematical theorem proving has been a long standing goal of artificial intelligence and indeed computer science. It&apos;s one of the areas I became very interested in recently. This is because I feel we may have the ingredients needed to make very, very significant progress:</p><ol><li>a structured search</li></ol>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/we-may-finally-crack-maths-but-should-we/"
        }
      ],
      "link": "https://www.inference.vc/we-may-finally-crack-maths-but-should-we/",
      "id": "6481b5876965110001df8c10",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Thu, 08 Jun 2023 15:58:13 GMT",
      "published_parsed": [
        2023,
        6,
        8,
        15,
        58,
        13,
        3,
        159,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<p>Automating mathematical theorem proving has been a long standing goal of artificial intelligence and indeed computer science. It&apos;s one of the areas I became very interested in recently. This is because I feel we may have the ingredients needed to make very, very significant progress:</p><ol><li>a structured search space with clear-cut success criterion that can be algorithmically generated: the language of formal mathematics</li><li>a path to obtaining very good heuristics to guide search in the space - LLMs trained on a mixture of code, formal and informal mathematics.</li><li>learning algorithms that can exploit the above, like AlphaZero and MuZero, with demonstrated ability of tackling some tricky search problems (in Go, and now with AlphaDev).</li></ol><p><strong>Wicked and Tame problems</strong></p><p>Some problems humans have to solve are just fundamentally harder than others. To reason about this, <a href=\"https://link.springer.com/article/10.1007/bf01405730?ref=inference.vc\">Rittel and Webber (1973)</a> defined the concept of wicked and tame problems in &#x201c;Dilemmas in a General Theory of Planning&#x201d;. Wicked problems have the following characteristics:</p><ul><li>they <strong>elude definitive formulation</strong></li><li>there is <strong>no clear stopping criterion</strong>, i.e. it&#x2019;s impossible to tell if a solution has been reached</li><li>solutions are not true-or-false but instead <strong>&#x201c;good-or-bad&#x201d;</strong></li><li>possible solution candidates <strong>cannot be enumerated</strong> or exhaustively described</li><li>there is <strong>no</strong> opportunity to learn from <strong>trial-and-error</strong> either because the problem deals with one-off situations or because any solution attempt may incur significant costs</li></ul><p>Most policy problems - where to put a highway, should our country go to war, how should we regulate AI, should Britain leave the European Union - are wicked problems. So are most business, artistic or lifestyle dilemmas we&#x2019;re faced with. One can&apos;t simply use BOTECs, decision theory and optimisation to capture all aspects of these problems.</p><p>By contrast, tame problems are the exact opposite: well defined problems, with clear stopping criteria, where the solution space is exhaustively describable, and trial-and-error learning is possible. Many problems in mathematics, puzzle-solving, engineering and the natural sciences, playing chess and some other sports fall into this category, according to Ritter and Webber.</p><p>To me, this classification almost reads like:</p><ul><li>tame = solvable by AI</li><li>wicked = requires human judgement, accountability, responsibility</li></ul><p>Mathematical theorem proving is a clear example of a tame problem. Notwithstanding some intricacies, theorem proving is a pretty well defined problem, and especially in formal mathematics, in languages like <a href=\"https://github.com/leanprover-community/mathlib?ref=inference.vc\">LEAN</a>, Isabel and others, the possible attempts at proving a statement are exhaustively describable. For the most part, it can be determined if a solution has been reached, and solutions are either true or false. Finally, getting a proof wrong has no cost or negative consequences attached to (other than perhaps energy or time cost), so the problem is amenable to trial-and-error learning.</p><p>Of course, there is more to mathematics than the generation of formally verified proofs of statements in an arguably ugly and hard-to-read language based on dependent type theory. There is conjecture generation: how do we discover statements that are likely provable. Some proofs have aesthetic value, or can be described as beautiful. Some theorems or definitions are more useful than others, as they may have implications or might make the discovery of other theorems easier. All of these aspects notwithstanding, I argue the core problem of formal theorem proving passes Ritter and Webber&#x2019;s definition of a tame problem, and thus machine intelligence may be able to deal with the problem without any human judgment in the loop.</p><p><strong>Tame Doesn&#x2019;t Mean Easy</strong></p><p>It&#x2019;s important thought to note that tame doesn&#x2019;t mean easy, nor does it mean it will be solved. Indeed, a convincing solution to general automated theorem proving eluded the field for decades. I believe the correct conclusion from this analysis is that it is &#x2018;within reach&#x2019; for machine intelligence, and new evidence on the unreasonable efficiency of code generating LLMs points at new avenues that may result in significant breakthrough. But many, great difficulties remain:</p><ul><li><strong>the search space is vast:</strong> The search space of all synthactically correct proofs is obviously enormous, up there with Go, and probably even larger/slightly less well structured. </li><li><strong>the reward signal is sparse:</strong> In two-player games like Go, one of the players always wins, and thus gets positive reinforcement after an episode of self-play. Mathematical theorem proving, and code generation in general, is a single-player game. Most proof attempts fail. Positive reinforcement only comes if the proof is verifiably correct, and that happens very rarely.</li><li><strong>data is - currently - limited:</strong> While there are plenty of data out there on informal mathematics (mathematical statements and proofs written in human language), as well as code in python or JavaScript, there is relatively little data on the internet containing formally verified software or formal mathematical proofs. <a href=\"https://github.com/leanprover-community/mathlib?ref=inference.vc\">LEAN&apos;s matlib</a> is probably the largest repository of formal mathematical proofs, anecdotally it covers all of the Imperial College undergraduate curriculum, but it&apos;s still but a tiny sliver of mathematical knowledge humans developed. There are attempts to leverage LLMs to help <a href=\"https://arxiv.org/abs/2205.12615?ref=inference.vc\">automatically formalize</a> informal mathematics thus creating more data, but it&apos;s still an open question how much of the data bottleneck can be eliminated this way.</li></ul><h3 id=\"what-if-we-do-crack-this\">What if we do crack this</h3><p>So, let&#x2019;s assume for a minute that we can crack the mathematical theorem proving puzzle, and in particular, we will build programs capable of generating correct proofs of non-trivial mathematical statements, and perhaps even generating new mathematical statements together with correct proofs. What are the implications of this?</p><p><strong>Dual use</strong></p><p>AI systems that do useful things can often be repurposed accomplish very bad things. For example, AI methods designed to help with safe drug design can also be used to design deathly toxic compounds simply by switching a sign in an objective function, as shown in <a href=\"https://www.nature.com/articles/s42256-022-00465-9?ref=inference.vc\">Dual-use of artificial-intelligence-powered drug discovery.</a></p><p>So what would the paper &#x201d;Dual-use of artificial-intelligence-powered mathematical theorem proving&#x201d; be about? I&#x2019;m not sure it&#x2019;s a question many folk working on the topic ask themselves. My approach was that theorems don&#x2019;t hurt anyone (I know several young people who would passionately disagree with this sentiment). But it&apos;s one question I started thinking about.</p><p>Mathematical theorem proving, especially the one based on formal mathematics, is closely related to software verification, and formally verifiable software engineering. It turns out, the main source of funding for research in the US is the Department of Defence, and indeed, this technology has military applications. Talia Ringer discusses a couple concrete examples in her <a href=\"https://www.inference.vc/rss/Proof Automation, Class 2: Ethics - YouTube  YouTube https:/www.youtube.com &#x203A; watch\">lecture on the ethics</a> of verification.</p><p>Besides military applications, it is also conceivable that formal verification could play a role in cybercrime or the creation of malicious software of different types. Verification is used in the design of AntiVirus software - at least at a very handwavy level, it is conceivable that it can be used to make &#x201c;provably bad&#x201d; computer viruses, too.</p><p>As you can tell I&apos;m very hand-wavy and high-level in my understanding of this, but it&apos;s an important topic to consider. If anyone has concrete pointers on the dual use of theorem proving, please send them my way.</p><p><strong>Contributions to General Language-based AI</strong></p><p>Another possible concern is that a breakthrough in mathematical theorem proving may further accelerate the development and deployment of general-purpose AI tools. &#xa0;And that can be a good thing or a bad thing, depending on your perspective.</p><p>Why would this happen? There is anecdotal evidence - or urban legend - out there that including formal languages (computer code) in the training data improves the quality of language models, especially having a positive an effect on their <a href=\"https://arxiv.org/abs/2210.07128?ref=inference.vc\">common-sense reasoning ability</a>. Of course, the idea that learning to code improves your ability to reason is sure to put a smug smile on any computer scientist&#x2019;s face, so I&#x2019;d be skeptical with how strongly supported this claim is.</p><p>But, assuming that the above hypothesis holds - that there is a positive transfer effect from training on formal languages to common sense reasoning ability in LLMs - we can reasonably expect that a bunch of new training data generated in the form of formally verified proofs could turn out to be very valuable, sweet training data for LLMs. In theory, automatic theorem proving or formal verification may provide virtually unlimited high-quality training data for LLMs, further enhancing their apparent reasoning abilities. Depending on what you think about the pace of development and deployment of general-purpose language AIs, this may be an outcome you applaud, or fear.</p><h4 id=\"loss-of-meaning\">Loss of meaning</h4><p>Of course, another topic that comes up often is just how depressing and sad forfeiting mathematics to machine intelligence is. For many, mathematics is not only a job, but a pursuit they derive meaning from. If AI is ever becomes better at Maths than people, as Fields-medalist Timothy Gowers <a href=\"https://twitter.com/wtgowers/status/1644750842526941192?ref=inference.vc\">discusses on Twitter</a>, what happens to human mathematics, what will people derive meaning from.</p><p>First of all, one can enjoy chess even if not playing at grandmaster level, and even if a Chess program easily beats me. Similarly, I personally enjoy understanding mathematics topics that have long been solved, without being the one actually discovering or proving it. So the &apos;pleasure of proving something&apos; may not be completely lost. On the other hand &apos;being the first to prove&apos; may become difficult, but it is already a rare source of pleasure.</p><p>Secondly, I think breakthroughs in automated theorem proving will probably pose new kind of questions for folk with mathematical intuition. Perhaps the work is going to be less about trying to prove or disprove conjectures, but a bit more about categorising, making human sense of mathematical knowledge, or guiding an increasingly automated process of mathematical discovery.</p>"
        }
      ]
    },
    {
      "title": "Mortal Komputation: On Hinton's argument for superhuman AI.",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Mortal Komputation: On Hinton's argument for superhuman AI."
      },
      "summary": "<p></p><p>Last week in Cambridge was Hinton bonanza. He visited the university town where he was once an undergraduate in experimental psychology, and gave a series of back-to-back talks, Q&amp;A sessions, interviews, dinners, etc. He was stopped on the street by random passers-by who recognised him from the lecture,</p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p></p><p>Last week in Cambridge was Hinton bonanza. He visited the university town where he was once an undergraduate in experimental psychology, and gave a series of back-to-back talks, Q&amp;A sessions, interviews, dinners, etc. He was stopped on the street by random passers-by who recognised him from the lecture,</p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/mortal-computation-hintons/"
        }
      ],
      "link": "https://www.inference.vc/mortal-computation-hintons/",
      "id": "64724f2336605b0001f5e7fa",
      "guidislink": false,
      "tags": [
        {
          "term": "AGI",
          "scheme": null,
          "label": null
        },
        {
          "term": "opinion",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Tue, 30 May 2023 13:48:52 GMT",
      "published_parsed": [
        2023,
        5,
        30,
        13,
        48,
        52,
        1,
        150,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<p></p><p>Last week in Cambridge was Hinton bonanza. He visited the university town where he was once an undergraduate in experimental psychology, and gave a series of back-to-back talks, Q&amp;A sessions, interviews, dinners, etc. He was stopped on the street by random passers-by who recognised him from the lecture, students and postdocs asked to take a selfie with him after his packed lectures.</p><p>Things are very different from the last time I met Hinton in Cambridge: I was a PhD student, around 12 years ago, in a Bayesian stronghold safe from deep learning influence. There was the usual email about a visiting academic, with an opportunity to put your name down if you wanted a 30 minute 1:1 conversation with him. He told us he figured out how the brain worked (again)! The idea he shared back then would eventually transform to <a href=\"https://en.wikipedia.org/wiki/Capsule_neural_network?ref=inference.vc\">capsule networks</a>. Of course everyone in our lab knew his work, but people didn&apos;t quite go as crazy.</p><p>While the craziness is partly explained by the success of deep learning, the Turing award, etc, it is safe to say that his recent change of heart on AI existential risk played a big role, too. I have to say, given all the press coverage I already read, I wasn&apos;t expecting much from the talks by way of content. But I was wrong there, the talks actually laid out a somewhat technical argument. And it worked - some very smart colleagues are now considering a change in their research direction towards beneficial AI.</p><p>I enjoyed the talks, but did I buy the arguments? I suppose I never really do. So I thought I&apos;ll try my best to write it up here, followed by a couple points of criticism I have been thinking about since then. Though touching on many topics, including subjective experiences and feelings LLMs might have, he very clearly said he only is qualified to comment on the differences between biological and digital intelligences, which he has studied for decades. Thus, I will focus on this argument, and whether this should, in itself, convince you to change or update your views on AI and X-risk.</p><h3 id=\"summary\">Summary</h3><ul><li>Hinton compares intelligence on digital and analogue hardware.</li><li>Analogue hardware allows for lower energy cost but at the cost of <em>mortality: </em>algorithm and hardware are inseparable - the argument goes.</li><li>Digital intelligence has two advantages: aggregating learning from parallel experiences, and backpropagation which is implausible on analogue hardware</li><li>Hinton concludes these advantages can/will lead to superhuman digital intelligence.</li><li>I critically evaluate the claims about both parallelism and the superiority of backprop over biologically plausible algorithms</li></ul><h2 id=\"mortal-computation\">Mortal Computation</h2><p>For a long time Hinton, and others, considered our current neural network-based &quot;artificial brains&quot;, which run on digital computers, to be inferior to biological brains. Digital neural networks fall short on energy-efficiency: biological brains consume much less energy even though by some measures they are orders of magnitude bigger and more complex than today&apos;s digital neural networks.</p><p>Hinton therefore set out to build more energy-efficient &quot;brains&quot; based on analogue hardware. Digital computers, he argues, achieve perfect separation of software and hardware by working at the level of abstraction of discrete bits. This enables computation that runs on one computer to be exactly reproduced on any other digital computer. In this sense, the software is <em>immortal</em>: if the hardware dies, the algorithm can live on on another computer. This immortality comes at a high energy price: ensuring digital computers work accurately, they consume quite a lot of energy.</p><p>This is in contrast with analogue hardware, which may contain flaws and slight variations in conductances. Thus every analogue computer is slightly different, and learning algorithms running in them have to adapt to the imperfections of analogue hardware. &#xa0;While they may consume a lot less energy, this also means that a &quot;model&quot; trained on one analogue machine cannot be easily ported to another piece of hardware as it has adapted to the specific flaws and imprecisions of the chip it was trained on. Brains running on analogue hardware are <em>mortal</em>: once the hardware dies, the algorithm dies with it.</p><blockquote>tldr: anaogue intelligence is energy efficient but mortal, digital intelligence is immortal but energy-hungry</blockquote><h2 id=\"advantages-of-digital-brains\">Advantages of digital brains</h2><p>Hinton then realised that learning algorithms running on digital devices have advantages compared to &quot;mortal&quot; algorithms running on analogue hardware.</p><p><em>Parallelism:</em> Since computation is portable, parallel copies of the same model can be run, and information/knowledge can be exchanged between these copies using high-bandwidth sharing of weights or gradient updates. Consequently, a digital &quot;mind&quot; might be performing tens of thousands of tasks in parallel, then aggregate the learnings from each of these parallel activities into a single brain. By contrast, analogue brains cannot be parallelised this way, because the imprecision of hardware makes communicating information about the contents of the model impossible. The best they can do is to &quot;tell each other&quot; what they learned, and exchange information using an inefficient form of <a href=\"https://en.wikipedia.org/wiki/Knowledge_distillation?ref=inference.vc\">knowledge distillation</a>.</p><p><em>Backpropagation:</em> In addition, a further advantage is that digital hardware allows for the implementation of algorithms like back-propagation. Hinton argued for a long time that backpropagation seems biologically implausible, and cannot be implemented on analogue hardware. The best learning algorithms Hinton could come up with for mortal computation is the <a href=\"https://arxiv.org/abs/2212.13345?ref=inference.vc\">forward-forward algorithm</a>, which is resembles <a href=\"https://www.inference.vc/evolutionary-strategies-embarrassingly-parallelizable-optimization/\">evolution strategies</a>. Its updates are a lot noisier compared to backpropagated gradients, and it really doesn&apos;t scale to any decent sized learning problem.</p><p>These two observations: that digital computation can be parallelised, and enables a superior learning algorithm, backpropagation, which analogue brains cannot implement, lead Hinton to conclude that <strong>digital brains will eventually become smarter than biological brains</strong>, and based on recent progress he believes this may happen much sooner he had previously thought, within the next 5-20 years.</p><h2 id=\"does-the-argument-hold-water\">Does the argument hold water?</h2><p>I can see a number of ways in which the new arguments laid out for why digital &apos;brains&apos; will be superior to biological ones could be attacked. Here are the two main points of counterarguments:</p><h3 id=\"how-humans-learn-vs-how-hintons-brains-learn\">How humans learn vs how Hinton&apos;s brains learn</h3><p>Hinton&apos;s argument actually critically hinges on artificial neural networks being as efficient at learning from any single interaction as biological brains are. After all, it doesn&apos;t matter how many parallel copies of an ML algorithm you run if the amount of &quot;learning&quot; you get from each of those interactions is orders of magnitude smaller than what a human would learn. So let&apos;s look at this more closely.</p><p>Hinton actually considered a very limited form of learning: imitation learning or distillation. He argues that when Alice teaches something to Bob, Bob will change the weights of his brain so that he becomes more likely to say what Alice just told her in the future. This may be how an LLM might learn, but it&apos;s not how humans learn from interaction. Let&apos;s consider an example.</p><p>As a non-native English speaker, I remember when I first encountered the concept of <a href=\"https://en.wikipedia.org/wiki/Irreversible_binomial?ref=inference.vc\">irreversible binomials</a> in English. I watched a language learning video whose content was very simple, something like:<br />&quot;We always say apples and oranges, never oranges and apples.<br />We always say black and white, never white and black.<br />etc...&quot;<br />Now, upon hearing this, I understood what this meant. I learnt the rule. Next time I said something about apples and oranges, I remembered that I shouldn&apos;t say &quot;oranges and apples&quot;. Perhaps I made a mistake, I remembered the rule exists, felt embarrassed, and probably generated some negative reinforcement from which further learning occurred. Hearing this one sentence <strong>changed how I apply this rule in lots of specific circumstances</strong>, it didn&apos;t make me more likely to go around and tell people &quot;We always say apples and oranges, never oranges and apples&quot;, I understood how to apply the rule to change my behaviour in relevant circumstances.</p><p>Suppose you wanted to teach an LLM a new irreversible binomial, for example that it should never say &quot;LLMs and humans&quot;, it should always say &quot;humans and LLMs&quot; instead. With today&apos;s model you could either</p><ul><li>fine-tune on lots of examples of sentences containing &quot;humans and LLMs&quot;, or</li><li>show it RLHF instances where a sentence containing &quot;humans and LLMs&quot; was preferred by a human over a similar sentence containing &quot;LLMs and humans&quot;</li><li>or prepend the above rule to the prompt in the future, storing the rule in-context. (this one doesn&apos;t seem like it would necessarily work well)</li></ul><p>In contrast, you can simply tell this rule to a human, they will remember it, recognise if the rule is relevant in a new situation, and use it right away, perhaps even without practice. This kind of &apos;metacognition&apos; - knowing what to learn from content, recognising if a mistake was made and learning from it - is currently is completely missing from LLMs, although as I wrote above, perhaps not for a very long time.</p><p>As a result, even if an LLM sat down with 10,000 physics teachers simultaneously, it wouldn&apos;t necessarily get 10,000 more value out of those interactions than a single biological brain spending time with a single physics teacher. That&apos;s because LLMs learn from examples, or from human preferences between various generated sentences, rather than by understanding rules and later recalling them in relevant situations. Of course, this may change very fast, this kind of learning from instruction may be possible in LLMs, but the basic point is:</p><blockquote>there is a limit to how much learning digital brains can extract from interacting with the world currently</blockquote><h3 id=\"the-it-will-never-work-type-arguments\">The &quot;it will never work&quot; type arguments</h3><p>In one of his presentations, Hinton reminded everyone that for a long time, neural networks were completely dismissed: optimisation will get stuck in a local minimum, we said, they will never work. That turned out to be completely false and misleading, local minima are not a limitation of deep learning after all.</p><p>Yet his current argument involves saying that &quot;analogue brains&quot; <strong>can&apos;t have</strong> a learning algorithm as good as backpropagation. This is mostly based on the evidence that although he tried hard, he failed to find a biologically plausible learning algorithm that is as efficient as backpropagation in statistical learning. But what if that&apos;s just what we currently think? After all the whole ML community could convince ourselves that support vector machines were superior to neural networks? What if we prematurely conclude digital brains are superior to analogue brains just because we haven&apos;t yet managed to make analogue computation work better.</p><h2 id=\"summary-and-conclusion\">Summary and Conclusion</h2><p>To summarise, Hinton&apos;s argument has two pillars:</p><ul><li>that digital intelligence can create efficiencies over analogue intelligence through parallelism, aggregating learning from multiple interactions into a single model</li><li>and that digital intelligence enables fundamentally more efficient learning algorithms (backprop-based) which analogue intelligence cannot match</li></ul><p>As we have seen, neither of these arguments are watertight, and both can be questioned. So how much credence should we put on this?</p><p>I say it passes my bar for an interesting narrative. However, as a narrative, I don&apos;t consider it much stronger than the ones we developed when we argued &quot;methods based on non-convex optimisation won&apos;t work&quot;, or &quot;nonparametric ML methods are ultimately superior to parametric ones&quot;, or &quot;very large models will overfit&quot;.</p><p>Whether LLMs, perhaps LLMs with a small number of bells and whistles used creatively will pass the &apos;human level&apos; bar (solve most tasks a human could accomplish through a text-based interface with the world)? I am currently equally skeptical of the theoretically motivated arguments either way. I personally don&apos;t expect anyone to be able to produce a convincing enough argument that it&apos;s not possible. I am a lot less skeptical about the whole premise than back in 2016 when I <a href=\"https://www.inference.vc/solve-intelligence/\">wrote about DeepMind&apos;s pursuit of intelligence</a>.</p>"
        }
      ]
    },
    {
      "title": "Autoregressive Models, OOD Prompts and the Interpolation Regime",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Autoregressive Models, OOD Prompts and the Interpolation Regime"
      },
      "summary": "<p>A few years ago I was very much into maximum likelihood-based generative modeling and autoregressive models (see <a href=\"https://www.inference.vc/composite-ovjective-functions-for-training-autoregressive-models/\">this</a>, <a href=\"https://www.inference.vc/maximum-likelihood-for-representation-learning-2/\">this</a> or <a href=\"https://www.inference.vc/masked-autoencoders-icml-paper-highlight/\">this</a>). More recently, my focus shifted to characterising inductive biases of gradient-based optimization focussing mostly on supervised learning. I only very recently started combining the two ideas, revisiting autoregressive models</p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>A few years ago I was very much into maximum likelihood-based generative modeling and autoregressive models (see <a href=\"https://www.inference.vc/composite-ovjective-functions-for-training-autoregressive-models/\">this</a>, <a href=\"https://www.inference.vc/maximum-likelihood-for-representation-learning-2/\">this</a> or <a href=\"https://www.inference.vc/masked-autoencoders-icml-paper-highlight/\">this</a>). More recently, my focus shifted to characterising inductive biases of gradient-based optimization focussing mostly on supervised learning. I only very recently started combining the two ideas, revisiting autoregressive models</p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/autoregressive-models-in-out-of-distribution/"
        }
      ],
      "link": "https://www.inference.vc/autoregressive-models-in-out-of-distribution/",
      "id": "64230e9f8c9ebb003c71c8d7",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Thu, 30 Mar 2023 11:54:26 GMT",
      "published_parsed": [
        2023,
        3,
        30,
        11,
        54,
        26,
        3,
        89,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<p>A few years ago I was very much into maximum likelihood-based generative modeling and autoregressive models (see <a href=\"https://www.inference.vc/composite-ovjective-functions-for-training-autoregressive-models/\">this</a>, <a href=\"https://www.inference.vc/maximum-likelihood-for-representation-learning-2/\">this</a> or <a href=\"https://www.inference.vc/masked-autoencoders-icml-paper-highlight/\">this</a>). More recently, my focus shifted to characterising inductive biases of gradient-based optimization focussing mostly on supervised learning. I only very recently started combining the two ideas, revisiting autoregressive models throuh the lens of inductive biases, motivated by a desire to understand a bit more about LLMs. As I did so, I found myself surprised by a number of observations, which really should not have been surprising to me at all. This post documents some of these.</p><h2 id=\"ar-models-distributions-over-sequences\">AR models &gt; distributions over sequences</h2><p>I have always associated AR models as just a smart way to parametrize probability distributions over multidimensional vectors or sequences. Given a sequence, one can write</p><p>$$<br />p(x_1, x_2, \\ldots, x_N;\\theta) = \\prod_{n=1}^{N} p(x_n\\vert x_{1:n-1}, \\theta)<br />$$</p><p>This way of defining a parametric joint distribution is computationally useful because it is significantly easier to ensure proper normalization of a conditional probability over a single variable than it is to ensure proper normalization over a combinatorially more complex space of entire sequences.</p><p>In my head, I considered there to be a 1-to-1 mapping between autoregressive models (a family of conditional probabilities $\\{p(x_n\\vert x_{1:n-1}), n=1\\ldots N\\}$) and joint distributions over sequences $p(x_1,\\ldots,x_N)$. However, I now understand that is, generally, a many-to-one mapping.</p><figure class=\"kg-card kg-image-card\"><img alt=\"alt\" class=\"kg-image\" height=\"367\" src=\"https://www.inference.vc/content/images/2023/03/Untitled-drawing--7-.png\" width=\"679\" /></figure><p>Why is this the case? Consider a distribution over two binary variables $X_1$ and $X_2$. Let&apos;s say that, $\\mathbb{P}(X_1=0, X_2=0) = \\mathbb{P}(X_1=0, X_2=1) = 0$, meaning that $\\mathbb{P}(X_1=0)=0$, i.e. $X_1$ always takes the value of $1$. Now, consider two autoregressive models, $Q_1$ and $Q_2$. If these models are to be consistent with $P$, they must agree on some facts:</p><ul><li>$\\mathbb{Q}_1[X_2=x\\vert X_1=1] = \\mathbb{Q}_2[X_2=x\\vert X_1=1] = \\mathbb{P}[X_2=x\\vert X_1=1] = \\mathbb{P}[X_2=x]$ and</li><li>$\\mathbb{Q}_1[X_1=1] = \\mathbb{Q}_2[X_1=1] = 1$.</li></ul><p>However, they can completely disagree on the conditional distribution of $X_2$ given that the value of $X_1$ is $0$, for example $\\mathbb{Q}_1[X_2=0\\vert X_1=0] =1$ but $\\mathbb{Q}_2[X_2=0\\vert X_1=0] =0$. The thing is, $X_1$ is never actually $0$ under either of the models, or $P$, yet both models still specify this conditional probability. So, $Q_1$ and $Q_2$ are not the same, yet they define exactly the same joint distribution over $X_1$ and $X_2$.</p><h3 id=\"zero-probability-prompts\">Zero probability prompts</h3><p>The meaning of the above example is that two AR models that define exactly the same distribution over sequences can arbitrarily disagree on how they complete $0$-probability prompts. Let&apos;s look at a more language modely example of this, and why this matters for concepts like systemic generalisation.</p><p>Let&apos;s consider fitting AR models on a dataset generated by a probabilistic context free grammar $\\mathbb{P}[S=\\&quot;a^nb^n\\&quot;]=q^n(1-q)$. To clarify my notation, this is a distribution over sentences $S$ that take are formed by a number of `a` characters followed by the same number of `b` charactes. Such a langauge would sample $ab$, $aabb$, $aaabbb$, etc, with decreasing probability, and only these. </p><p>Now consider two autoregressive models, $Q_1$ and $Q_2$, which both perfectly fit our training distribution. By this I mean that $\\operatorname{KL}[Q_1\\|P] = \\operatorname{KL}[Q_2\\|P] = 0$. Now, you can ask these models different kinds of questions.</p><ul><li>You can ask for a random sentence $S$, and you get random samples returned, which are indistinguishable from sampling from $P$. $Q_1$ and $Q_2$ are <strong>equivalent</strong> in this scenario.</li><li>You can use a weird sampling method, like temperature scaling or top-p sampling to produce samples from a different distribution than $P$. It&apos;s a very interesting question what this does, but if you do this in $Q_1$ and $Q_2$, you&apos;d still get the same distribution of samples out, so the two models would be <strong>equivalent</strong>, assuming they both fit $P$ perfectly.</li><li>You can ask each model to complete a typical prompt, like $aaab$. Both models will complete this prompt exactly in the same way, responding $bb$ and the end of sequence token. The two models are <strong>equivalent</strong> in terms of completions of in-distribution prompts.</li><li>You can ask each model to complete a prompt that would never naturally be seen, like $aabab$, the two models are <strong>not equivalent</strong>. P doesn&apos;t prescribe how this broken prompt should be completed as it doesn&apos;t follow the grammar. But both $Q_1$ and $Q_2$ will give you a completion, and they may give completely different ones.</li></ul><p>When asked to complete an out-of-distribution prompt, how will the models behave? There is no right or wrong way to do this, at least not that one can derive from $P$ alone. So, let&apos;s say:</p><ul><li>$Q_1$ samples completions like $\\mathbf{aabab}b$, $\\mathbf{aabab}bab$, $\\mathbf{aabab}bbaba$, while</li><li>$Q_2$ samples $\\mathbf{aabab}$, $\\mathbf{aabab}bbb$, $\\mathbf{aabab}bbbbbb$.</li></ul><p>In this example we can describe the <em>behaviour</em> of each of the two models. $Q_1$ applies the rule &quot;the number of $a$s and $b$s must be the same&quot;. This is a rule that holds in $P$, so this model has extrapolated this rule to sequences outside of the grammar. $Q_2$ follows the rule &quot;once you have seen a $b$ you should not generate any more $a$s&quot;. Both of these rules apply in $P$, and in fact, the context free grammar $a^nb^n$ can be defined as the product of these two rules (thanks to Gail Weiss for pointing this out during her Cambridge visit). So we can say that $Q_1$ and $Q_2$ extrapolate very differently, yet, they are equivalent generative models at least so far as the distribution of sequences is concerned. They both are global minima of the training loss. Which one we&apos;re more likely t get as a result of optimization is up to inductive biases. And of course, there&apos;s more than just these two models, there are infinitely many different AR models consistent with P that differ in their behaviour.</p><h3 id=\"consequence-for-maximum-likelihood-training\">Consequence for maximum likelihood training</h3><p>An important consequence of this is that model likelihood alone doesn&apos;t tell us the full story about the quality of an AR model. Even though cross entropy is convex in the space of probability distributions, it is not strictly convex in the space of AR models (unless the data distribution places non-zero probability mass on all sequences, which is absolutely not the case with natural or computer language). Cross entropy has multiple - infinitely many - global minima, even in the limit of infinite training data. And these different global minima can exhibit a broad range of behaviours when asked to complete out-of-distribution prompts. Thus, it is down to inductive biases of the training process to &quot;choose&quot; which minima are likelier to be reached than others.</p><figure class=\"kg-card kg-image-card\"><img alt=\"alt\" class=\"kg-image\" height=\"367\" src=\"https://www.inference.vc/content/images/2023/03/Untitled-drawing--8-.png\" width=\"679\" /></figure><h3 id=\"low-probability-prompts\">Low probability prompts</h3><p>But surely, we are not just interested in evaluating language models on zero probability prompts. And how do we even know if the prompts we are using are truly zero probability under the &quot;true distribution of language&quot;? Well, we don&apos;t really need to consider truly zero probability promots. The above observation can likely be softened to consider AR models&apos; behaviour on a set of prompts with sufficiently small probability of occurrence. Here is a conjecture, and I leave it to readers to formalize this fully, or prove some simple bounds for this:</p><p>Given a set $\\mathcal{P}$ of prompts such that $\\mathbb{P}(\\mathcal{P}) = \\epsilon$, it is possible to find two models $Q_1$ and $Q_2$ such that $Q_1$ and $Q_2$ approximate $P$ well, yet $Q_1$ and $Q_2$ give very different completions to prompts in \\mathcal{P}.</p><p>Consequence: so long as a desired &apos;capability&apos; of a model can be measured by evaluating completions of an AR model on a set of prompts that occurr with small probability, cross-entropy cannot distinguish between capable and poor models. For any language model with good performance on a limited benchmark, we will be able to find another language model that matches the original on cross entropy loss, but which fails on the benchmark completely.</p><p>A good illustration of the small probability prompt set is given by Xie et al (2021) &#xa0;who interpret in-context learning ability of language models as <a href=\"https://www.inference.vc/implicit-bayesian-inference-in-sequence-models/\">implicit Bayesian inference</a>. In that paper, the set of prompts that correspond to in-context few-shot learning prompts have vanishing probability under the model, yet the model gives meaningful responses in that case.</p><h3 id=\"language-models-and-the-interpolation-regime\">Language Models and the Interpolation Regime</h3><p>Over the years, a large number of mathematical models, concepts and analysis regimes have been put forward in an attempt to generate meaningful explanations of the success of deep learning. One such idea is to study optimization in the <strong>interpolation regime</strong>: when our models are powerful enough to achieve zero training error, and mimimizing the training loss thus becomes an underdetermined problem. The interesting outstanding question becomes which of the global minima of the training loss our optimization selects.</p><p>The scenarios often studied in this regime include regression with a finite number of samples or separable classification where even a simple linear model can reach zero training error. I&apos;m very excited about the prospect of studying AR generative models in the interpolation regime: where cross entropy approaches the entropy of the training data. Can we say something about the types of OOD behaviour gradient-based optimization favours in such cases?</p>"
        }
      ]
    },
    {
      "title": "We May be Surprised Again: Why I take LLMs seriously.",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "We May be Surprised Again: Why I take LLMs seriously."
      },
      "summary": "<p>&quot;Deep Learning is Easy, Learn something Harder&quot; - I proclaimed in <a href=\"https://www.inference.vc/deep-learning-is-easy/\">one of my early and provocative blog posts</a> from 2016. While some observations were fair, that post is now evidence that I clearly underestimated the impact simple techniques will have, and probably gave counterproductive advice.</p><p>I wasn&</p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>&quot;Deep Learning is Easy, Learn something Harder&quot; - I proclaimed in <a href=\"https://www.inference.vc/deep-learning-is-easy/\">one of my early and provocative blog posts</a> from 2016. While some observations were fair, that post is now evidence that I clearly underestimated the impact simple techniques will have, and probably gave counterproductive advice.</p><p>I wasn&</p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/we-may-be-surprised-again/"
        }
      ],
      "link": "https://www.inference.vc/we-may-be-surprised-again/",
      "id": "6409ab352445f5003d117961",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Wed, 22 Mar 2023 14:55:54 GMT",
      "published_parsed": [
        2023,
        3,
        22,
        14,
        55,
        54,
        2,
        81,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<p>&quot;Deep Learning is Easy, Learn something Harder&quot; - I proclaimed in <a href=\"https://www.inference.vc/deep-learning-is-easy/\">one of my early and provocative blog posts</a> from 2016. While some observations were fair, that post is now evidence that I clearly underestimated the impact simple techniques will have, and probably gave counterproductive advice.</p><p>I wasn&apos;t alone in my deep learning skepticism, in fact I&apos;m far from being the most extreme deep learning skeptic. Many of us who <em>grew up</em> working in Bayesian ML, convex optimization, kernels and statistical learning theory confidently predicted the inevitable failure of deep learning, continued to claim deep nets did nothing more than memorize training data, ignoring all evidence to the contrary.</p><p>What was behind this? Beyond the unappealing hackiness of early DL, a key reason is that we misapplied some intuitions from key work in statistical learning theory. Well-known findings from learning theory (of the Vapnik&#x2013;Chervonenkis or Rademacher flabour) gave guarantees for generalisation when the model class was sufficiently small. Many of us informally misused these results to imply that &quot;you can&apos;t have generalisation unless your model class is simple&quot;. And deep learning is the opposite of a simple model classes. Ergo, it won&apos;t/can&apos;t work. Any evidence to the contrary was therefore dismissed as cherry picking, random seed hacking or overfitting dressed up as success.</p><p>To be clear, there has been a lot of poorly motivated or poorly reproducible research, especially in RL, and thus some skepticism was well justified. To alleviate doubts, I still think theoretically motivated skepticism is a good thing, and <a href=\"https://www.inference.vc/my-thoughts-on-alchemy/\">rigour</a> - which much of early deep learning lacked - is important. But some of deep learning is clearly more than overfitting dressed up as success, and many of us resisted this general idea for too long.</p><p>What changed this? An important change-point in many of our attitudes was the 2016 paper <a href=\"https://arxiv.org/abs/1611.03530?ref=inference.vc\">Understanding deep learning requires rethinking generalization</a>. The way I remember this paper is this: &quot;deep nets have maximal Rademacher complexity, generalisation theory thus predicts deep learning shouldn&apos;t work, but it is clear it does, therefore our theory is insufficient.&quot; This seems almost trivial now, but back then, it represented a massive shift. It&apos;s the theory that needed fixing, not deep learning. It opened up a massive opportunity for people to come up with new theory, develop new intuitions. We&apos;re nowhere near a modern theory of deep learning, but we know a lot more about the components at play.</p><blockquote>It may have been alchemy, but some actual gold was produced.</blockquote><h2 id=\"are-we-wrong-about-llms\">Are we wrong about LLMs?</h2><p>Today, I see a similar pattern of resistance to taking LLM results seriously. Many of my colleagues&apos; views on LLMs have not changed at all over the last couple of years. Some describe them as being good only at &quot;regurgitating training data&quot;. I also see similar patterns of misusing and overgeneralising theoretical arguments. </p><p>For example, the field of causal inference established the impossibility of inferring causal structure from i.i.d. observations. I wrote about this in my post on <a href=\"https://www.inference.vc/untitled/\">causal inference</a>. Many people then overgeneralise this important but narrow finding to mean &quot;an ML model can absolutely never learn causal reasoning unless you add some extra causal model&quot;. But what exactly does this result have to do with whether LLMs can (appear to) correctly reason about causal structure in the world when they complete prompts? LLMs, indeed, are statistical models fit to i.i.d. data. But the non-identifiability result is only relevant if we apply it to learning causal relationships between consecutive tokens in text. But those are not the causal relationships we need. We want the LLM to &quot;understand&quot; or appear to understand that kicking a ball results in the ball moving. That causal relationship is encoded in language, it doesn&apos;t have to be inferred from i.i.d. observational data.</p><h2 id=\"my-own-attempts-at-dismissing-llms\">My own attempts at dismissing LLMs</h2><p>My instinct has been to reject the idea of pre-trained LLMs as the general-purpose inference machines they are branded as. I visited OpenAI in 2018, where I was shown a preview of what turned out to be key GPT-2 and GPT-3 results: the ability for a pre-trained LM to solve problems it wasn&apos;t explicitly trained, via zero-shot prompt engineering. My immediate reaction was that this can&apos;t possibly work. Sure, you&apos;ll be able to do it better than chance, but this approach will never be even nearly competitive with specialised solutions. It&apos;s interesting to try and formalize why I thought this. I had two kinds of thoughts about this:</p><h3 id=\"the-no-free-lunch-flavoured-argument\">The No Free Lunch flavoured argument</h3><p>Even though I personally never thought very much of no free lunch theorems in ML, in retrospect my reason for dismissing the &apos;one model can do it all&apos; approach of GPT was essentially a no free lunch argument in disguise.</p><p>I tried to formalize my argument along these lines:</p><ul><li>Consider a distribution of tasks we want to solve. For simplicity, let&apos;s assume each task is a supervised prediction problem, where we have a joint distribution over some input $x$ and corresponding desired output $y$. Each task $\\mathcal{T}$ would then be a joint distribution over $(x, y)$ pairs, and perhaps an associated loss function.</li><li>I considered the language model as being nothing more than a stochastic process over character sequences. I didn&apos;t really care it was trained on natural language. I just thought of it as distribution of completions given prompts. I was happy to assume that the LLM could be the best such distribution there is.</li><li>We use the LLM to solve a task $\\mathcal{T}$ in an encoder-LLM-decoder sandwich. The encoder would be a mapping between some input $x$ to a character sequence or prompt (a.k.a. the prompt engineering part). The decoder would then take the LLM&apos;s completion and return a label or prediction $\\hat{y}$ of some sort.</li></ul><p>In this setting, it feels intuitively true that the more distinct tasks $\\mathcal{T}_i$ we want to be able to solve satisfactorily, the more likely it will be that conflicts emerge between the different tasks. Mathematically, this could be resolved by increasing the average length of prompts needed to solve all tasks. For example, if you only want your model to be able to solve a single task, like English-Spanish translation, your prompt only has to contain the source sentence. If you now also want to do English-French tanslation, or sentiment prediction, your prompts have to be longer to indicate to the model what task it should solve and how it should interpret the prompt. My hypothesis was, informally, that if we want a single encoder-LLM-decoder model to be able solve all tasks we care about, the prompt lengths in that model would have to be exponentially long at the very least. I think it likely that one could prove a formal result along these lines for a sufficiently rich of target tasks. This kind of thinking is very similar to Turing machine/universal grammar thinking, and there are a lot of parallels to Chomsky&apos;s objections to LLM.</p><p>However, I no longer subscribe to this no free lunch argument, for the same reason I actually never really felt my work was limited by other no free lunch theorems.</p><p> It&apos;s very difficult to describe all tasks we ever want an agent to solve before we label it &apos;intelligent&apos; or &apos;capable&apos;, or &apos;a good Bing&apos;. Let&apos;s call these tasks useful tasks. I now suspect that useful tasks are a tiny sliver of &quot;all possible tasks&quot; one would probably encounter in a formally stated no free lunch theorem. We don&apos;t need AI to solve any adversarially chosen task, we&apos;ll be happy if it can solve the typical task we <em>expect</em> it to solve. This is similar to saying we don&apos;t actually care about approximating any measurable function over images, we care about building tools that can do certain things in images, like recognise pedestrians.</p><h3 id=\"the-were-optimizing-the-wrong-objective-function-argument\">The &quot;We&apos;re optimizing the wrong objective function&quot; argument</h3><p>I have also been a long-time proponent of paying attention to the objective function, and less attention to things like architecture. I said if your objective function you optimise doesn&apos;t reflect the task you&apos;re using your model for, no amount of engineering or hacks will help you bridge that gap. I made this argument repeatedly in the context of representation learning, generative modelling, video compression, etc.</p><p>I argued <a href=\"https://www.inference.vc/maximum-likelihood-for-representation-learning-2/\">that maximum likelihood is not a good objective function</a> for representation learning, creating the memorable poop-diagram, which I continue to use in my lectures today. As LLMs are being trained via likelihood, it&apos;s only natural that my first instinct was that maximum likelihood can&apos;t be a good objective function for generally intelligent behaviour&#x2122; either. Why would getting better at next-token-prediction lead to near-optimal behaviour in a range of tasks that are, at best, underrepresented in the training data?</p><p>I have now abandoned this argument as well. Why? Because these arguments of mine do not consider inductive biases of the training process. I have now realised that $\\operatorname{argmin}\\mathcal{L}$ is a very poor description of what actually happens in deep learning. It&apos;s pointless to hope that any minimum of a loss function will have a desired property, it&apos;s sufficient in practice if the loss function has some good minima with the desired attributes and that SGD has a tendency to find those over the bad minima.</p><p>True, we have barely a clue on what inductive biases SGD on a model like GPT-3 has - even less if we consider components like RLHF or CoT prompting. But the fact that we that can&apos;t describe it doesn&apos;t mean that unreasonably helpful inductive biases can&apos;t be there. And evidence is mounting that they are there.</p><p>As intellectually unsatisfying as this conclusion is, the LLM approach works, but most likely not for any of the reasons we know. We may be surprised again.</p>"
        }
      ]
    },
    {
      "title": "Implicit Bayesian Inference in Large Language Models",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Implicit Bayesian Inference in Large Language Models"
      },
      "summary": "<p>This intriguing paper kept me thinking long enough for me to I decide it&apos;s time to resurrect my blogging (I started writing this during ICLR review period, and realised it might be a good idea to wait until that&apos;s concluded)</p><ul><li>Sang Michael Xie, Aditi Raghunathan, Percy</li></ul>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>This intriguing paper kept me thinking long enough for me to I decide it&apos;s time to resurrect my blogging (I started writing this during ICLR review period, and realised it might be a good idea to wait until that&apos;s concluded)</p><ul><li>Sang Michael Xie, Aditi Raghunathan, Percy</li></ul>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/implicit-bayesian-inference-in-sequence-models/"
        }
      ],
      "link": "https://www.inference.vc/implicit-bayesian-inference-in-sequence-models/",
      "id": "619e783462fbf1003bd8b017",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Thu, 03 Mar 2022 13:57:26 GMT",
      "published_parsed": [
        2022,
        3,
        3,
        13,
        57,
        26,
        3,
        62,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<p>This intriguing paper kept me thinking long enough for me to I decide it&apos;s time to resurrect my blogging (I started writing this during ICLR review period, and realised it might be a good idea to wait until that&apos;s concluded)</p><ul><li>Sang Michael Xie, Aditi Raghunathan, Percy Liang and Tengyu Ma (2021) <a href=\"https://arxiv.org/abs/2111.02080?ref=inference.vc\">An Explanation of In-context Learning as Implicit Bayesian Inference</a></li></ul><p>I liked this paper because it relates to one of my favourite concepts and ideas: exchangeability. And it took me back to thoughts I had back in 2015 (pre-historic by deep learning standards) about leveraging <a href=\"https://www.inference.vc/exchangeable-processes-via-neural-networks/\">exchangeable sequence models</a> to implement giant general-purpose learning machines. In <a href=\"https://www.inference.vc/exchangeable-processes-via-neural-networks/\">that old post</a> I made this observation about exchangeable models:</p><blockquote>If we had an exchangeable RNN, we could train it on multiple unsupervised learning problems over the same input space. Such system actually <strong><strong>learns to learn</strong></strong>. If you want to use it on a new dataset, you just feed it into the RNN, and it will give you Bayesian predictive probabilities without any additional computation. So it would be an ultimate general inference machine&#x2122;.</blockquote><p>Fast forwarding a bit, <em>ultimate general inference machine</em> (fortunate I trademarked it) is not actually all that far from how OpenAI&apos;s GPT-3 is sometimes branded and used. It&apos;s been demonstrated that you can repurpose them as few-shot (or in some cases zero-shot) learners in a surprising variety of tasks (<a href=\"https://arxiv.org/abs/2005.14165?ref=inference.vc\">Brown et al, 2020</a>). This ability of language models to solve different tasks by feeding them cleverly designed prompts is sometimes referred to as prompt-hacking or <em>in-context learning</em>.</p><p>Quite honestly, I never connected these dots until I read this paper: the motivations for leveraging one single big exchangeable sequence model as a general-purpose learner, and the more recent trend of in-context learning using GPT-3. In fact, I was deeply skeptical about the latter, thinking of it as another hack that must be somehow fundamentally flawed. But this paper by <a href=\"https://arxiv.org/abs/2111.02080?ref=inference.vc\">Xie et al (2021</a>) connected those dots for me, which is why I found it so fascinating, and I will never think of &apos;prompt hacking&apos; or in-context learning quite the same way.</p><h3 id=\"exchangeable-sequences-as-implicit-learning-machines\">Exchangeable sequences as Implicit Learning Machines</h3><p>Before talking about the paper, let me first refresh those old ideas about exchangeable sequences and implicit learning. An exchangeable sequence model is a probability distribution $p(x_1, x_2, \\ldots)$ over sequences that is invariant to permutations of the tokens within the sequence, i.e. $p(x_1, x_2, \\ldots, x_N) = p(x_{\\pi_1}, x_{\\pi_2}, \\ldots, x_{\\pi_N})$ for any permutation $\\pi$.</p><p>The de Finetti theorem connects such sequence models to Bayesian inference, saying that any such distribution can be decomposed as a mixture over i.i.d. sequence models:</p><p>$$<br />p(x_1, x_2, \\ldots, x_N) = \\int \\prod_{n=1}^N p(x_n\\vert \\theta) d\\pi(\\theta)<br />$$</p><p>As a consequence, the one-step-ahead predictive distribution (which predicts the next token in the sequence) also always has a decomposition as Bayesian integration:</p><p>$$<br />p(x_N \\vert x_1, x_2, \\ldots, x_{N-1}) = \\int &#xa0;p(x_N\\vert \\theta) d\\pi(\\theta\\vert x_1, \\ldots, x_N),<br />$$</p><p>where $\\pi(\\theta\\vert x_1, \\ldots, x_N)$ is the Bayesian posterior obtained from the prior $\\pi(\\theta)$ via the Bayes rule:</p><p>$$<br />\\pi(\\theta \\vert x_1, x_2, \\ldots, x_{N-1}) \\propto \\pi(\\theta) \\prod_{n=1}^{N-1}p(x_n\\vert \\theta)<br />$$</p><p>So in this sense, if we have an exchangeable sequence model, we can think of these one-step-ahead predictive distributions as <em>implicitly performing Bayesian inference</em>. Crucially, this happens even if we don&apos;t know what $\\theta$ is, or what our prior $\\pi$ is, or what the likelihood $p(x_i\\vert \\theta)$ is. We don&apos;t have to explicitly specify what those components are, de Finetti theorem guarantees that they exist, so long as the predictions $p(x_N \\vert x_1, x_2, \\ldots, x_{N-1})$ are consistent with an exchangeable sequence model.</p><p>This thought motivated me to try and design RNNs (remember this was in pre-transformer times) that always produce exchangeable distributions by construction. This turned out to be very difficult, but the idea eventually evolved into <a href=\"https://arxiv.org/abs/1802.07535?ref=inference.vc\">BRUNO</a> (named after Bruno de Finetti), a flexible meta-trained model for exchangeable data which exhibited few-shot concept learning abilities. This idea then got extended in a number of ways in <a href=\"https://irakorshunova.github.io/phd_iryna_korshunova.pdf?ref=inference.vc\">Ira Korshunova&apos;s PhD thesis</a>.</p><h3 id=\"from-exchangeable-sequences-to-mixtures-of-hmms\">From Exchangeable sequences to Mixtures of HMMs</h3><p>But GPT-3 is a language models, and clearly language tokens are not exchangeble. So whats the connection?</p><p>There are interesting extensions to the concept of exchangeability, which come with interesting generalisations of the de Finetti-type theorems. <em>Partial exchangeability,</em> as defined by <a href=\"https://projecteuclid.org/journals/annals-of-probability/volume-8/issue-1/De-Finettis-Theorem-for-Markov-Chains/10.1214/aop/1176994828.full?ref=inference.vc\">Diaconis and Freedman (1980)</a>, is an invariance property of a distribution over sequences which guarantees that the sequence can be decomposed as a mixture of Markov-chains. Thus, one can say that a partially exchangeable process implicitly performs Bayesian inference over Markov chains, much the same way exchangeable processes can be said to be preforming inference over i.i.d. data generating processes.</p><p>In this new paper, <a href=\"https://arxiv.org/abs/2111.02080?ref=inference.vc\">Xie et al (2021</a>), assume that the sequence model we work with is a mixture of hidden Markov models (HMMs). This is more general still than the partially exchangeable mixture of Markov chains of Diaconis and Freedman. I don&apos;t know if Mixtures of HMMs (MoHMMs) can be characterised by an exchangeability-like invariance property, but that&apos;s somewhat irrelevant now. In fact, <a href=\"https://arxiv.org/abs/2111.02080?ref=inference.vc\">Xie et al (2021</a>) never mention exchangeability in the paper. The core argument about implicit Bayeisan inferencec holds every time we work with a sequence model which is a mixture of simpler distributions: you can interpret the one-step-ahead predictions as implicitly performing Bayesian inference over some parameter. While it is unlikely that the distribution of human language from the internet follows a MoHMM distribution, it is reasonable to assume that the distribution over sequences that comes out of a GPT-3 is perhaps a mixture of some sort. And if that is the case, predicting the next token implicitly performs Bayesian inference over some parameter $\\theta$, which the authors refer to as a &apos;concept&apos;.</p><h3 id=\"in-context-learning\">In context learning</h3><p>The core idea of this paper is that perhaps in-context learning exploits this implicit Bayesian inference, inherent to statistical models of language, to solve tasks. Language models learn to implicitly make probabilistic inferences about concepts - whatever those are - because learning to carry out such inferences is needed to do well on next-token-prediction tasks. One that implicit learning capability is there, one can hijack it to perform other tasks that also require such inferences, including few-shot classification.</p><p>I think this is a very intriguing general idea. But then the key question the authors focus is somewhat disappointingly specific and artificial: Although a MoHMMs can be used to `complete sequences` drawn from a specific HMM (one of the mixture components) what happens if we ask the MoHMMs to complete sequences that they would never natively generate, for example an artificially constructed sequence that has a few-shot classification task embedded inside. This then becomes a question about distribution mismatch. The key findings is that, despite this distribution mismatch, the implicit inference machinery inside MoHMMs is able to identify the right concept and use it to make correct predictions in the few-shot task.</p><p>However - and please read the paper for specific details - , the analysis makes very strong assumptions about how the in-context-learning task embedded in the sequence is related to the MoHMM distribution. In a way, the in-context task the authors study is in fact more like a few-shot sequence completion task than, say, a classification task.</p><p>All in all, this was a fun paper to think about, and one that definitely changed my way of thinking about the whole in-context-learning and language-models-as-few-shot-learners direction.</p>"
        }
      ]
    },
    {
      "title": "Eastern European Guide to Writing Reference Letters",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Eastern European Guide to Writing Reference Letters"
      },
      "summary": "<p><em>Excruciating.</em> One phrase I often use to describe what it&apos;s like to read reference letters for Eastern European applicants to PhD and Master&apos;s programs in Cambridge. </p><p>Even objectively outstanding students often receive dull, short, factual, almost negative-sounding reference letters. This is a result of (A) <a href=\"https://erinmeyer.com/books/the-culture-map/?ref=inference.vc\">cultural</a></p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p><em>Excruciating.</em> One phrase I often use to describe what it&apos;s like to read reference letters for Eastern European applicants to PhD and Master&apos;s programs in Cambridge. </p><p>Even objectively outstanding students often receive dull, short, factual, almost negative-sounding reference letters. This is a result of (A) <a href=\"https://erinmeyer.com/books/the-culture-map/?ref=inference.vc\">cultural</a></p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/the-east-european-guide-to-writing-reference-letters/"
        }
      ],
      "link": "https://www.inference.vc/the-east-european-guide-to-writing-reference-letters/",
      "id": "6212686b35ff93003b346ee6",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Mon, 28 Feb 2022 14:29:15 GMT",
      "published_parsed": [
        2022,
        2,
        28,
        14,
        29,
        15,
        0,
        59,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<p><em>Excruciating.</em> One phrase I often use to describe what it&apos;s like to read reference letters for Eastern European applicants to PhD and Master&apos;s programs in Cambridge. </p><p>Even objectively outstanding students often receive dull, short, factual, almost negative-sounding reference letters. This is a result of (A) <a href=\"https://erinmeyer.com/books/the-culture-map/?ref=inference.vc\">cultural differences</a> - we are very good at sarcasm, painfully good at giving direct negative feedback, not so good at praising others and (B) the fact that reference letters play no role in Eastern Europe and most professors have never written or seen a good one before.</p><p>Poor reference letters hurt students. They give us no insight into the applicant&apos;s true strengths, and no ammunition to support the best candidates in scholarship competitions or the admission process in general. I decided to write this guide for students so they can share it with their professors when asking for reference letters. Although reading letters from the region is what triggered me to write this, mist of this advice should be generally useful for many other people who don&apos;t know how to write good academic reference letters.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img alt=\"alt\" class=\"kg-image\" height=\"759\" src=\"https://www.inference.vc/content/images/2022/02/quality-continuum-1.jpg\" width=\"636\" /><figcaption>Illustration of Eastern European subjective scale. Source: the almighty Internet.</figcaption></figure><h2 id=\"high-level-goals\">High-Level Goals</h2><ul><li><strong>Help the supervisor to make a case for admitting a student: </strong>The reference letter is very important in the whole admissions process. In competitive places in Europe, there is often competition not just between applicants, but also between different research groups and supervisors about whose student gets funding. Reference letters are often used as ammunition to justify decisions internally, and to determine who gets prioritised for various scholarship and funding competitions.</li><li><strong>Help put candidate&apos;s profile into context:</strong> If you write a reference letter from a region like Eastern Europe, keep in mind how difficult it is to compare candidates from wildly different education systems and backgrounds. Is someone with a 4.9/5.0 GPA from Hungary more impressive than someone with a 9.5/10.0 GPA from Serbia? Your job, partly, is to explain to the admissions committee what the student&apos;s achievements mean in a global context. Do not use abbreviations that are not internationally obvious. Do not assume the reader has ever heard of your institution. <em>Explain everything.</em></li></ul><h2 id=\"basic-hygiene-and-format\">Basic Hygiene and Format</h2><ul><li><strong>Confidentiality: </strong>Please do not ask the student to write their own recommendation letter. Sadly, many professors do it, but this is <em>not acceptable</em>, especially for the best students who apply to a top institution. You can also assume your reference letter is confidential. Don&apos;t share it with the student directly (Why? You probably want to write nicer things than you are comfortable sharing with them directly.)</li><li><strong>Length:</strong> Reference letters for the best candidates are often <em>2 full pages</em> long. Something that&apos;s half a page or just two paragraphs is interpreted as &apos;weak support&apos; or worse.</li><li><strong>Format:</strong> Although plain text is often accepted on submissions forms, when possible, please submit a PDF on letter-headed format (where the institution&apos;s logo, name, etc appear on the header). The format should follow the layout of a formal letter. You may address it &apos;To Whom It May Concern,&apos; or &apos;To the Admissions Committee,&apos; or to &apos;Dear Colleagues,&apos; or if you know the potential supervisor, by all means make it personal, address it to them. Obviously sign the letter with your name and title.</li><li><strong>Basic contents:</strong> Make sure that the letter mentions your full job title, affiliation, the candidate&apos;s full name, and the name of the programme/job/scholarship they are applying for.</li></ul><h2 id=\"contents\">Contents</h2><p>Below is an example structure that is often used. (I&apos;ll use Marta as an example because I don&apos;t have a student called Marta so it won&apos;t get personal)</p><ul><li><strong>Introduction: </strong>A few sentences mentioning who you&apos;re recommending &#xa0;and for what program, for example &quot;I&apos;m writing to recommend Marta Somethingova for the Cambridge MPhil in Advanced Computer Science.&quot; The second sentence should clearly indicate how strongly you are recommending this candidate. Factual statements signal this is a lukewarm recommendation (they asked me and I had to write something). To convey your enthusiasm, you can write something like &apos;&apos;Marta is the strongest student I&apos;ve worked with in the last couple years&quot;.</li><li><strong>Context, How do I know Marta: </strong>Since when, in what capacity and how closely you have known Marta. This is important - a reference from a thesis supervisor who has worked with the student for a year is more informative than a reference from someone who only met them in one exam. If you&apos;ve done a project together, include details on how many times you&apos;ve met, etc. What was the project about, how challenging was it, what was the student&apos;s contribution.</li><li><strong>Marta&apos;s academic results/performance, in context:</strong> How good is Marta, compared to other students/persons in a similar context? Be aware that whoever is reading your letter may not know your country&apos;s marking scheme, so something like a GPA of 4.8 out of 5 isn&apos;t all that informative. Try to put that in context as much as you can: how many other students would achieve similar results in your institution? Best if you can give a rank index (#8 out of a cohort of 300) relative to the whole cohort<em>. Context on your institution:</em> Similarly, assume the reader has no idea how selective your institution is, so include a few details like &apos;top/most selective computer science program in the country&apos; or something.<em> Try to put this in context by making a prediction</em> about how well the student will do in the course you&apos;re recommending them to, or how well they would have done in a more challenging program. Do your research here, if you can.</li><li><strong>Details of research/project, if applicable:</strong> If you&apos;re recommending someone who has worked on a research project with you, include enough technical information (ideally with references or pointers) so that the reader can judge how serious that project was, and what Marta&apos;s contribution was. Don&apos;t worry, nobody is going to steal your research idea if you write it down in a recommendation letter - we are way too busy reading reference letters to do any research :D</li><li><strong>Marta&apos;s specific strengths:</strong> What quality of Marta do you think will be first noticed in an interview? Is Marta particularly good at understanding complex ideas fast? Is Marta very good at getting things done? Or writing clean code, mentoring others? Where appropriate, try to focus on talent and potential, before commenting on diligence or effort: If the first thing you write is &quot;Marta is very hard working&quot; it may be misinterpreted as a covert way of saying she tries very hard because she is not as good as the students who just get it without much effort. Be conscious of possible gender stereotypes that often come up here. E.g. she is quiet. <em>Make a prediction</em> about Marta&apos;s career prospects: She&apos;s on a good track to an academic career/well positioned for a career in industry. Please consider what the people reading your letter will want to see. If you recommend someone for an academic pure Maths program, you don&apos;t want to say the student is well positioned to end up in a boring finance job. If you feel like you MUST, you can include relative weaknesses here, but please phrase these as opportunities of growth, and what Marta needs to improve.</li><li><strong>Other/extracurricular activities:</strong> If you&apos;re aware of other things the student is doing - like organising meetups, volunteering, competitions, whatever - you can include them here if you feel they are relevant. Your job, again, is to put these in context.</li><li><strong>Further background on Marta&apos;s education history: </strong>This may be useful to support candidates who achieved impressive things in their country, but whose achievements may not make a lot of sense in an international context. For example, did they go to a very selective secondary school known for some specialization? Or, to the contrary, did they do exceptionally well despite not having access to the best education? Did they participate in country-specific olympiads or competitions? If so, what do those results mean? How many students do those things? Did they get a scholarship for their academic performance? If so, how many students get those? Did they participate in some kind of university activity? If so, what&apos;s the relevance of that? The most important assumption to remember is: Whoever reads the applicant&apos;s CV or your recommendation letter will know absolutely nothing about your country. You have to fill in the blanks, and explain everything from the ground up.<em> NO ACRONYMS!</em></li><li><strong>Your mini-CV: </strong>It&apos;s worth including one paragraph about yourself, the referee. What is your job title, how long you&apos;ve been doing what you&apos;re doing, what&apos;s your specialty, etc. The purpose of this is to prove you are qualified and able to spot talent. Make this as internationally attractive and meaningful as you can.</li><li><strong>Conclusion: </strong>Here is your chance to reiterate the strength of your recommendation. If you think you&apos;re describing a not-to-miss candidate, say so explicitly. One sentence we often include here is along the lines of &apos;If Marta were to apply for a PhD/Masters under my supervision I would not hesitate to take her as a student&apos;.</li></ul><h2 id=\"relative-ranking-of-students\">Relative ranking of students</h2><ul><li>Often, reference submission websites ask you to place the student in the top X% of students you&apos;ve worked with. More depends on this than you might think. Be honest, but be aware that these judgments often go into a formula for scoring or pre-filtering applications. In a competitive program, if you say someone is top 20%, that is likely a death sentence for the student&apos;s chances of getting a scholarship. Again, don&apos;t lie, just make sure you don&apos;t put the student in a lower bucket than they really deserve to be in.</li></ul><h2 id=\"writing-style\">Writing style</h2><ul><li>Be aware of cultural differences in how we praise others and give direct feedback to/on colleagues. I often recommend the <a href=\"https://erinmeyer.com/books/the-culture-map/?ref=inference.vc\">Culture Map</a> book by Erin Meyer on this topic. Though individuals are individuals, by and large, those who socialise in the U.S. Academic system tend to write recommendation letters with a higher baseline level of enthusiasm. If you feel your letter is too positive, that may be appropriate compensation for these differences, so long as your letter is honest, of course.</li><li>Writing style and tone are the most difficult to get right if you haven&apos;t seen examples before. I suggest you write a draft a couple weeks before submitting a letter, and then return to it before submitting. Re-reading after a week often allows you to better notice where the letter isn&apos;t conveying what you wanted.</li><li>Ask for help! If you have a candidate you enthusiastically support, don&apos;t be afraid to ask for help writing the reference letter. Ideally, ask someone who is experienced, doesn&apos;t know the candidate, and who is not part of the decision making at the institution the student is applying.</li></ul><p>In summary, please take time to write strong recommendation letters for your best students. There may not be many students at your institution who apply to top programs, but those who do are likely the ones who really need and deserve your attention.</p>"
        }
      ]
    },
    {
      "title": "Causal inference 4: Causal Diagrams, Markov Factorization, Structural Equation Models",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Causal inference 4: Causal Diagrams, Markov Factorization, Structural Equation Models"
      },
      "summary": "<p>This post is written with my PhD student and now <strong>guest author <a href=\"https://twitter.com/rpatrik96?ref=inference.vc\">Patrik Reizinger</a></strong> and is part 4 of a series of posts on causal inference:</p><ul><li><a href=\"https://www.inference.vc/untitled\">Part 1: Intro to causal inference and do-calculus</a></li><li><a href=\"https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/\">Part 2: Illustrating Interventions with a Toy Example</a></li><li><a href=\"https://www.inference.vc/causal-inference-3-counterfactuals/\">Part 3: Counterfactuals</a></li><li>&#x27a1;&#xfe0f;&#xfe0f; <a href=\"https://www.inference.vc/causal-inference-4/www.inference.vc/causal-inference-4/\">Part 4:</a></li></ul>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>This post is written with my PhD student and now <strong>guest author <a href=\"https://twitter.com/rpatrik96?ref=inference.vc\">Patrik Reizinger</a></strong> and is part 4 of a series of posts on causal inference:</p><ul><li><a href=\"https://www.inference.vc/untitled\">Part 1: Intro to causal inference and do-calculus</a></li><li><a href=\"https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/\">Part 2: Illustrating Interventions with a Toy Example</a></li><li><a href=\"https://www.inference.vc/causal-inference-3-counterfactuals/\">Part 3: Counterfactuals</a></li><li>&#x27a1;&#xfe0f;&#xfe0f; <a href=\"https://www.inference.vc/causal-inference-4/www.inference.vc/causal-inference-4/\">Part 4:</a></li></ul>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/causal-inference-4/"
        }
      ],
      "link": "https://www.inference.vc/causal-inference-4/",
      "id": "607571006b8e12003bfa0782",
      "guidislink": false,
      "tags": [
        {
          "term": "causal inference",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Patrik Reizinger"
        }
      ],
      "author": "Patrik Reizinger",
      "author_detail": {
        "name": "Patrik Reizinger"
      },
      "published": "Thu, 10 Jun 2021 13:56:20 GMT",
      "published_parsed": [
        2021,
        6,
        10,
        13,
        56,
        20,
        3,
        161,
        0
      ],
      "media_content": [
        {
          "url": "https://www.inference.vc/content/images/2021/06/many_to_one-1.png",
          "medium": "image"
        }
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<img alt=\"Causal inference 4: Causal Diagrams, Markov Factorization, Structural Equation Models\" src=\"https://www.inference.vc/content/images/2021/06/many_to_one-1.png\" /><p>This post is written with my PhD student and now <strong>guest author <a href=\"https://twitter.com/rpatrik96?ref=inference.vc\">Patrik Reizinger</a></strong> and is part 4 of a series of posts on causal inference:</p><ul><li><a href=\"https://www.inference.vc/untitled\">Part 1: Intro to causal inference and do-calculus</a></li><li><a href=\"https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/\">Part 2: Illustrating Interventions with a Toy Example</a></li><li><a href=\"https://www.inference.vc/causal-inference-3-counterfactuals/\">Part 3: Counterfactuals</a></li><li>&#x27a1;&#xfe0f;&#xfe0f; <a href=\"https://www.inference.vc/causal-inference-4/www.inference.vc/causal-inference-4/\">Part 4: Causal Diagrams, Markov Factorization, Structural Equation Models</a></li></ul><p>One way to think about causal inference is that causal models require a more fine-grained models of the world compared to statistical models. Many causal models are equivalent to the same statistical model, yet support different causal inferences. This post elaborates on this point, and makes the relationship between causal and statistical models more precise.</p><!--kg-card-begin: markdown--><h1 id=\"markovfactorization\">Markov factorization</h1>\n<!--kg-card-end: markdown--><p>Do you remember those combinatorics problems from school where the question was how many ways exist to get from a start position to a target field on a chessboard? And you can only move one step right or one step down. If you remember, then I need to admit that we will not consider problems like that. But its (one possible) takeaway actually can help us to understand Markov factorizations.</p><!--kg-card-begin: markdown--><p>You know, it is totally indifferent how you traversed the chessboard, the result is the same. So we can say that - from the perspective of target position and the process of getting there - this is a many-to-one mapping. The same holds for random variables and causal generative models.</p>\n<p>If you have a bunch of random variables - let&apos;s call them $X_1, X_2, \\dots, X_n$ -, their joint distribution is $p \\left(X_1, X_2, \\dots, X_n \\right) $. If you invoke the <em>chain rule of probability,</em> you will have several options to express this joint as a product of factors:</p>\n<p>$$<br />\np \\left(X_1, X_2, \\dots, X_n \\right) = \\prod p(X_{\\pi_i}\\vert X_{\\pi_1}, \\ldots, X_{\\pi_{i-1}}),<br />\n$$</p>\n<p>where $\\pi_i$ is a permutation of indices. Since you can do this for any permutation $\\pi$, the mapping between such factorizations and the joint distribution they express is many-to-one. As you can see this in the image below. The different factorizations induce a different graph, but have the same joint distribution.</p>\n<!--kg-card-end: markdown--><figure class=\"kg-card kg-image-card\"><img alt=\"Causal inference 4: Causal Diagrams, Markov Factorization, Structural Equation Models\" class=\"kg-image\" height=\"388\" src=\"https://www.inference.vc/content/images/2021/04/many_to_one-1.png\" width=\"775\" /></figure><p>Since you are reading this post, you may already be aware that in causal inference we often talk about a causal factorization, which looks like</p><p>$$<br />p \\left(X_1, X_2, \\dots, X_n \\right) = \\prod_{i=1}^{n} p\\left(X_i | X_{\\mathrm{pa}(i)}\\right),<br />$$</p><p>where $\\mathrm{pa}(X_i)$ denotes the causal parents of node $X_i$. This is one of many possible ways you can factorize the joint distribution, but we consider this one special. In the recent work, <a href=\"https://arxiv.org/abs/2102.11107?ref=inference.vc\">Sch&#xf6;lkopf et al.</a> call it a <em>disentangled</em> model. What are disentangled models? Disentangled factors describe independent aspects of the mechanism that generated the data. And they are not independent because you factored them in this way, but you were looking for this factorization because its factors are independent.</p><p>In other words, for every joint distribution there are many possible factorizations, but we assume that only one, the causal or disentangled factorization, describes the true underlying process that generated the data.</p><p>Let&apos;s consider an example for <em>disentangled</em> models. We want to model the joint distribution of altitude $A$ and temperature $T$. In this case, the causal direction is $A \\rightarrow T$ - if the altitude changes, the distribution of the temperature will change too. But you cannot change the altitude by artificially heating a city - otherwise we all would enjoy views as in Miami; global warming is real but fortunately has no altitude-changing effect.<br />In the end, we get the factorization of $p(A)p(T|A)$. The important insights here are the answers to the question: What do we expect from these factors? The previously-mentioned Sch&#xf6;lkopf et al. paper calls the main takeaway the <strong>Independent Causal Mechanisms (ICM) Principle</strong>, i.e.</p><blockquote>By conditioning on the parents of any factor in the disentangled model, the factor will neither be able to give you further information about other factors nor is able to influence them.</blockquote><p>In the above example, this means that if you consider different countries with their altitude distributions, you can still use the same $p(T|A),$ i.e., the factors <strong>generalize</strong> well. For <em>no influence</em>, the example holds straight above the ICM Principle. Furthermore, knowing any of the factors - e.g. $p(A)$ - won&apos;t tell anything about the other <em>(no information)</em>. If you know which country you are in, so will have no clue about the climate (if you consulted the website of the corresponding weather agency, that&apos;s what I call cheating). In the other direction, despite being the top-of-class student in climate matters, you won&apos;t be able to tell the country if somebody says to you that here the altitude is 350 meters and the temperature is 7&#xb0;C!</p><!--kg-card-begin: markdown--><h2 id=\"statisticalvscausalinference\">Statistical vs causal inference</h2>\n<p>We discussed Markov factorizations, as they help us understand the philosophical difference between statistical and causal inference. The beauty, and a source of confusion, is that one can use Markov factorizations in both paradigms.</p>\n<blockquote>\n<p>However, while using Markov factorizations is optional for statistical inference, it is a must for causal inference.</p>\n</blockquote>\n<p>So why would a statistical inference person use Markov factorizations? Because they make life easier in the sense that you do not need to worry about too high electricty costs. Namely, factorized models of data can be computationally much more efficient. Instead of modeling a joint distribution directly, which has a lot of parameters - in the case of $n$ binary variables, that is $2^n-1$ different values -, a factorized version can be pretty lightweight and parameter-efficient. If you are able to factorize the joint in a way that you have 8 factors with $n/8$ variables each, then you can describe your model with $8\\times2^{n/8}-1$ parameters. If $n=16$, that is $65,535$ vs $31$. Similarly, representing your distibution in a factorized form gives rise to efficient, general-purpose message-passing algorithms, such as belief propagation or expectation propagation.</p>\n<p>On the other hand, causal inference people really need this, otherwise, they are lost. Because without Markov factorizations, they cannot really formulate causal claims.</p>\n<blockquote>\n<p>A causal practicioner uses Markov factorizations, because this way she is able to reason about interventions.</p>\n</blockquote>\n<p>If you do not have the disentangled factorization, you cannot model the effect of interventions <em>on the real mechanisms</em> that make the system tick.</p>\n<h3 id=\"connectiontodomainadaptation\">Connection to domain adaptation</h3>\n<p>In plain machine learning lingo, what you want to do is <em>domain adaptation,</em> that is, you want to draw conclusions about a distribution you did not observe (these are the interventional ones). The Markov factorization prescribes ways in which you expect the distribution to change - one factor at a time - and thus the set of distributions you want to be able to robustly generalise to or draw inferences about.</p>\n<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><h2 id=\"docalculus\">Do calculus</h2>\n<p>Do-caclculus, the topic of the <a href=\"http://inference.vc/untitled?ref=inference.vc\">first post</a> in the series, can be relatively simply described using Markov factorizations. As you remember, $\\mathrm{do}(X=x)$ means that we set the variable $X$ to the value $x$, meaning that the distribution of that variable $p(X)$ collapses to a point mass. We can model this intervention mathematically by replacing the factor $p( x \\vert \\mathrm{pa}(X))$ by a Dirac-delta $\\delta_x$, resulting in the deletion of all incoming edges of the intervened factors in the graphical model. We then marginalise over $x$ to calculate the joint distribution of the remaining variables. For example, if we have two variables $x$ and $y$ we can write:</p>\n<p>$$<br />\np(y\\vert do(X=x_0)) = \\int p(x,y) \\frac{\\delta(x - x_0)}{p(x\\vert y)} dx<br />\n$$</p>\n<h2 id=\"semsmarkovfactorizationandthereparamtrizationtrick\">SEMs, Markov factorization, and the reparamtrization trick</h2>\n<p>If you&apos;ve read the previous parts in this series, you&apos;ll know that Markov factorizations aren&apos;t the only tool we use in causal inference. For counterfactuals, we used structural equation models (SEMs). In this part we will illustrate the connection between these with a cheesy reference to the reparametrization trick used in VAEs among others.</p>\n<p>But before that, let&apos;s recap SEMs. In this case, you define the relationship between the child node and its parents via a functional assignment. For node $X$ with parents $\\mathrm{pa}(X)$ it has the form of</p>\n<p>$$<br />\nX = f(\\mathrm{pa}(X), \\epsilon),<br />\n$$</p>\n<p>with some noise $\\epsilon.$ Here, you should read &quot;=&quot; in the sense of an assigment (like in Python), in mathematics, this should be &quot;:=&quot;.<br />\nThe above equation expresses the conditional probability $ p\\left(X| \\mathrm{pa}(X)\\right)$ as a <em>deterministic</em> function of $X$ and some noise variable $\\epsilon$. Wait a second..., isn&apos;t it the same thing what the reparametrization trick does? Yes it is.</p>\n<blockquote>\n<p>So the SEM formulation (called the <em>implicit distribution</em>) is related via the reparametrization trick to the conditional probability of $X$ given its parents.</p>\n</blockquote>\n<h3 id=\"classesofcausalmodels\">Classes of causal models</h3>\n<p>Thus, we can say that a SEM is a conditional distribution, and vica versa. Okay, but how do the sets of these constructs relate to each other?<br />\nIf you have a SEM, then you can read off the conditional, which is <strong>unique</strong>. On the other hand, you can find more SEMs for the same conditional.  Just as you can express a conditional distribution in multiple different ways using different reparametrizations, it is possible to express the same Markov factorization by multiple SEMs. Consider for example that if your distribution is $\\mathcal{N}(\\mu,\\sigma),$ then multiplying it by -1 gives you the same distribution. In this sense, SEMs are a richer class of models than Markov factorizations, thus they allow us to make inferences (counterfactual) which we weren&apos;t able to express in the more coarse grained language of Markov Factorizations.</p>\n<p>As we discussed above, a single joint distribution has multiple valid Markov factorizations, and the same Markov factorization can be expressed as different SEMs. We can think of joint distributions, Markov factorizations, and SEMs as increasingly fine-grained model classes: joint distributions $\\subset$ Markov facorizations $\\subset$ SEMs. The more aspects of the data generating process you model, the more elaborate the set of inferences you can make become. Thus, Joint distributions allow you to make predictions under no mechanism shift, Markov factorizations allow you to model interventions, SEMs allow you to make counterfactual statements.</p>\n<p>The price you pay for more expressive models is that they also get generally much harder to estimate from data. In fact, some aspects of causal models are impossible to infer from i.i.d. observational data. Moreover, some counterfactual inferences are experimentally not verifiable.</p>\n<!--kg-card-end: markdown-->"
        }
      ]
    },
    {
      "title": "On Information Theoretic Bounds for SGD",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "On Information Theoretic Bounds for SGD"
      },
      "summary": "<p>Few days ago we had a talk by Gergely Neu, who presented his recent work:</p><ul><li>Gergely Neu <a href=\"https://arxiv.org/abs/2102.00931?ref=inference.vc\">Information-Theoretic Generalization Bounds for Stochastic Gradient Descent</a></li></ul><p>I&apos;m writing this post mostly to annoy him, by presenting this work using super hand-wavy intuitions and cartoon figures. If this isn&apos;t</p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>Few days ago we had a talk by Gergely Neu, who presented his recent work:</p><ul><li>Gergely Neu <a href=\"https://arxiv.org/abs/2102.00931?ref=inference.vc\">Information-Theoretic Generalization Bounds for Stochastic Gradient Descent</a></li></ul><p>I&apos;m writing this post mostly to annoy him, by presenting this work using super hand-wavy intuitions and cartoon figures. If this isn&apos;t</p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/on-information-theoretic-bounds-for-sgd/"
        }
      ],
      "link": "https://www.inference.vc/on-information-theoretic-bounds-for-sgd/",
      "id": "60817747e5f995003a1b5643",
      "guidislink": false,
      "tags": [
        {
          "term": "generalization",
          "scheme": null,
          "label": null
        },
        {
          "term": "information theory",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Fri, 23 Apr 2021 14:17:49 GMT",
      "published_parsed": [
        2021,
        4,
        23,
        14,
        17,
        49,
        4,
        113,
        0
      ],
      "media_content": [
        {
          "url": "https://www.inference.vc/content/images/2021/04/PNG-image-16ED63A8BA1E-1-6.png",
          "medium": "image"
        }
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<img alt=\"On Information Theoretic Bounds for SGD\" src=\"https://www.inference.vc/content/images/2021/04/PNG-image-16ED63A8BA1E-1-6.png\" /><p>Few days ago we had a talk by Gergely Neu, who presented his recent work:</p><ul><li>Gergely Neu <a href=\"https://arxiv.org/abs/2102.00931?ref=inference.vc\">Information-Theoretic Generalization Bounds for Stochastic Gradient Descent</a></li></ul><p>I&apos;m writing this post mostly to annoy him, by presenting this work using super hand-wavy intuitions and cartoon figures. If this isn&apos;t enough, I will even find a way to mention GANs in this context.</p><p>But truthfully, I&apos;m just excited because for once, there is a little bit of learning theory that I half-understand, at least at an intuitive level, thanks to its reliance on KL divergences and the mutual information.</p><h2 id=\"a-simple-guessing-game\">A simple &#xa0;guessing game</h2><p>Let&apos;s start this with a simple thought experiment to illustrate why and how mutual information may be useful in describing an algorithm&apos;s ability to generalize. Say we&apos;re given two datasets, $\\mathcal{D}_{train}$ and &#xa0;$\\mathcal{D}_{test}$, of the same size for simplicity. We play the following game: we both have access to $\\mathcal{D}_{train}$ and &#xa0;$\\mathcal{D}_{test}$, and we both know what learning algorithm, $\\operatorname{Alg}$ we&apos;re going to use.</p><p>Now I toss a coin &#xa0;and I keep the result (recorded as random variable $Y$) a secret. If it&apos;s heads, I run $\\operatorname{Alg}$ on the training set $\\mathcal{D}_{train}$. If it&apos;s tails, I run $\\operatorname{Alg}$ on the test data $\\mathcal{D}_{test}$ instead. I don&apos;t tell you which of these I did, I only reveal to you the final parameter value $W$. Can you guess, just by looking at $W$, whether I trained on training or test data?</p><p>If you cannot guess $Y$, that means that the algorithm returns the same random $W$ irrespective of whether you train it on training or test data. So the training and test losses become interchangeable. This implies that the algorithm will generalize very well (on average) and not overfit to the data it&apos;s trained on.</p><p>The mutual information, in this case between $W$ and $Y$ quantifies your theoretical ability to guess $Y$ from $W$. The higher this value is, the easier it is to tell which dataset the algorithm was trained on. If it&apos;s easy to reverse engineer my coin toss from parameters, it means that the algorithm&apos;s output is very sensitive to the input dataset it&apos;s trained on. And that likely implies poor generalization.</p><p>Note by: an algorithm generalizing well on average doesn&apos;t mean it works well on average. It just means that there won&apos;t be a large gap between the expected training and expected test error. Take for example an algorithm returns a randomly initialized neural network, without even touching the data. That algorithm generalizes extremely well on average: it does just as poorly on test data as it does on training data.</p><h2 id=\"illustrating-this-in-more-detail\">Illustrating this in more detail</h2><p>Below is an illustration of my thought experiment for SGD.</p><figure class=\"kg-card kg-image-card\"><img alt=\"On Information Theoretic Bounds for SGD\" class=\"kg-image\" height=\"1276\" src=\"https://www.inference.vc/content/images/2021/04/two_SGD.png\" width=\"2000\" /></figure><p>In the top row, I doodled the distribution of the parameter $W_t$ at various timesteps $t=0,1,2,\\ldots,T$ of SGD. We start the algorithm by initializing $W$ randomly from a Gaussian (left panel). Then, each stochastic gradient update changes the distribution of $W_t$ a bit compared to the distribution of $W_{t-1}$. How the shape of the distribution changes depends on the data we use in the SGD steps. In the top row, let&apos;s say I ran SGD on $\\mathcal{D}_{train}$ and in the bottom, I run it on $\\mathcal{D}_{test}$. The distibutions $p(W_t\\vert \\mathcal{D})$ I drew here describe where the SGD iterate is likely to be after $t$ steps of SGD started from random initialization. They are not to be confused with Bayesian posteriors, for example.</p><p>We know that running the algorithm on the test set would produce low test error. Therefore, sampling a weight vector $W$ from $p(W_T\\vert \\mathcal{D}_{test})$ would be great if we could do that. But in practice, we can&apos;t train on the test data, all we have the ability to sample from is $p(W_T\\vert \\mathcal{D}_{train})$. So what we&apos;d like for good generalization, is if $p(W_T\\vert \\mathcal{D}_{test})$ and $p(W_T\\vert \\mathcal{D}_{train})$ were as close as possible. The mutual information between $W_T$ and my coinflip $Y$ measures this closeness in terms of the Jensen-Shannon divergence:</p><p>$$<br />\\mathbb{I}[Y, W_T] = \\operatorname{JSD}[p(W_T\\vert \\mathcal{D}_{test})\\|p(W_T\\vert \\mathcal{D}_{train})]<br />$$</p><p>So, in summary, if we can guarantee that the final parameter an algorithm comes up with doesn&apos;t reveal too much information about what dataset it was trained on, we can hope that the algorithm has good generalization properties.</p><h2 id=\"mutual-inforrmation-based-generalization-bounds\">Mutual Inforrmation-based Generalization Bounds</h2><p>These vague intuitions can be formalized into real information-theoretic generalization bounds. These were first presented in a general context in (<a href=\"http://proceedings.mlr.press/v51/russo16.html?ref=inference.vc\">Russo and Zou, 2016</a>) and in a more clearly machine learning context in (<a href=\"https://proceedings.neurips.cc/paper/2017/hash/ad71c82b22f4f65b9398f76d8be4c615-Abstract.html?ref=inference.vc\">Xu and Raginsky, 2017</a>). I&apos;ll give a quick - and possibly somewhat handwavy - overview of the main results.</p><p>Let $\\mathcal{D}$ and $\\mathcal{D}&apos;$ be random datasets of size $n$, drawn i.i.d. from some underlying data distribution $P$. Let $W$ be a parameter vector, which we obtain by running a learning algorithm $\\operatorname{Alg}$ on the training data $\\mathcal{D}$: $W = \\operatorname{Alg}(\\mathcal{D})$. The algorithm may be non-deterministic, i.e. it may output a random $W$ given a dataset. Let $\\mathcal{L}(W, \\mathcal{D})$ denote the loss of model $W$ on dataset $\\mathcal{D}$. The expected generalization error of $\\operatorname{Alg}$ is defined as follows:</p><p>$$<br />\\text{gen}( \\operatorname{Alg}, P) = \\mathbb{E}_{\\mathcal{D}\\sim P^n,\\mathcal{D}&apos;\\sim P^n, W\\vert \\mathcal{D}\\sim \\operatorname{Alg}(\\mathcal{D})}[\\mathcal{L}(W, \\mathcal{D}&apos;) - \\mathcal{L}(W, \\mathcal{D})]<br />$$</p><p>If we unpack this, we have two datasets $\\mathcal{D}$ and $\\mathcal{D}&apos;$, the former taking the role of the training dataset, the latter of the test data. We look at the expected difference between the training and test losses ($\\mathcal{L}(W, \\mathcal{D})$ and $\\mathcal{L}(W, \\mathcal{D}&apos;)$), where $W$ is obtained by running $\\operatorname{Alg}$ on the training data $\\mathcal{D}$. The expectation is taken over all possible random training sets, test sets, and over all possible random outcomes of the learning algorithm.</p><p>The information theoretic bound states that for any learning algorithm, and any loss function that&apos;s bounded by $1$, the following inequality holds:</p><p>$$<br />gen(\\operatorname{Alg}, P) \\leq \\sqrt{\\frac{\\mathbb{I}[W, \\mathcal{D}]}{n}}<br />$$</p><p>The main term in the RHS of this bound is the mutual infomation between the training data \\mathcal{D} and the pararmeter vector $W$ the algorithm finds. It essentially quantifies the number of bits of information the algorithm leaks about the training data into the parameters it learns. The lower this number, the better the algorithm generalizes.</p><h2 id=\"why-we-can-t-apply-this-to-sgd\">Why we can&apos;t apply this to SGD?</h2><p>The problem with applying these nice, intuitive bounds to SGD is that SGD, in fact, leaks too much information about the specific minibatches it is trained on. Let&apos;s go back to my illustrative example of having to guess if we ran the algorithm on training or test data. Consider the scenario where we start form some parameter value $w_t$ and we update either with a random minibatch of training data (blue) or a random minibatch of test data (orange).</p><figure class=\"kg-card kg-image-card\"><img alt=\"On Information Theoretic Bounds for SGD\" class=\"kg-image\" height=\"881\" src=\"https://www.inference.vc/content/images/2021/04/parameter_updates.png\" width=\"1047\" /></figure><p>Since the training and test datasets are assumed to be of finite size, there are only a finite number of possible minibatches. Each of these minibatches can take the parameter to a unique new location. The problem is, the set of locations you can reach with one dataset (blue dots) does not overlap with the set of locations you can reach if you update with the other dataset (orange dots). Suddenly, if I give you $w_{t+1}$, you can immediately tell if it&apos;s an orange or blue dot, so you can immediately reconstruct my coinflip $Y$.</p><p>In the more general case, the problem with SGD in the context of information-theoretic bounds is that the amount of information SGD leaks about the dataset it was trained on is high, and in some cases may even be infinite. This is actually related to the problem that several of us noticed in the context of GANs, where the true and fake distributions may have non-overlapping support, making the KL divergence infinite, and saturating out the Jensen-Shannon divergence. The first trick we came up with to solve this problem was to smooth things out by <a href=\"https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/\">adding Gaussian noise</a>. Indeed, adding noise is key what researches have been doing to apply these information-theoretic bounds to SGD.</p><h3 id=\"adding-noise-to-sgd\">Adding noise to SGD</h3><p>The first thing people did (<a href=\"https://arxiv.org/abs/1801.04295?ref=inference.vc\">Pensia et al, 2018</a>) is to study a noisy cousin of SGD: stochastic gradient Langevin dynamics (SGLD). SGDL is like SGD but in each iteration we add a bit of Gaussian noise to the parameters in addition to the gradient update. To understand why SGLD leaks less information, consider the previous example with the orange and blue point clouds. SGLD makes those point clouds overlap by convolving them with Gaussian noise.</p><p>However, SGLD is not exactly SGD, and it&apos;s not really used as much in practice. In order to say something about SGD specifically, <a href=\"https://arxiv.org/abs/2102.00931?ref=inference.vc\">Neu (2021)</a> did something else, while still relying on the idea of adding noise. Instead of baking the noise in as part of the algorithm, Neu only adds noise as part of the analysis. The algorithm being analysed is still SGD, but when we measure the mutual information we will measure the mutual information between $\\mathbb{I}[W + \\xi; \\mathcal{D}]$, where $\\xi$ is Gaussian noise. </p><p>I leave it to you to check out the details of the paper. While the findings fall short of explaining whether SGD have any tendency to find solutions that generalise well, some of the results are nice and interpretable: they connect the generalization of SGD to the noisiness of gradients as well as the smoothness of the loss along the specific optimization path that was taken.</p>"
        }
      ]
    },
    {
      "title": "Notes on the Origin of Implicit Regularization in SGD",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Notes on the Origin of Implicit Regularization in SGD"
      },
      "summary": "<p>I wanted to highlight an intriguing paper I presented at a journal club recently:</p><ul><li>Samuel L Smith, Benoit Dherin, David Barrett, Soham De (2021) <a href=\"https://openreview.net/forum?id=rq_Qr0c1Hyo&amp;ref=inference.vc\">On the Origin of Implicit Regularization in Stochastic Gradient Descent</a></li></ul><p>There&apos;s actually a related paper that came out simultaneously, studying full-batch gradient descent instead</p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>I wanted to highlight an intriguing paper I presented at a journal club recently:</p><ul><li>Samuel L Smith, Benoit Dherin, David Barrett, Soham De (2021) <a href=\"https://openreview.net/forum?id=rq_Qr0c1Hyo&amp;ref=inference.vc\">On the Origin of Implicit Regularization in Stochastic Gradient Descent</a></li></ul><p>There&apos;s actually a related paper that came out simultaneously, studying full-batch gradient descent instead</p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/notes-on-the-origin-of-implicit-regularization-in-stochastic-gradient-descent/"
        }
      ],
      "link": "https://www.inference.vc/notes-on-the-origin-of-implicit-regularization-in-stochastic-gradient-descent/",
      "id": "6065ae116b8e12003bfa063b",
      "guidislink": false,
      "tags": [
        {
          "term": "deep learning",
          "scheme": null,
          "label": null
        },
        {
          "term": "generalization",
          "scheme": null,
          "label": null
        },
        {
          "term": "SGD",
          "scheme": null,
          "label": null
        },
        {
          "term": "differerntial equations",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Thu, 01 Apr 2021 14:23:24 GMT",
      "published_parsed": [
        2021,
        4,
        1,
        14,
        23,
        24,
        3,
        91,
        0
      ],
      "media_content": [
        {
          "url": "https://www.inference.vc/content/images/2021/04/xRYHk0m-1.png",
          "medium": "image"
        }
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<img alt=\"Notes on the Origin of Implicit Regularization in SGD\" src=\"https://www.inference.vc/content/images/2021/04/xRYHk0m-1.png\" /><p>I wanted to highlight an intriguing paper I presented at a journal club recently:</p><ul><li>Samuel L Smith, Benoit Dherin, David Barrett, Soham De (2021) <a href=\"https://openreview.net/forum?id=rq_Qr0c1Hyo&amp;ref=inference.vc\">On the Origin of Implicit Regularization in Stochastic Gradient Descent</a></li></ul><p>There&apos;s actually a related paper that came out simultaneously, studying full-batch gradient descent instead of SGD:</p><ul><li>David G.T. Barrett, Benoit Dherin (2021) <a href=\"https://arxiv.org/abs/2009.11162?ref=inference.vc\">Implicit Gradient Regularization</a></li></ul><p>One of the most important insights in machine learning over the past few years relates to the importance of optimization algorithms in generalization performance.</p><h2 id=\"why-deep-learning-works-at-all\">Why deep learning works at all</h2><p>In order to understand why deep learning works as well as it does, it is insufficient to reason about the loss function or the model class, which is what classical generalisation theory focussed on. Instead, the algorithms we use to find minima (namely, stochastic gradient descent) seem to play an important role. In many tasks, powerful neural networks are able to interpolate training data, i.e. achieve near-0 training loss. There are in fact several minima of the training loss which are virtually indistinguishably good on the training data. Some of these minima generalise well (i.e. result in low test error), others can be arbitrarily badly overfit.</p><p>What seems to be important then is not whether the optimization algorithm converges quickly to a local minimum, but which of the available &quot;virtually global&quot; minima it prefers to reach. It seems to be the case that the optimization algorithms we use to train deep neural networks <em>prefer</em> some minima over others, and that this preference results in better generalisation performance. The preference of optimization algorithms to converge to certain minima while avoiding others is described as <em>implicit regularization</em>.</p><p>I wrote <a href=\"https://hackmd.io/75gt3X6WQbu1_A3pF8svWg?ref=inference.vc\">this note</a> as an overview on how we/I currently think about why deep networks generalize.</p><h2 id=\"analysing-the-effect-of-finite-stepsize\">Analysing the effect of finite stepsize</h2><p>One of the interesting new theories that helped me imagine what happens in deep learning training is that of <a href=\"https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/\">neural tangent kernels</a>. In this framework we study neural network training in the limit of infinitely wide layers, full-batch training and infinitesimally small learning rate, i.e. when gradient becomes continuous gradient flow, described by an ordinary differential equation. Although the theory is useful and appealing, full-batch training with infinitesimally small learning rates is very much a cartoon version of what we actually do in practice. In practice, the smallest learning date doesn&apos;t always work best. Secondly, the stochasticity of gradient updates in minibatch-SGD seems to be of importance as well.</p><p>What <a href=\"https://openreview.net/forum?id=rq_Qr0c1Hyo&amp;ref=inference.vc\">Smith et al (2021)</a> do differently in this paper is they try to study minibatch-SGD, for small, but not infinitesimally small, learning rates. This is much closer to practice. The toolkit that allows them to study this scenario is borrowed from the study of differential equations and is called backward error analysis. The cartoon illustration below shows what backward error analysis tries to achieve:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Notes on the Origin of Implicit Regularization in SGD\" class=\"kg-image\" height=\"482\" src=\"https://www.inference.vc/content/images/2021/04/Backward-error-analysis.png\" width=\"531\" /></figure><p>Let&apos;s say we have a differential equation $\\dot{\\omega} = f(\\omega)$. The solution to this ODE with initial condition $\\omega_0$ is a continuous trajectory $\\omega_t$, shown in the image in black. We usually can&apos;t compute this solution in closed form, and instead simulate the ODE using the Euler&apos;s method, $\\omega_{k+1} = \\omega_k + \\epsilon f(\\omega_k)$. This results in a discrete trajectory shown in teal. Due to discretization error, for finite stepsize $\\epsilon$, this discrete path may not lie exactly where the continuous black path lies. Errors accumulate over time, as shown in this illustration. The goal of backward error analysis is to find a different ODE, $\\dot{\\omega} = \\tilde{f}(\\omega)$ such that the approximate discrete path we got from Euler&apos;s method lieas near the the continuous path which solves this new ODE. Our goal is to reverse engineer a modified $\\tilde{f}$ such that the discrete iteration can be well-modelled by an ODE.</p><p>Why is this useful? Because the form $\\tilde{f}$ takes can reveal interesting aspects of the behaviour of the discrete algorithm, particularly if it has any implicit bias towards moving into different areas of the space. When the authors apply this technique to (full-batch) gradient descent, it already suggests the kind of implicit regularization bias gradient descent has.</p><p>In Gradient descent with a cost function $C$, the original ODE is $f(\\omega) = -\\nabla C (\\omega)$. The modified ODE which corresponds to a finite stepsize $\\epsilon$ takes the form $\\dot{\\omega} = -\\nabla\\tilde{C}_{GD}(\\omega)$ where</p><p>$$<br />\\tilde{C}_{GD}(\\omega) = C(\\omega) + \\frac{\\epsilon}{4} \\|\\nabla C(\\omega)\\|^2<br />$$</p><p>So, gradient descent with finite stepsize $\\epsilon$ is like running gradient flow, but with an added penalty that penalises the gradients of the loss function. The second term is what <a href=\"https://arxiv.org/pdf/2009.11162.pdf?ref=inference.vc\">Barret and Dherin (2021)</a> call &#xa0;implicit gradient regularization.</p><h2 id=\"stochastic-gradients\">Stochastic Gradients</h2><p>Analysing SGD in this framework is a bit more difficult because the trajectory in stochastic gradient descent is, well, stochastic. Therefore, you don&apos;t have have a single discrete trajectory to optimize, but instead you have a distribution of different trajectories which you&apos;d traverse if you randomly reshuffle your data. Here&apos;s a picture illustrating this situation:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Notes on the Origin of Implicit Regularization in SGD\" class=\"kg-image\" height=\"482\" src=\"https://www.inference.vc/content/images/2021/04/Backward-error-analysis--1-.png\" width=\"531\" /></figure><p>Starting from the initial point $\\omega_0$ we now have multiple trajectories. These correspond to different ways we can shuffle data (in the paper we assume we have a fixed allocation of datapoints to minibatches, and the randomness comes from the order in which the minibatches are considered). The two teal trajectories illustrate two potential paths. The paths end up at a random location, the teal dots show additional random endpoints where trajectories may end up at. The teal star shows the mean of the distribution of random trajectory endpoints.</p><p>The goal in (Smith et al, 2021) is to reverse-engineer an ODE so that the continuous (orange) path lies close to this mean location. The corresponding ODE is of the form $\\dot{omega} = -\\nabla C_{SGD}(\\omega)$, where</p><p>$$<br />\\tilde{C}_{SGD}(\\omega) = C(\\omega) + \\frac{\\epsilon}{4m} \\sum_{k=1}^{m} \\|\\nabla \\hat{C}_k(\\omega)\\|^2,<br />$$</p><p>where $\\hat{C}_k$ is the loss function on the $k^{th}$ minibatch. There are $m$ minibatches in total. Note that this is similar to what we had for gradient descent, but instead of the norm of the full-batch gradient we now have the average norm of minibatch gradients as the implicit regularizer. Another interesting view on this is to look at the difference between the GD and SGD regularizers:</p><p>$$<br />\\tilde{C}_{SGD} = \\tilde{C}_{GD} + \\frac{\\epsilon}{4m} \\sum_{k=1}^{m} \\|\\nabla \\hat{C}_k(\\omega) - C(\\omega)\\|^2<br />$$</p><p>This additional regularization term, $\\frac{1}{m}\\sum_{k=1}^{m} \\|\\nabla \\hat{C}_k(\\omega) - C(\\omega)\\|^2$, is something like the total variance of minibatch gradients (the trace of the empirical Fisher information matrix). Intuitively, this regularizer term will avoid parts of the parameter-space where the variance of gradients calculated over different minibatches is high.</p><p>Importantly, while $C_{GD}$ has the same minima as $C$, this is no longer true for $C_{SGD}$. Some minima of $C$ where the variance of gradients is high, is no longer a minimum of $C_{SGD}$. As an implication, not only does SGD follow different trajectories than full-batch GD, it may also converge to completely different solutions.</p><p>As a sidenote, there are many versions of SGD, based on how data is sampled for the gradient updates. Here, it is assumed that the datapoints are assigned to minibatches, but then the minibatches are randomly sampled. This is different from randomly sampling datapoints with replacement from the training data (<a href=\"https://arxiv.org/abs/1511.06251?ref=inference.vc\">Li et al (2015)</a> consider that case), and indeed an analysis of that variant may well lead to different results.</p><h2 id=\"connection-to-generalization\">Connection to generalization</h2><p>Why would an implicit regularization effect avoiding high minibatch gradient variance be useful for generalisation? Well, let&apos;s consider a cartoon illustration of two local minima below:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Notes on the Origin of Implicit Regularization in SGD\" class=\"kg-image\" height=\"370\" src=\"https://www.inference.vc/content/images/2021/04/xRYHk0m.png\" width=\"926\" /></figure><p>Both minima are the same as much as the average loss $C$ is concerned: the value of the minimum is the same, and the width of the two minima are the same. Yet, in the left-hand situation, the wide minimum arises as the average of several minibatch losses, which all look the same, and which all are relatively wide themselves. In the right-hand minimum, the wide average loss minimum arises as the average of a lot of sharp minibatch losses, which all disagree on where exactly the location of the minimum is.</p><p>If we have these two options, it is reasonable to expect the left-hand minimum to generalise better, because the loss function seems to be less sensitive to whichever specific minibatch we are evaluating it on. As a consequence, the loss function also may be less sensitive to whether a datapoint is in the training set or in the test set.</p><h2 id=\"summary\">Summary</h2><p>In summary, this paper is a very interesting analysis of stochastic gradient descent. While it has its limitations (which the authors don&apos;t try to hide and discuss transparently in the paper), it nevertheless contributes a very interesting new technique for analysing optimization algorithms with finite stepsize. I found the paper to be well-written, with the explanation of somewhat tedious details of the analysis clearly laid out. But perhaps I liked this paper most because it confirmed my intuitions about why SGD works, and what type of minima it tends to prefer.</p>"
        }
      ]
    },
    {
      "title": "An information maximization view on the $\\beta$-VAE objective",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "An information maximization view on the $\\beta$-VAE objective"
      },
      "summary": "<h6 id=\"guest-post-with-d-ra-j-mbor\">guest post with <a href=\"http://dorajambor.com/?ref=inference.vc\">D&#xf3;ra J&#xe1;mbor</a></h6><p><br />This is a half-guest-post written jointly with D&#xf3;ra, a fellow participant in a reading group where we recently discussed the original paper on $\\beta$-VAEs:</p><ul><li>Irina Higgins et al (ICLR 2017): <a href=\"http://www.matthey.me/publication/beta-vae/?ref=inference.vc\">$\\beta$-VAE: Learning Basic Visual Concepts with a</a></li></ul>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<h6 id=\"guest-post-with-d-ra-j-mbor\">guest post with <a href=\"http://dorajambor.com/?ref=inference.vc\">D&#xf3;ra J&#xe1;mbor</a></h6><p><br />This is a half-guest-post written jointly with D&#xf3;ra, a fellow participant in a reading group where we recently discussed the original paper on $\\beta$-VAEs:</p><ul><li>Irina Higgins et al (ICLR 2017): <a href=\"http://www.matthey.me/publication/beta-vae/?ref=inference.vc\">$\\beta$-VAE: Learning Basic Visual Concepts with a</a></li></ul>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/beta-vae/"
        }
      ],
      "link": "https://www.inference.vc/beta-vae/",
      "id": "60493ffd17b186003957870d",
      "guidislink": false,
      "tags": [
        {
          "term": "VAE",
          "scheme": null,
          "label": null
        },
        {
          "term": "generative models",
          "scheme": null,
          "label": null
        },
        {
          "term": "deep learning",
          "scheme": null,
          "label": null
        },
        {
          "term": "KL divergence",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dora Jambor"
        }
      ],
      "author": "Dora Jambor",
      "author_detail": {
        "name": "Dora Jambor"
      },
      "published": "Thu, 18 Mar 2021 15:18:12 GMT",
      "published_parsed": [
        2021,
        3,
        18,
        15,
        18,
        12,
        3,
        77,
        0
      ],
      "media_content": [
        {
          "url": "https://www.inference.vc/content/images/2021/03/Screenshot-2021-03-18-at-15.17.10.png",
          "medium": "image"
        }
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<h6 id=\"guest-post-with-d-ra-j-mbor\">guest post with <a href=\"http://dorajambor.com/?ref=inference.vc\">D&#xf3;ra J&#xe1;mbor</a></h6><img alt=\"An information maximization view on the $\\beta$-VAE objective\" src=\"https://www.inference.vc/content/images/2021/03/Screenshot-2021-03-18-at-15.17.10.png\" /><p><br />This is a half-guest-post written jointly with D&#xf3;ra, a fellow participant in a reading group where we recently discussed the original paper on $\\beta$-VAEs:</p><ul><li>Irina Higgins et al (ICLR 2017): <a href=\"http://www.matthey.me/publication/beta-vae/?ref=inference.vc\">$\\beta$-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</a></li></ul><p>On the surface of it, $\\beta$-VAEs are a straightforward extension of VAEs where we are allowed to directly control the tradeoff between the reconstruction and KL loss terms. In an attempt to better understand where the $\\beta$-VAE objective comes from, and to further motivate why it makes sense, here we derive &#xa0;$\\beta$-VAEs from different first principles than it is presented in the paper. Over to mostly D&#xf3;ra for the rest of this post:</p><h3 id=\"first-some-notation-\">First, some notation:</h3><ul><li>$p_\\mathcal{D}(\\mathbf{x})$: data distribution</li><li>$q_\\psi(\\mathbf{z}\\vert \\mathbf{x})$: representation distribution</li><li>$q_\\psi(\\mathbf{z}) = \\int p_\\mathcal{D}(\\mathbf{x})q_\\psi(\\mathbf{z}\\vert \\mathbf{x})$: aggregate posterior - marginal distribution of representation $Z$</li><li>$q_\\psi(\\mathbf{x}\\vert \\mathbf{z}) = \\frac{q_\\psi(\\mathbf{z}\\vert \\mathbf{x})p_\\mathcal{D}(\\mathbf{x})}{q_\\psi(\\mathbf{z})}$: &quot;inverted posterior&quot;</li></ul><h3 id=\"motivation-and-assumptions-of-beta-vaes\">Motivation and assumptions of $\\beta$-VAEs</h3><p>Learning disentangled representations that recover the independent data generative factors has been a long-term goal for unsupervised representation learning.</p><p>$\\beta$-VAEs were introduced in 2017 with a proposed modification to the original VAE formulation that can achieve better disentanglement in the posterior $q_\\psi(\\mathbf{z}\\vert \\mathbf{x})$. An assumption of $\\beta$-VAEs is that there are two sets of latent factors, $\\mathbf{v}$ and $\\mathbf{w}$, that contribute to generating observations $x$ in the real world. One set, $\\mathbf{v}$, is coordinate-wise conditionally independent given the observed variable, i.e., $\\log p(\\mathbf{v}\\vert x) = \\sum_k \\log p(v_k\\vert \\mathbf{x})$. At the same time, we don&apos;t assume anything about the remaining factors $\\mathbf{w}$.</p><p>The factors $v$ are going to be the main object of interest for us. The conditional independence assumption allows us to formulate what it means to disentangle these factors of variation. Consider a representation $\\mathbb{z}$ which <em>entangles<strong> </strong></em> coordinates of $v$, in that each coordinate of $\\mathbb{z}$ depends on multiple coordinates of $\\mathbb{v}$, e.g. $z_1 = f_1(v_1, v_2)$ and $z_2 = f_2(v_1, v_2)$. Such a $\\mathbb{z}$ won&apos;t necessarily satisfy co-ordinatewise conditional independence $\\log p(\\mathbf{z}\\vert x) = \\sum_k \\log p(z_k\\vert \\mathbf{x})$. However, if each component of $\\mathbb{z}$ depended only on one corresponding coordinate of $\\mathbf{v}$, for example $z_1 = g_1(v_1)$ and $z_2 = g_2(v_2)$, the component-wise conditional independence would hold for $\\mathbb{z}$ too.</p><p>Thus, under these assumptions we can encourage disentanglement to happen by encouraging the posterior $q_\\psi(\\mathbf{z}\\vert \\mathbf{x})$ to be coordinate-wise conditionally independent. This can be done by adding a new hyperparameter $\\beta$ to the original VAE formulation</p><p>$$<br />\\mathcal{L}(\\theta, \\phi; x, z, \\beta) = -\\mathbb{E}_{q_{\\phi}(x\\vert z)p_\\mathcal{D}(x)}[\\log p_{\\theta}(x \\vert z)] + \\beta \\operatorname{KL} (q_{\\phi(z\\vert x)}\\| p(z)),<br />$$</p><p><br />where $\\beta$ controls the trade-off between the capacity of the latent information channel and learning conditionally independent latent factors. When $\\beta$ is higher than 1, we encourage the posterior $q_\\psi(z\\vert x)$ to be close to the isotropic unit Gaussian $p(z) = \\mathcal{N}(0, I)$, which itself is coordnate-wise independent.</p><h3 id=\"marginal-versus-conditional-independence\">Marginal versus Conditional Independence</h3><p>In this post, we revisit the conditional independence assumption of latent factors, and argue that a more appropriate objective would be to have marginal independence in the latent factors. To show you our intuition, let&apos;s revisit the &quot;Explaining Away&quot; phenomenon from Probabilistic Graphical Models.</p><h4 id=\"explaining-away\">Explaining away</h4><p>Consider three random variables:<br />$A$: Ferenc is grading exams<br />$B$: Ferenc is in a good mood<br />$C$: Ferenc is tweeting a meme</p><p>with the following graphical model $A \\rightarrow C \\leftarrow B$.</p><p>Here we could assume that Ferenc grading exams is independent of him being in a good mood, i.e., $A \\perp B$. However, the pressure of marking exams results in increased likelihood of procrastination, which increases the chances of tweeting memes, too.</p><p>However, as soon as we see a meme being tweeted by him, we know that he either in a good mood or he is grading exams. If &#xa0;we know he is grading exams, that explains why he is tweeting memes, so it&apos;s less likely he&apos;s tweeting memes because he&apos;s a good mood. Consequently, $A \\not!\\perp B\\vert C$.</p><p>In all seriousness, if we have a graphical model $A \\rightarrow C \\leftarrow B$, in evidence of $C$, independence between $A$ and $B$ no longer holds.</p><h3 id=\"why-does-this-matter\">Why does this matter?</h3><p>We argue that the explaining away phenomenon makes the conditional independence of latent factors undesirable. A much more reasonable assumption about the generative process of the data is that the factors of variation $\\mathbf{v}$ are drawn independently, and then the observations are generated conditoned on them. However, if we consider two coordinates of $\\mathbb{v}$ and the observation $\\mathbf{x}$, we now have a $V_1 \\rightarrow \\mathbf{X} \\leftarrow V_2$ graphical model, thus, conditional independence cannot hold.</p><p></p><p>Instead, we argue that to recover the generative factors of the data, we should encourage latent factors to be marginally independent. In the next section, we set out to derive an algorithm that encourages marginal independence in the representation Z. We will also show how the resulting loss function from this new derivation is actually equivalent to the original $\\beta$-VAEs formulation.</p><h3 id=\"marginally-independent-latent-factors\">Marginally Independent Latent Factors</h3><p>We&apos;ll start from desired properties of the representation distribution $q_\\psi(z\\vert x)$. We&apos;d like this representation to satisfy two properties:</p><ol><li>Marginal independence: We would like the aggregate posterior $q_\\psi(z)$ to be close to some fixed and factorized unit Gaussian prior $p(z) = \\prod_i p(z_i)$. This encourages $q_\\psi(z)$ to exhibit coordinate-wise independence.</li><li>Maximum Information: We&apos;d like the representation $Z$ to retain as much information as possible about the input data $X$.</li></ol><p>Note that without (1), (2) is insufficient, because then any deterministic and invertible function of $X$ would contain maximum information about $X$ but that wouldn&apos;t make it a useful or disentangled representation. Similarly, without (2), (1) is insufficient because if we set $q_\\psi(z\\vert x) = p(z)$ it would give us a latent representation Z that is coordinate-wise independent, but it is also independent of the data which is not very useful.</p><h3 id=\"deriving-a-practical-objective\">Deriving a practical objective</h3><p>We can achieve a combination of these desiderata by optimizing an objective with the weighted combination of two terms corresponding to the two goals we set out above:</p><p>$$<br />\\mathcal{L}(\\psi) = \\operatorname{KL}[q_\\psi(z)| p(z)] - \\lambda \\mathbb{I}_{q_\\psi(z\\vert x) p_\\mathcal{D}(x)}[X, Z]<br />$$</p><p>Remember, we use $q_\\psi(z)$ to denote the aggregate posterior. We will refer to this as the InfoMax objective. Now we&apos;re going to show how this objective can be related to the $\\beta$-VAE objective. Let&apos;s first consider the KL term in the above objective:</p><!--kg-card-begin: markdown--><p>\\begin{align}<br />\n\\operatorname{KL}[q_\\psi(z)| p(z)] &amp;= \\mathbb{E}_{q_\\psi(z)} \\log \\frac{q_\\psi(z)}{p(z)}\\\\<br />\n&amp;= \\mathbb{E}_{q_\\psi(z\\vert x)p_\\mathcal{D}(x)} \\log \\frac{q_\\psi(z)}{p(z)}\\\\<br />\n&amp;= \\mathbb{E}_{q_\\psi(z\\vert x)p_\\mathcal{D}(x)} \\log \\frac{q_\\psi(z)}{q_\\psi(z\\vert x)} + \\mathbb{E}_{q_\\psi(z\\vert x)p_\\mathcal{D}(x)} \\log \\frac{q_\\psi(z\\vert x)}{p(z)}\\\\<br />\n&amp;= \\mathbb{E}_{q_\\psi(z\\vert x)p_\\mathcal{D}(x)} \\log \\frac{q_\\psi(z)p_\\mathcal{D}(x)}{q_\\psi(z\\vert x)p_\\mathcal{D}(x)} + \\mathbb{E}_{q_\\psi(z\\vert x)p_\\mathcal{D}(x)} \\log \\frac{q_\\psi(z\\vert x)}{p(z)}\\\\<br />\n&amp;= -\\mathbb{I}_{q_\\psi(z\\vert x)p_\\mathcal{D}(x)}[X,Z] + \\mathbb{E}_{p_\\mathcal{D}}\\operatorname{KL}[q_\\psi(z\\vert x)| p(z)]<br />\n\\end{align}</p>\n<!--kg-card-end: markdown--><p>This is interesting. If the mutual information between $X$ and $Z$ is non-zero (which is ideally the case), the above equation shows that latent factors cannot be both marginally and conditionally independent at the same time. It also gives us a way to relate &#xa0;the KL terms representing marginal and conditional independence.</p><p>Putting this back into the InfoMax objective, we have that</p><!--kg-card-begin: markdown--><p>\\begin{align}<br />\n\\mathcal{L}(\\psi) &amp;= \\operatorname{KL}[q_\\psi(z)| p(z)] - \\lambda \\mathbb{I}_{q_\\psi(z\\vert x)p_\\mathcal{D}(x)}[X, Z]\\\\<br />\n&amp;= \\mathbb{E}_{p_\\mathcal{D}}\\operatorname{KL}[q_\\psi(z\\vert x)| p(z)] - (\\lambda + 1) \\mathbb{I}_{q_\\psi(z\\vert x)p_\\mathcal{D}(x)}[X, Z]\\<br />\n\\end{align}</p>\n<!--kg-card-end: markdown--><p>Using the KL term in the InfoMax objective, we were able to recover the KL-divergence term that also appears in the $\\beta$-VAE (and consequently, VAE) objective.</p><p>At this point, we still haven&apos;t defined the generative model $p_\\theta(x\\vert z)$, the above objective expresses everything in terms of the data distribution $p_\\mathcal{D}$ and the posterior/representation distribution $q_\\psi$.</p><p>We will now focus on the 2nd term in our desired objective, the weighted mutual information, which we still can&apos;t easily evaluate. We will now show that we can recover the reconstruction term in $\\beta$-VAEs by doing a variational approximation to the mutual information.</p><h3 id=\"variational-bound-on-mutual-information\">Variational bound on mutual information</h3><p>Note the following equality:</p><p>\\begin{equation}<br />\\mathbb{I}[X,Z] = \\mathbb{H}[X] - \\mathbb{H}[X\\vert Z]<br />\\end{equation}</p><p>Since we sample X from the data distribution $p_\\mathcal{D}$, we see that the first term $\\mathbb{H}[X]$, the entropy of $X$, is constant with respect to the variational parameter $\\psi$. We are left to focus on finding a good approximation to the second term $\\mathbb{H}[X\\vert Z]$. We can do so by minimizing the KL divergence between $q_\\psi(x\\vert z)$ and an auxilliary distribution $p_\\theta(x\\vert z)$ to make a variational appoximation to the mutual information:</p><!--kg-card-begin: markdown--><p>$$\\mathbb{H}[X\\vert Z] = - \\mathbb{E}_{q\\psi(z\\vert x)p_\\mathcal{D}(x)} \\log q_\\psi(x\\vert z) \\leq \\inf_\\theta - \\mathbb{E}_{q\\psi(z\\vert x)p_\\mathcal{D}(x)} \\log p_\\theta(x\\vert z)$$</p>\n<!--kg-card-end: markdown--><h3 id=\"putting-this-bound-back-together-\">Putting this bound back together:</h3><p>Finding this lower bound to MI, we have now recovered the reconstruction term from the $\\beta$-VAE objective:</p><!--kg-card-begin: markdown--><p>$$<br />\n\\mathcal{L}(\\psi) + \\text{const} \\leq - (1 + \\lambda) \\mathbb{E}_{q_\\psi(z\\vert x)p_\\mathcal{D}(x)} \\log p_\\theta(x\\vert z) + \\mathbb{E}_{p_\\mathcal{D}}\\operatorname{KL}[q_\\psi(z\\vert x)| p(z)]<br />\n$$</p>\n<!--kg-card-end: markdown--><p>This is essentially the same as the $\\beta$-VAE objective function, where $\\beta$ is related to our previous $\\lambda$. In particular, $\\beta = \\frac{1}{1 + \\lambda}$. Thus, since we assumed $\\lambda&gt;0$ for the InfoMax objective to make sense, we can say that the $\\beta$-VAE objective encourages disentanglement in the InfoMax sense for values of $0&lt;\\beta&lt;1$.</p><h3 id=\"takeaways\">Takeaways</h3><p>Conceptually, this derivation is interesting because the main object of interest is now the recognition model, $q_\\psi(z\\vert x)$. That is, the posterior becomes a the focus of the objective function - something that is not the case when we are maximizing model likelihood alone (as explained <a href=\"https://www.inference.vc/maximum-likelihood-for-representation-learning-2/\">here</a>). In this respect, this derivation of the $\\beta$-VAE makes more sense from a representation learning viewpoint than the derivation of VAE from maximum likelihood.</p><p>There is a nice symmetry to these two views. There are two joint distributions over latents and observable variables in a VAE. On one hand we have $q_\\psi(z\\vert x)p_\\mathcal{D}(x)$ and on the other we have $p(x)p_\\theta(x\\vert z)$. The &quot;latent variable model&quot; $q_\\psi(z\\vert x)p_\\mathcal{D}(x)$ is a family of LVMs which has a marginal distribution on observable $\\mathbf{x}$ that is exactly the same as the data distribution $p_\\mathcal{D}$. So one can say $q_\\psi(z\\vert x)p_\\mathcal{D}(x)$ is a parametric family of latent variable models with whose likelihood is maximal - and we want to choose from this family a model where the representation $q_\\psi(z\\vert x)$ has nice properties.</p><p>On the flipside, $p(z)p_\\theta(x\\vert z)$ is a parametic set of models where the marginal distribution of latents is coordinatewise independent, but we would like to choose from this family a model that has good data likelihood.</p><p>The VAE objective tries to move these two latent variable models closer to one another. From the perspective of $q_\\psi(z\\vert x)p_\\mathcal{D}(x)$ this amounts to reproducing the prior $p(z)$ with the aggregate posterior. from the perspective of $p(z)p_\\theta(x\\vert z)$, it amounts to maximising the data likelihood. When the $\\beta$-VAE objective is used, we additionally wish to maximise the mutual information between the observed data and the representation.</p><p>This dual role of information maximization and maximum likelihood has been pointed out before, for example in this paper about the <a href=\"http://aivalley.com/Papers/MI_NIPS_final.pdf?ref=inference.vc\">IM algorithm</a>. The symmetry of variational learning has been exploited a few times, for example in <a href=\"https://link.springer.com/chapter/10.1007/978-3-662-07952-2_22?ref=inference.vc\">yin-yang machines</a>, and more recently also in methods like <a href=\"https://arxiv.org/abs/1606.00704?ref=inference.vc\">adversarially learned inference</a>. </p>"
        }
      ]
    },
    {
      "title": "Some Intuition on the Neural Tangent Kernel",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Some Intuition on the Neural Tangent Kernel"
      },
      "summary": "<p>Neural tangent kernels are a useful tool for understanding neural network training and implicit regularization in gradient descent. But it&apos;s not the easiest concept to wrap your head around. The paper that I found to have been most useful for me to develop an understanding is this one:</p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>Neural tangent kernels are a useful tool for understanding neural network training and implicit regularization in gradient descent. But it&apos;s not the easiest concept to wrap your head around. The paper that I found to have been most useful for me to develop an understanding is this one:</p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/"
        }
      ],
      "link": "https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/",
      "id": "5fb6a4e717ce640039c507c5",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Fri, 20 Nov 2020 13:57:12 GMT",
      "published_parsed": [
        2020,
        11,
        20,
        13,
        57,
        12,
        4,
        325,
        0
      ],
      "media_content": [
        {
          "url": "https://www.inference.vc/content/images/2020/11/download-45-1.png",
          "medium": "image"
        }
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<img alt=\"Some Intuition on the Neural Tangent Kernel\" src=\"https://www.inference.vc/content/images/2020/11/download-45-1.png\" /><p>Neural tangent kernels are a useful tool for understanding neural network training and implicit regularization in gradient descent. But it&apos;s not the easiest concept to wrap your head around. The paper that I found to have been most useful for me to develop an understanding is this one:</p><ul><li>Lee et al (2019) <a href=\"https://arxiv.org/abs/1902.06720?ref=inference.vc\">Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</a></li></ul><p>In this post I will illustrate the concept of neural tangent kernels through a simple 1D regression example. Please feel free to peruse the <a href=\"https://colab.research.google.com/drive/1zAMTtSuxR5V8xBJL02FMNtCLJuKY9TWg?usp=sharing&amp;ref=inference.vc\">google colab notebook</a> I used to make these plots.</p><h2 id=\"example-1-warming-up\">Example 1: Warming up</h2><p>Let&apos;s start from a very boring case begin with. Let&apos;s say we have a function defined over integers between -10 and 20. We parametrize our function as a look-up table, that is the value of the function $f(i)$ at each integer $i$ is described by a separate parameter $\\theta_i = f(i)$. I&apos;m initializing the parameters of this function as $\\theta_i = 3i+2$. The function is shown by the black dots below:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Some Intuition on the Neural Tangent Kernel\" class=\"kg-image\" height=\"321\" src=\"https://www.inference.vc/content/images/2020/11/download-40.png\" width=\"522\" /></figure><p>Now, let&apos;s consider what happens if we observe a new datapoint, $(x, y) =(10, 50)$, shown by the blue cross. We&apos;re going to take a gradient descent step updating $\\theta$. Let&apos;s say we use the squared error loss function $(f(10; \\theta) - 50)^2$ and a learning rate $\\eta=0.1$. Because the function&apos;s value at $x=10$ only depends on one of the parameter $\\theta_10$, only this parameter will be updated. The rest of the parameters, and therefore the rest of the function values remain unchanged. The red arrows illustrate the way function values move in a single gradient descent step: Most values don&apos;t move at all, only one of them moves closer to the observed data. Hence only one visible red arrow.</p><p>However, in machine learning we rarely parametrize functions as lookup tables of individual function values. This parametrization is pretty useless as it doesn&apos;t allow you to interpolate let alone extrapolate to unseen data. Let&apos;s see what happens in a more familiar model: linear regression.</p><h2 id=\"example-2-linear-function\">Example 2: Linear function</h2><p>Let&apos;s now consider the linear function $f(x, \\theta) = \\theta_1 x + \\theta_2$. I initialize the parameters to $\\theta_1=3$ and $\\theta_2=1$, so at initialisation, I have exactly the same function over integers as I had in the first example. Let&apos;s look at what happens to this function as I update $\\theta$ by performing single gradient descent step incorporating the observation $(x, y) =(10, 50)$ as before. Again, red arrows are show how function values move:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Some Intuition on the Neural Tangent Kernel\" class=\"kg-image\" height=\"348\" src=\"https://www.inference.vc/content/images/2020/11/download-37.png\" width=\"522\" /></figure><p>Whoa! What&apos;s going on now? Since individual function values are no longer independently parametrized, we can&apos;t move them independently. The model binds them together through its global parameters &#xa0;$\\theta_1$ and $\\theta_2$. If we want to move the function closer to the desired output $y=50$ at location $x=10$ the function values elsewhere have to change, too.</p><p>In this case, updating the function with an observation at $x=10$ changes the function value far away from the observation. &#xa0;It even changes the function value in the opposite direction than what one would expect.. This might seem a bit weird, but that&apos;s really how linear models work.</p><p>Now we have a little bit of background to start talking about this neural tangent kernel thing.</p><h2 id=\"meet-the-neural-tangent-kernel\">Meet the neural tangent kernel</h2><p>Given a function $f_\\theta(x)$ which is parametrized by $\\theta$, its <em>neural tangent kernel</em> $k_\\theta(x, x&apos;)$ quantifies how much the function&apos;s value at $x$ changes as we take an infinitesimally small gradient step in $\\theta$ incorporating a new observation at $x&apos;$. Another way of phrasing this is: $k(x, x&apos;)$ measures how sensitive the function value at $x$ is to prediction errors at $x&apos;$.</p><p>In the plots before, the size of the red arrows at each location $x$ were given by the following equation:</p><p>$$<br />\\eta \\tilde{k}_\\theta(x, x&apos;) = f\\left(x, \\theta + \\eta \\frac{f_\\theta(x&apos;)}{d\\theta}\\right) - f(x, \\theta)<br />$$</p><p>In neural network parlance, this is what&apos;s going on: The loss function tells me to increase the function value $f_\\theta(x&apos;)$. I back-propagate this through the network to see what change in $\\theta$ do I have to make to achieve this. However, moving $f_\\theta(x&apos;)$ this way also simultaneously moves $f_\\theta(x)$ at other locations $x \\neq x&apos;$. $\\tilde{k}_\\theta(x, x&apos;)$ expresses by how much.</p><p>The neural kernel is basically something like the limit of $\\tilde{k}$ in as the stepsize becomes infinitesimally small. In particular:</p><p>$$<br />k(x, x&apos;) = \\lim_{\\eta \\rightarrow 0} \\frac{f\\left(x, \\theta + \\eta \\frac{df_\\theta(x&apos;)}{d\\theta}\\right) - f(x, \\theta)}{\\eta}<br />$$</p><p>Using a 1st order Taylor expansion of $f_\\theta(x)$, it is possible to show that</p><p>$$<br />k_\\theta(x, x&apos;) = \\left\\langle \\frac{df_\\theta(x)}{d\\theta} , \\frac{f_\\theta(x&apos;)}{d\\theta} \\right\\rangle<br />$$</p><p>As homework for you: find $k(x, x&apos;)$ &#xa0;and/or $\\tilde{k}(x, x&apos;)$ for a fixed $\\eta$ in the linear model in the pervious example. Is it linear? Is it something else?</p><p>Note that this is a different derivation from what&apos;s in the paper (which starts from continuous differential equation version of gradient descent).</p><p>Now, I&apos;ll go back to the examples to illustrate two more important property of this kernel: sensitivity to parametrization, and changes during training.</p><h2 id=\"example-3-reparametrized-linear-model\">Example 3: Reparametrized linear model</h2><p>It&apos;s well known that neural networks can be repararmetized in ways that don&apos;t change the actual output of the function, but which may lead to differences in how optimization works. <a href=\"https://arxiv.org/pdf/1502.03167.pdf?ref=inference.vc\">Batchnorm</a> is a well-known example of this. Can we see the effect of reparametrization in the neural tangent kernel? Yes we can. Let&apos;s look at what happens if I reparametrize the linear function I used in the second example as:</p><p>$$<br />f_\\theta(x) = \\theta_1 x + \\color{blue}{10\\cdot}\\theta_2<br />$$</p><p>but now with parameters $\\theta_1=3, \\theta_2=\\color{blue}{0.1}$. I highlighted in blue what changed. The function itself, at initialization is the same since $10 * 0.1 = 1$. The function class is the same, too, as I can still implement arbitrary linear functions. However, when we look at the effect of a single gradient step, we see that the function changes differently when gradient descent is performed in this parametrisation.</p><figure class=\"kg-card kg-image-card\"><img alt=\"Some Intuition on the Neural Tangent Kernel\" class=\"kg-image\" height=\"348\" src=\"https://www.inference.vc/content/images/2020/11/download-39.png\" width=\"522\" /></figure><p>In this parametization, it became easier for gradient descent to push the whole function up by a constant, while in the previous parametrisation it decided to change the slope. What this demonstrates is that the neural tangent kernel $k_\\theta(x, x&apos;)$ is sensitive to reparametrization.</p><h2 id=\"example-4-tiny-radial-basis-function-network\">Example 4: tiny radial basis function network</h2><p>While the linear models may be good illustration, let&apos;s look at what $k_\\theta(x, x&apos;)$ looks like in a nonlinear model. Here, I&apos;ll consider a model with two squared exponential basis functions:</p><p>$$<br />f_\\theta(x) = \\theta_1 \\exp\\left(-\\frac{(x - \\theta_2)^2}{30}\\right) + \\theta_3 \\exp\\left(-\\frac{(x - \\theta_4)^2}{30}\\right) &#xa0;+ \\theta_5,<br />$$</p><p>with initial parameter values $(\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5) = (4.0, -10.0, 25.0, 10.0, 50.0)$. These are chosen somewhat arbitrarily and to make the result visually appealing:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Some Intuition on the Neural Tangent Kernel\" class=\"kg-image\" height=\"328\" src=\"https://www.inference.vc/content/images/2020/11/download-41.png\" width=\"504\" /></figure><p>We can visualise the function $\\tilde{k}_\\theta(x, 10)$ directly, rather than plotting it on top the function. Here I also normalize it by dividing by $\\tilde{k}_\\theta(10, 10)$.</p><figure class=\"kg-card kg-image-card\"><img alt=\"Some Intuition on the Neural Tangent Kernel\" class=\"kg-image\" height=\"246\" src=\"https://www.inference.vc/content/images/2020/11/download-43.png\" width=\"504\" /></figure><p>What we can see is that this starts to look a bit like a <em>kernel</em> function in that it has higher values near $10$ and decreases as you go farther away. However, a few things are worth noting: the maximum of this kernel function is not at $x=1o$, but at $x=7$. It means, that the function value $f(7)$ changes more in reaction to an observation at $x&apos;=10$ than the value $f(10)$. Secondly, there are some negative values. In this case the previous figure provides a visual explanation why: we can increase the function value at $x=10$ by pushing the valley centred at $\\theta_1=4$ away from it, to the left. This parameter change in turn decreases function values on the left-hand wall of the valley. Third, the kernel function converges to a positive constant at its tails - this is because of the offset $\\theta_5$.</p><h2 id=\"example-5-changes-as-we-train\">Example 5: Changes as we train</h2><p>Now I&apos;m going to illustrate another important property of the neural tangent kernel: in general, the kernel depends on the parameter value $\\theta$, and therefore it changes as the model is trained. Here I show what happens to the kernel as I take 15 gradient ascent steps trying to increase $f(10)$. The purple curve is the one I had at initialization (above), and the orange ones show the kernel at the last gradient step.</p><figure class=\"kg-card kg-image-card\"><img alt=\"Some Intuition on the Neural Tangent Kernel\" class=\"kg-image\" height=\"246\" src=\"https://www.inference.vc/content/images/2020/11/download-45.png\" width=\"490\" /></figure><p>The corresponding changes to the function $f_\\theta_t$ changes are shown below:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Some Intuition on the Neural Tangent Kernel\" class=\"kg-image\" height=\"239\" src=\"https://www.inference.vc/content/images/2020/11/download-46.png\" width=\"504\" /></figure><p>So we can see that as the parameter changes, the kernel also changes. The kernel becomes flatter. An explanation of this is that eventually we reach a region of parameter space, where $\\theta_4$ changes the fastest. </p><h2 id=\"why-is-this-interesting\">Why is this interesting?</h2><p>It turns out the neural tangent kernel becomes particularly useful when studying learning dynamics in infinitely wide feed-forward neural networks. Why? Because in this limit, two things happen:</p><ol><li>First: if we initialize $\\theta_0$ randomly from appropiately chosen distributions, the initial NTK of the network $k_{\\theta_0}$ approaches a deterministic kernel as the width increases. This means, that at initialization, $k_{\\theta_0}$ doesn&apos;t really depend on $\\theta_0$ but is a fixed kernel independent of the specific initialization.</li><li>Second: in the infinite limit the kernel $k_{\\theta_t}$ stays constant over time as we optimise $\\theta_t$. This removes the parameter dependence during training.</li></ol><p>These two facts put together imply that gradient descent in the infinitely wide and infinitesimally small learning rate limit can be understood as a pretty simple algorithm called <em>kernel gradient descent</em> with a fixed kernel function that depends only on the architecture (number of layers, activations, etc).</p><p>These results, taken together with an older known result by <a href=\"https://arxiv.org/pdf/1912.02803.pdf?ref=inference.vc\">Neal, (1994)</a>, allows us to characterise the probability distribution of minima that gradient descent converges to in this infinite limit as a Gaussian &#xa0;process. For details, see the paper mentioned above.</p><h2 id=\"don-t-mix-your-kernels\">Don&apos;t mix your kernels</h2><p>There are two somewhat related sets of results both involving infinitely wide neural netwoks and kernel functions, so I just wanted to clarify the difference between them:</p><ol><li>the older, well-known result by <a href=\"https://arxiv.org/pdf/1912.02803.pdf?ref=inference.vc\">Neal, (1994)</a>, later extended by others, is that the distribution of $f_\\theta$ under random initialization of $\\theta$ converges to a Gaussian process. This Gaussian process has a kernel or covariance function which is not, in general, the same as the neural tangent kernel. This old result doesn&apos;t say anything about gradient descent, and is typically used to motivate the use of Gaussian process-based Bayesian methods.</li><li>the new, NTK, result is that the evolution of $f_{\\theta_t}$ during gradient descent training can be described in terms of a kernel, the neural tangent kernel, and that in the infinite limit this kernel stays constant during training and is deterministic at initialization. Using this result, it is possible to show that in some cases the distribution of $f_{\\theta_t}$ is a Gaussian process at every timestep $t$, not just at initialization. This result also allows us to identify the Gaussian process which describes the limit as $t \\rightarrow \\infty$. This limiting Gaussian process however is not the same as the posterior Gaussian process which Neal and others would calculate on the basis of the first result.</li></ol><p>So I hope this post helps a bit by building some intuition about what the neural tangent kernel is. If you&apos;re interested, check out the simple <a href=\"https://colab.research.google.com/drive/1zAMTtSuxR5V8xBJL02FMNtCLJuKY9TWg?usp=sharing&amp;ref=inference.vc\">colab notebook</a> I used for these illustrations.</p>"
        }
      ]
    },
    {
      "title": "Notes on Causally Correct Partial Models",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Notes on Causally Correct Partial Models"
      },
      "summary": "<p>I recently encountered this cool paper in a reading group presentation:</p><ul><li>Rezende et al (2020) Rezende <a href=\"https://arxiv.org/abs/2002.02836v1?ref=inference.vc\">Causally Correct Partial Models for Reinforcement Learning</a></li></ul><p>It&apos;s frankly taken me a long time to understand what was going on, and it took me weeks to write this half-decent explanation of it.</p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>I recently encountered this cool paper in a reading group presentation:</p><ul><li>Rezende et al (2020) Rezende <a href=\"https://arxiv.org/abs/2002.02836v1?ref=inference.vc\">Causally Correct Partial Models for Reinforcement Learning</a></li></ul><p>It&apos;s frankly taken me a long time to understand what was going on, and it took me weeks to write this half-decent explanation of it.</p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/notes-on-causally-correct-partial-models-2/"
        }
      ],
      "link": "https://www.inference.vc/notes-on-causally-correct-partial-models-2/",
      "id": "5fad12006af0b10039bc5c8f",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Thu, 12 Nov 2020 14:58:37 GMT",
      "published_parsed": [
        2020,
        11,
        12,
        14,
        58,
        37,
        3,
        317,
        0
      ],
      "media_content": [
        {
          "url": "https://www.inference.vc/content/images/2020/11/Screenshot-2020-10-28-at-13.41.29-1.png",
          "medium": "image"
        }
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<img alt=\"Notes on Causally Correct Partial Models\" src=\"https://www.inference.vc/content/images/2020/11/Screenshot-2020-10-28-at-13.41.29-1.png\" /><p>I recently encountered this cool paper in a reading group presentation:</p><ul><li>Rezende et al (2020) Rezende <a href=\"https://arxiv.org/abs/2002.02836v1?ref=inference.vc\">Causally Correct Partial Models for Reinforcement Learning</a></li></ul><p>It&apos;s frankly taken me a long time to understand what was going on, and it took me weeks to write this half-decent explanation of it. The <a href=\"https://hackmd.io/ag2vVxJzSf-Ve8R88rZWlQ?ref=inference.vc\">first notes</a> I wrote followed the logic of the paper more, this in this post I thought I&apos;d just focus on the high level idea, after which hopefully the paper is more straightforward. I wanted to capture the key idea, without the distractions of RNN hidden states, etc, which I found confusing to think about.</p><h2 id=\"pomdp-setup\">POMDP setup</h2><p>To start with the basics, this paper deals with the &#xa0;partially observed Markov decision process (POMDP) setup. The diagram below illustrates what&apos;s going on:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Notes on Causally Correct Partial Models\" class=\"kg-image\" height=\"266\" src=\"https://www.inference.vc/content/images/2020/10/Screenshot-2020-10-28-at-13.41.29.png\" width=\"564\" /></figure><p>The grey nodes $e_t$ show the unobserved state of the environment at each timestep $t=0,1,2\\ldots$. At each timestep the agent observes $y_0$ which depends on the current state of the environment (red-ish nodes). The agent then updates their state $s_t$ based on its past state $s_{t-1}$, the new observation $y_t$, and the previous action taken $a_{t-1}$. This is shown by the blue squares (they&apos;re squares, signifying that this node depends deterministically on its parents). Then, based on the agent&apos;s state, it chooses an action $a_t$ from by sampling from policy $\\pi(a_t\\vert s_t)$. The action influences how the environment&apos;s state, $e_{t+1}$ changes.</p><p>We assume that the agent&apos;s ultimate goal is to maximise reward at the last state at time $T$, which we assume is a deterministic function of the observation $r(y_T)$. Think of this reward as the score in an atari game, which is written on the screen whose contents are made available in $y_t$.</p><h3 id=\"the-ultimate-goal\">The ultimate goal</h3><p>Let&apos;s start by stating what we ultimately would like estimate from the data we have. The assumption is that we sampled the data using some policy $\\pi$, but we would like to be able to say how well a different policy $\\tilde{\\pi}$ would do, in other words, what would be the expected score at time $T$ if instead of $\\pi$ we used a different policy $\\tilde{\\pi}$.</p><p>What we are interested in, is a causal/counterfactual query:</p><p>$$<br />\\mathbb{E}_{\\tau\\sim\\tilde{p}}[r(y_T)],<br />$$</p><p>where $\\tau = [(s_t, y_t, e_t, a_t) : t=0\\ldots T]$ denotes a trajectory or rollout up to time $T$, and $\\tilde{p}$ denotes the generative process when using policy $\\tilde{\\pi}$, that is:</p><p>$$<br />\\tilde{p}(\\tau) = p(e_0)p(y_0\\vert e_0) \\tilde{\\pi}(a_0\\vert s_0) p(s_0)\\prod_{t=1}^T p(e_t\\vert a_{t-1}, e_{t-1}) p(y_t\\vert e_t) \\tilde{\\pi}(a_t\\vert s_t) \\delta (s_t - g(s_{t-1}, y_t))<br />$$</p><p>I called this a causal or counterfactual query, because we are interested in making predictions under a different distribution $\\tilde{p}$ than $p$ which we have observations from. The difference between $\\tilde{p}$ and $p$ can be called an intervention, where we replace specific factors in the data generating process with different ones.</p><p>There are - at least - two ways one could go about estimating such counterfactual distribution:</p><ol><li>model-free, via <em>importance sampling</em>. This method tries to directly estimate the causal query by calculating a weighted average over the observed data. The weights are given by the ratio between $\\tilde{\\pi}(a_t\\vert s_t)$, the probability by which $\\tilde{\\pi}$ would choose an action and $\\pi(a_t\\vert s_t)$, the probability it was chosen by the policy that we used to collect the data. A great paper explaining how this works is <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2013/11/bottou13a.pdf?ref=inference.vc\">(Bottou et al, 2013)</a>. Importance sampling as the advantage that we don&apos;t have to build any model of the environment, we can directly evaluate the average reward from the samples we have, using only $\\pi$ and $\\tilde{\\pi}$ &#xa0;to calculate the weights. The downside, however, is that importance sampling &#xa0;often incredibly high variance estimate, and is only reliable if $\\tilde{\\pi}$ and $\\pi$ are very close.</li><li>model-based, via <em>causal calculus</em>. If possible, we can <a href=\"https://www.inference.vc/untitled/\">use do-calculus</a> to express the causal query in an alternative way, using various conditional distributions estimated from the observed data. This approach has the disadvantage that it requires us build a model from the data first. We then use the conditional distributions learned from the data to approximate the quantity of interest by plugging them into the formula we got from do-calculus. If our models are imperfect, these imperfections/approximation errors can compound when the causal estimand is calculated, potentially leading to large biases and inaccuracies. On the other hand, our models may be accurate enough to extrapolate to situations where importance weighting would be unreliable.</li></ol><p>In this paper, we focus on solving the problem with causal calculus. This requires us to build a model of observed data, which we can then use to make causal predictions. The key question this paper asks is</p><blockquote>How much of the data do we have to model to be able to make the kinds of causal inferences we would like to make?</blockquote><h3 id=\"option-1-model-almost-everything\">Option 1: model (almost) everything</h3><p>One way we can answer the query above is to model the joint distribution of everything, or mostly everything, that we can observe. For example, we could build a full autoregressive model of observations $y_t$ conditioned on actions $a_t$. In essence this would amount to fitting a model to $p(y_{0:T}\\vert a_{0:T})$.</p><p>If we had such model, we would theoretically be able to make causal predictions, for reasons I will explain later. However, this option is ruled out in the paper because we assume the observations $y_t$ are very high dimensional, such as images rendered in a computer game. Thus, modelling the joint distribution of the whole observation sequence $y_{1:T}$ accurately is hopeless and would require a lot of data. Therefore, we would like to get away without modelling the whole observation sequence $y_{1:T}$, which brings us to partial models.</p><h3 id=\"option-2-partial-models\">Option 2: partial models</h3><p>Partial models try to avoid modelling the joint distribution of high-dimensional observations $y_{1:T}$ or agent-state sequences $s_{0:T}$, and focus on modelling directly the distribution of $r(y_T)$ - i.e. only the reward component of the last observation, given the action-sequence $a_{0:T}$. This is clearly a lot easier to do, because $r(y_T)$ is assumed to be a low-dimensional aspect of the full observation $y_T$, so all we have to learn is a model of a scalar conditioned on a sequence of actions $q_\\theta(r(y_T)\\vert a_{0:T})$. We know very well how to fit such models to realistic amounts of data.</p><p>However, if we don&apos;t include either $y_t$ or $s_t$ in our model, we won&apos;t be able to make the counterfactual inferences we wanted to make in the first place. Why? Let&apos;s look at he data generating process once more:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Notes on Causally Correct Partial Models\" class=\"kg-image\" height=\"266\" src=\"https://www.inference.vc/content/images/2020/10/Screenshot-2020-10-28-at-13.41.29.png\" width=\"564\" /></figure><p>We are trying to model the causal impact of actions $a_0$ and $a_1$ on the outcome $y_2$. Let&apos;s focus on $a_1$. $y_2$ is clearly statistically dependent on $a_1$. However, this statistical dependence emerges due to completely different effects:</p><ul><li><strong>causal association:</strong> $a_1$ influences the state of the environment $e_2$, resulting in an observation $y_2$. Therefore, $a_1$ has an direct causal effect on $y_2$, mediated by $e_2$</li><li><strong>spurious association due to confounding:</strong> The unobserved hidden state $e_1$ is a confounder between the action $a_1$ and the observation $y_2$. The state $e_1$ has an indirect causal effect on $a_1$ mediated by the observation $y_1$ and the agent&apos;s state $s_1$. Similarly $e_1$ has an indirect effect on $y_2$ mediated by $e_2$.</li></ul><p>I illustrated these two sources of statistical association by colour-coding the different paths in the causal graph. The blue path is the confounding path: correlation is induced because both $a_1$ and $y_2$ have $e_1$ as causal ancestor. The red path is the causal path: $a_1$ indirectly influences $y_2$ via the hidden state $e_2$.</p><figure class=\"kg-card kg-image-card\"><img alt=\"Notes on Causally Correct Partial Models\" class=\"kg-image\" height=\"266\" src=\"https://www.inference.vc/content/images/2020/11/Screenshot-2020-10-28-at-13.41.29.png\" width=\"564\" /></figure><p>If we would like to correctly evaluate the consequence of changing policies, we have to be able to disambiguate between these two sources of statistical association, get rid of the blue path, and only take the red path into account. Unfortunately, this is not possible in a partial model, where we only model the distribution of $y_2$ conditional on $a_0$ and $a_1$.</p><p>If we want to draw causal inferences, we <strong>must model the distribution of at least one variable along blue path.</strong> Clearly, $y_1$ and $s_1$ are theoretically observable, and are on the confounding path. Adding either of these to our model would allow us to use the backdoor adjustment formula (explained in the paper). However, this would take us back to Option 1, where we have to model the joint distribution of either sequences of observations $y_{0:T}$ or sequences of states $s_{0:T}$, both assumed to be high-dimensional and difficult to model.</p><h3 id=\"option-3-causally-correct-partial-models\">Option 3: causally correct partial models</h3><p>So we finally got to the core of what is proposed in the paper: a kind of halfway-house between modeling everything and modeling too little. We are going to model <em>enough </em>variables to be able to evaluate causal queries, while keeping the dimensionality of the model we have to fit low. To do this, we change the data generating process slightly - by splitting the policy into two stages:</p><figure class=\"kg-card kg-image-card\"><img alt=\"Notes on Causally Correct Partial Models\" class=\"kg-image\" height=\"270\" src=\"https://www.inference.vc/content/images/2020/11/JrUSsb4.png\" width=\"634\" /></figure><p>The agent first generates $z_t$ from the state $s_t$, and then uses the sampled $z_t$ value to make a decision $a_t$. One can understand $z_t$ as being a stochastic bottleneck between the agent&apos;s high-dimensional state $s_t$, and the low-dimensional action $a_t$. The assumption is that the sequence $z_{0:T}$ should be a lot easier to model than either $y_{0:T}$ or $s_{0:T}$. However, if we now build a model $p(r(y_T), z_{0:T} \\vert a_{0:T})$ are now able to use this model evaluate the causal queries of interest, thanks for the backdoor adjustment formula. For how to precisely do this, please refer to the paper.</p><p>Intuitively, this approach helps by adding a low-dimensional stochastic node along the confounding path. This allows us to compensate for confounding, without having to build a full generative model of sequences of high-dimensional variables. It allows us to solve the problem we need to solve without having to solve a ridiculously difficult subproblem.</p>"
        }
      ]
    },
    {
      "title": "Meta-Learning Millions of Hyper-parameters using the Implicit Function Theorem",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "Meta-Learning Millions of Hyper-parameters using the Implicit Function Theorem"
      },
      "summary": "<p>Last night on the train I read this nice paper by David Duvenaud and colleagues. Around midnight I got a calendar notification &quot;it&apos;s David Duvenaud&apos;s birthday&quot;. So I thought it&apos;s time for a David Duvenaud birthday special (don&apos;t get too</p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<p>Last night on the train I read this nice paper by David Duvenaud and colleagues. Around midnight I got a calendar notification &quot;it&apos;s David Duvenaud&apos;s birthday&quot;. So I thought it&apos;s time for a David Duvenaud birthday special (don&apos;t get too</p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/meta-learning-with-the-implicit-function-theorem/"
        }
      ],
      "link": "https://www.inference.vc/meta-learning-with-the-implicit-function-theorem/",
      "id": "5dcd5c2289b7c10044af6516",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Thu, 14 Nov 2019 15:33:49 GMT",
      "published_parsed": [
        2019,
        11,
        14,
        15,
        33,
        49,
        3,
        318,
        0
      ],
      "media_content": [
        {
          "url": "https://www.inference.vc/content/images/2019/11/Screen-Shot-2019-11-14-at-3.03.24-PM.png",
          "medium": "image"
        }
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<img alt=\"Meta-Learning Millions of Hyper-parameters using the Implicit Function Theorem\" src=\"https://www.inference.vc/content/images/2019/11/Screen-Shot-2019-11-14-at-3.03.24-PM.png\" /><p>Last night on the train I read this nice paper by David Duvenaud and colleagues. Around midnight I got a calendar notification &quot;it&apos;s David Duvenaud&apos;s birthday&quot;. So I thought it&apos;s time for a David Duvenaud birthday special (don&apos;t get too excited David, I won&apos;t make it an annual tradition...)</p><ul><li>Jonathan Lorraine, Paul Vicol, David Duvenaud (2019) <a href=\"https://arxiv.org/abs/1911.02590?ref=inference.vc\">Optimizing Millions of Hyperparameters by Implicit Differentiation</a></li></ul><h2 id=\"background\">Background</h2><p>I recently covered<a href=\"https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/\"> iMAML: the meta-learning algorithm</a> that makes use of implicit gradients to sidestep backpropagating through the inner loop optimization in meta-learning/hyperparameter tuning. The method presented in (<a href=\"https://arxiv.org/abs/1911.02590?ref=inference.vc\">Lorraine et al, 2019</a>) uses the same high-level idea, but introduces a different - on the surface less fiddly - approximation to the crucial inverse Hessian. I won&apos;t spend a lot of time introducing the whole meta-learning setup from scratch, you can use the previous post as a starting point.</p><h2 id=\"implicit-function-theorem\">Implicit Function Theorem</h2><p>Many - though not all - meta-learning or hyperparameter optimization problems can be stated as nested optimization problems. If we have some hyperparameters $\\lambda$ and some parameters $\\theta$ we are interested in</p><p>$$<br />\\operatorname{argmin}_\\lambda \\mathcal{L}_V (\\operatorname{argmin}_\\theta \\mathcal{L}_T(\\theta, \\lambda)),<br />$$</p><p>Where $\\mathcal{L}_T$ is some training loss and $\\mathcal{L}_V$ a validation loss. The optimal parameter to the training problem, $\\theta^\\ast$ implicitly depends on the hyperparameters $\\lambda$:</p><p>$$<br />\\theta^\\ast(\\lambda) = \\operatorname{argmin} f(\\theta, \\lambda)<br />$$</p><p>If this implicit function mapping $\\lambda$ to $\\theta^\\ast$ is differentiable, and subject to some other conditions, the implicit function theorem states that its derivative is</p><p>$$<br />\\left.\\frac{\\partial\\theta^{\\ast}}{\\partial\\lambda}\\right\\vert_{\\lambda_0} = \\left.-\\left[\\frac{\\partial^2 \\mathcal{L}_T}{\\partial \\theta \\partial \\theta}\\right]^{-1}\\frac{\\partial^2\\mathcal{L}_T}{\\partial \\theta \\partial \\lambda}\\right\\vert_{\\lambda_0, \\theta^\\ast(\\lambda_0)}<br />$$</p><p>The formula we obtained for iMAML is a special case of this where the $\\frac{\\partial^2\\mathcal{L}_T}{\\partial \\theta \\partial \\lambda}$ is the identity This is because there, the hyperparameter controls a quadratic regularizer $\\frac{1}{2}\\|\\theta - \\lambda\\|^2$, and indeed if you differentiate this with respect to both $\\lambda$ and $\\theta$ you are left with a constant times identity.</p><p>The primary difficulty of course is approximating the inverse Hessian, or indeed matrix-vector products involving this inverse Hessian. This is where iMAML and the method proposed by <a href=\"https://arxiv.org/abs/1911.02590?ref=inference.vc\">Lorraine et al, (2019</a>) differ. iMAML uses a conjugate gradient method to iteratively approximate the gradient. In this work, they use a Neumann series approximation, which, for a matrix $U$ looks as follows:</p><p>$$<br />U^{-1} = \\sum_{i=0}^{\\infty}(I - U)^i<br />$$</p><p>This is basically a generalization of the better known sum of a geometric series: if you have a scalar $\\vert u \\vert&lt;1$ then</p><p>$$<br />\\sum_{i=0}^\\infty q^i = \\frac{1}{1-q}.<br />$$</p><p>Using a finite truncation of the Neumann series one can approximate the inverse Hessian in the following way:</p><p>$$<br />\\left[\\frac{\\partial^2 \\mathcal{L}_T}{\\partial \\theta \\partial \\theta}\\right]^{-1} \\approx \\sum_{i=1}^j \\left(I - \\frac{\\partial^2 \\mathcal{L}_T}{\\partial \\theta \\partial \\theta}\\right)^i.<br />$$</p><p>This Neumann series approximation, at least on the surface, seems significantly less hassle to implement than running a conjugate gradient optimization step.</p><h2 id=\"experiments\">Experiments</h2><p>One of the fun bits of this paper is the interesting set of experiments the authors used to demonstrate the versatility of this approach. For example, in this framework, one can treat the training dataset as a hyperparameter. Optimizing pixel values in a small training dataset, one image per class, allowed the authors to &quot;distill&quot; a dataset into a set of prototypical examples. If you train your neural net on this distilled dataset, you get relatively good validation performance. The results are not quite as image-like as one would imagine, but for some classes, like bikes, you even get recognisable shapes:</p><p>In another experiment the authors trained a network to perform data augmentation, treating parameters of this network as a hyperparameter of a learning task. In both of these cases, the number of hyperparameters optimized were in the hundreds of thousands, way beyond the number we usually consider as hyperparameters.</p><h2 id=\"limitations\">Limitations</h2><p>This method inherits some of the limitations I already discussed with <a href=\"https://www.inference.vc/notes-on-imaml-meta-learning-without-differentiating-through/\">iMAML.</a> Please also see the comments where various people gave pointers to work that overcomes some of these limitations.</p><p>Most crucially, methods based on implicit gradients assume that your learning algorithm (inner loop) finds a unique, optimal parameter that minimises some loss function. This is simply not a valid assumption for SGD where different random seeds might produce very different and differently behaving optima.</p><p>Secondly, this assumption only allows for hyperparameters that control the loss function, but not for ones that control other aspects of the optimization algorithm, such as learning rates, batch sizes or initialization. For those kind of situations, explicit differentiation may still be the most competitive solution. On that note, I also recommend reading this recent paper on <a href=\"https://arxiv.org/abs/1910.01727?ref=inference.vc\">generalized inner-loop meta-learnin</a>g and the associated pytorch package <a href=\"https://github.com/facebookresearch/higher?ref=inference.vc\">higher</a>.</p><h2 id=\"conclusion\">Conclusion</h2><p>Happy birthday David. Nice work!</p>"
        }
      ]
    },
    {
      "title": "The secular Bayesian: Using belief distributions without really believing",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "The secular Bayesian: Using belief distributions without really believing"
      },
      "summary": "<h2 id=\"the-religious-bayesian\">The religious Bayesian</h2><p>My parents didn&apos;t raise me in a religious tradition. It all started to change when a great scientist took me under his wing and taught me the teachings of Bayes. I travelled the world and spent 4 years in a Bayesian monastery in Cambridge, UK.</p>",
      "summary_detail": {
        "type": "text/html",
        "language": null,
        "base": "https://www.inference.vc/rss/",
        "value": "<h2 id=\"the-religious-bayesian\">The religious Bayesian</h2><p>My parents didn&apos;t raise me in a religious tradition. It all started to change when a great scientist took me under his wing and taught me the teachings of Bayes. I travelled the world and spent 4 years in a Bayesian monastery in Cambridge, UK.</p>"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://www.inference.vc/the-secular-bayesian-using-belief-distributions-without-really-believing/"
        }
      ],
      "link": "https://www.inference.vc/the-secular-bayesian-using-belief-distributions-without-really-believing/",
      "id": "5dbac64b6c822a003820974f",
      "guidislink": false,
      "authors": [
        {
          "name": "Ferenc Huszar"
        }
      ],
      "author": "Ferenc Huszar",
      "author_detail": {
        "name": "Ferenc Huszar"
      },
      "published": "Thu, 31 Oct 2019 15:13:21 GMT",
      "published_parsed": [
        2019,
        10,
        31,
        15,
        13,
        21,
        3,
        304,
        0
      ],
      "media_content": [
        {
          "url": "https://www.inference.vc/content/images/2019/10/image-url-1.jpg",
          "medium": "image"
        }
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://www.inference.vc/rss/",
          "value": "<h2 id=\"the-religious-bayesian\">The religious Bayesian</h2><img alt=\"The secular Bayesian: Using belief distributions without really believing\" src=\"https://www.inference.vc/content/images/2019/10/image-url-1.jpg\" /><p>My parents didn&apos;t raise me in a religious tradition. It all started to change when a great scientist took me under his wing and taught me the teachings of Bayes. I travelled the world and spent 4 years in a Bayesian monastery in Cambridge, UK. This particular place practiced the nonparametric Bayesian doctrine.</p><p>We were religious Bayesians. We looked at the world and all we saw the face of Bayes: if something worked, it did because it had a Bayesian interpretation. If an algorithm did not work, we shunned its creator for being unfaithful to Bayes. &#xa0;We scorned at point estimates, despised p-values. Bayes had the answer to everything. But above all, we believed in our models.</p><figure class=\"kg-card kg-image-card\"><img alt=\"The secular Bayesian: Using belief distributions without really believing\" class=\"kg-image\" src=\"https://www.inference.vc/content/images/2019/10/Bayes_toast.jpg\" /></figure><h2 id=\"possessed-by-deamons\">Possessed by deamons</h2><p>At a convention dominated by Bayesian thinkers I was approached by a frequentist, let&apos;s call him Lucifer (in fact his real name is <a href=\"https://www.szit.bme.hu/~gyorfi/indexen.html?ref=inference.vc\">Laci</a> so not that far off). &quot;Do you believe your data exists?&quot; - he asked. &quot;Yes&quot; I answered. &quot;Do you believe your model and its parameters exist?&quot; &quot;Well, not really, it&apos;s just a model I use to describe reality&quot; I said. Then he told me the following, poisoning my pure Bayesian heart forever: &quot;If you use Bayes&apos; rule, you assume that a joint distribution between model parameters and data exist. This, however, only exists if your data and your parameters both exist, in the same $\\sigma$-algebra. You can&apos;t have it both ways. You have to think your model really exists somewhere.&quot; </p><p>I never forgot this encounter, but equally I didn&apos;t think much about it since then. Over the years, I started to doubt more and more aspects of my Bayesian faith. I realised the likelihood was important, but not the only thing that exists. There were scoring rules, loss functions which couldn&apos;t be written as a log-likelihood. I noticed nonparametric Bayesian models weren&apos;t automatically more useful than large parametric ones. &#xa0;I worked on weird stuff like <a href=\"http://proceedings.mlr.press/v15/lacoste_julien11a.html?ref=inference.vc\">loss-calibrated Bayes</a>. I started having thoughts about model misspecification, kind of a taboo topic in the Bayesian church.</p><h2 id=\"the-secular-bayesian\">The secular Bayesian</h2><p>Over the years I came to terms with my Bayesian heritage, and I now live my life as a secular Bayesian. Certain elements of the Bayesian way are no doubt useful: Engineering inductuve biases explicitly into a prior distribution, using probabilities, divergences, information, variational bounds as tools for developing new algorithms. Posterior distributions can capture model uncertainty which can be exploited for active learning or exploration in interactive learning. Bayesian methods often - though not always - lead to increased robustness, better calibration, and so much more. At the same time, I can carry on living my life, use gradient descent to find local minima, use bootstrap to capture uncertainty. And first and foremost, I do not have to believe that my models really exist or perfectly describe reality anymore. I am free to think about model misspecification.</p><p>Lately, I have started to familiarize myself with a new body of work, which I call secular Bayesianism, that combines Bayesian inference with more frequentists ideas about learning from observation. In this body of work, people study model misspecification (see e.g. M-open Bayesian inference). And, I found a resolution to the &quot;you have to believe in your model, you can&apos;t have it both ways&quot; problem that bothered me all these years.</p><h2 id=\"a-generalized-framework-for-updating-belief-distributions\">A generalized framework for updating belief distributions</h2><p>After this rather long intro, let me present the paper this post is really about and which, as a secular Bayesian, I found very interesting:</p><ul><li>P.G. Bissiri, C.C. Holmes and S.G. Walker (2016) <a href=\"https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12158?ref=inference.vc\">A General Framework for Updating Belief Distributions</a></li></ul><p>This paper basically asks: can we take the belief out of belief distributions? Let&apos;s say we want to estimate some parameter of interest $\\theta$ from data. Does it still make sense to specify a prior distribution over this parameter, and then update them in light of data using some kind of Bayes rule-like update mechanism to form posterior distributions, all without assuming that the parameter of interest $\\theta$ and the observations $x_i$ are linked to one another via a probabilistic model? And if it is meaningful, what form would that update rule take.</p><h3 id=\"the-setup\">The setup</h3><p>First of all, for simplicity, let&apos;s assume that data $x_i$ is sampled i.i.d from some distribution $P$. That&apos;s right, not exchangeable, actually i.i.d. like in frequentist settings. Let&apos;s also assume that we have some parameter of interest $\\theta$. Unlike in Bayesian analysis where $\\theta$ usually parametrises some kind of generative model for data $x_i$, we don&apos;t assume anything like that. All we assume is that there is a loss function $\\ell$ which connects the parameter to the observations: $\\ell(\\theta, x)$ measures how well the estimate $\\theta$ agrees with observation $x$.</p><p>Let&apos;s say that a priori, without seeing any datapoints we have a prior distribution $\\pi$ over $\\theta$. Now we observe a datapoint $x_1$. How should we make use of our observation $x_1$, the loss function $\\ell$ and the prior $\\pi$ to come up with some kind of posterior over this parameter? Let&apos;s denote this update rule $\\psi(\\ell(\\cdot, x_1), \\pi)$. There are many ways we could do this, but is there one which is better than the rest?</p><h3 id=\"desiderata\">Desiderata</h3><p>The paper lists a number of desiderata - desired properties the update rule $\\psi$ should satisfy. These are all meaningful assumptions to have. The main one is coherence, which is a property somewhat analogous to exchangeability: if we observe a sequence of observations, we would like the resulting posterior to be the same, irrespective of which order the observations are presented. The coherence property can be written as follows</p><p>$$<br />\\psi\\left(\\ell(\\cdot, x_2), \\psi\\left(\\ell(\\cdot, x_1), \\pi\\right)\\right) = \\psi\\left(\\ell(\\cdot, x_1), \\psi\\left(\\ell(\\cdot, x_2), \\pi \\right)\\right)<br />$$</p><p>As a desired property, this makes a lot of sense, and Bayes rule obviously satisfies it. However, this is not really how the authors actually define coherence. In Equation (3) they use a more restrictive definition of coherence, further restricting the set of acceptable update rules as follows:</p><p>$$<br />\\psi\\left(\\ell(\\cdot, x_2), \\psi\\left(\\ell(\\cdot, x_1), \\pi\\right)\\right) = \\psi\\left(\\ell(\\cdot, x_1) + \\ell(\\cdot, x_2), \\pi \\right)<br />$$</p><p>By combining losses from the two observations in an additive way, one can indeed ensure permuation invariance. However, the sum is not the only way to do this. Any pooling operation over observations would also have satisfied this. For example, one could replace the $\\ell(\\cdot, x_1) + \\ell(\\cdot, x_2)$ bit by $\\max(\\ell(\\cdot, x_1), \\ell(\\cdot, x_2))$ and still satisfy the general principle of coherence. The most general class of permutation invariant functions which would satisfy the general coherence desideratum are discussed in <a href=\"https://www.inference.vc/deepsets-modeling-permutation-invariance/\">DeepSets</a>. Overall, my hunch is that going with the sum is a design choice, rather than a general desideratum. This choice is the real reason why the resulting update rule will end up very Bayes-rule like, as we will see later.</p><p>The other desiderata the paper proposes are actually discussed separately in Section 1.2 of <a href=\"https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12158?ref=inference.vc\">(Brissini et al, 2016)</a>, and called assumptions instead. These are much more basic requirements for the update function. Assumption 2 for example talks about how restricting the prior to a subset should result in a posterior which is also the restricted version of the original posterior. Assumption 3 requires that lower evidence (larger loss) for a parameter should yield smaller posterior probabilities - a monotonicity property.</p><h3 id=\"uniqueness-of-coherent-update-rule\">Uniqueness of coherent update rule</h3><p>One contribution of the paper is showing that all the desiderata mentioned above pinpoint a specific update rule $\\psi$ which satisfies all the desired properties. This update takes the following form:</p><p>$$<br />\\pi(\\theta\\vert x_{1:N}) = \\psi(\\ell(\\cdot, x), \\pi) \\alpha \\exp\\{-\\sum_{n=1}^N\\ell(\\theta, x_N)\\}\\pi(\\theta)<br />$$</p><p>Just like Bayes rule we have a normalized product of the prior with something that takes the role of the likelihood term. If the loss is the logarithmic loss of a probabilistic model, we recover the Bayes rule, but this update rule makes sense for arbitrary loss functions.</p><p>Again, this solution is unique under the very strong and specific desideratum that we&apos;d like the losses from i.i.d. observations combine in an additive way, and I presume that, had we chosen a different permutation invariant function, we would end up with a similar generalization of Bayes rule with that permutation invariant function appearing in the exponent.</p><h3 id=\"rationality\">Rationality</h3><p>Now that we have an update rule which satisfies our desiderata, can we say if it&apos;s actually a good or useful update rule? It seems it is, in the following sense.</p><p>Let&apos;s think about a way to measure the usefulness of a posterior $\\nu$. Suppose we have data sampling distribution $P$, losses are still measured by $\\ell$, and our prior is $\\pi$. A good posterior does two things well: it allows us to make good decisions in some kind of downstream test scenario, and it is informed by our prior. It therefore makes sense to define a loss function over the posterior $\\nu$ as a sum of two terms:</p><p>$$<br />L(\\nu; \\ell, \\pi, P) = h_1(\\nu; \\ell, P) + h_2(\\nu; \\pi)<br />$$</p><p>The first term, $h_1$ measures the posterior&apos;s usefulness at test time, and $h_2$ measures how well it&apos;s influenced by the prior. The authors define $h_1$ to be as follows:</p><p>$h_1(\\nu; \\ell, P) = \\mathbb{E}_{x\\sim P} \\mathbb{E}_\\theta\\sim\\nu \\ell(x, \\theta)$</p><p>So basically, we will sample from the posterior, and then evaluate the random sample parameter $\\theta$ on a randomly chosen test datapoint $x$ using our loss $\\ell$. I would say this is a rather narrow view on what it means for a posterior to do well on a downstream task, more about it later in the criticism section. In any case it&apos;s one possible goal for a posterior to try to achieve.</p><p>Now we turn to choosing $h_2$, and the authors note something very interesting. If we want the resulting optimal posterior to possess the coherence property (as defined in their Eqn. (3)), it turns out the only choice for $h_2$ is the KL divergence between the prior and posterior. Any other choice would lead to incoherent updates. This, I believe is only true for the additive definition of coherence, not the more general definition I gave above.</p><p>Putting $h_1$ and $h_2$ together it turns out that the posterior that minimizes this loss function is precisely of the form $\\pi(\\theta\\vert x_{1:N}) \\alpha \\exp\\{-\\sum_{n=1}^N \\ell(\\theta, x_n)\\}$. So, not only is this update rule the only update rule that satisfies the desired properties, it is also optimal under this particular definition of optimality/rationality.</p><h2 id=\"why-is-this-significant\">Why is this significant?</h2><p>This work is interesting because it gives a new justification for Bayes rule-like updates to belief distributions, and as a result it also provides a different/new perspective on Bayesian inference. Crucially, never in this derivation did we have to reason about a joint distribution between $\\theta$ and the observations $x$ (or conditionals of one given the other). Even though I wrote $\\pi(\\theta \\vert x_{1:N})$ to denote a posterior, this is really just a shorthand notation, syntactic sugar. This is important. One of the main technical criticisms of the Bayesian terminology is that in order to reason about the joint distribution between two random variables ($x$ and $\\theta$), these variables have to live in the same probability space, so if you believe that your data exists, you have to believe in your model, and model parameters exist as well. This framework sidesteps that.</p><blockquote>It allows rational updates of belief distributions, without forcing you to believe in anything.</blockquote><p>From a practical viewpoint, this work also extends Bayesian inference in a meaningful way. While Bayesian inference only made sense if you inferred the whole set of parameters jointly, here you are allowed to specify any loss function, and really focus on the parameter of importance. For example, if you&apos;re only interested in estimating the median of a distribution in a Bayesian way, without assuming it follows a certain distribution, you can now do this by specifying your loss to be $\\vert x-\\theta\\vert$. This is explained a lot more clearly in the paper, so I encourage you to read it.</p><h2 id=\"criticism\">Criticism</h2><p>My main criticism of this work is that it made a number of assumptions that ultimately limited the range of acceptable solutions, and to my cynical eye it appears that these choices were specifically made so that Bayes rule-like update rules came out winning. So rather than really deriving Bayesian updates from first principles, we engineered principles under which Bayesian updates are optimal. In other words, the top-down analysis was rigged in favour of familiar Bayes-like updates. There are two specific assumptions which I would personally like to see relaxed:</p><p>The first one is the restrictive notion of coherence, which requires losses to combine additively from multiple observations. I think this very clearly gives rise to the convenient exponential, log-additive form in the end. It would be interesting to see whether other types of permutation invariant update rules also make sense in practice.</p><p>Secondly, the way the authors defined optimality, in terms of the loss $h_1$ above is very limiting. We rarely use posterior distributions in this way (take a random sample). Instead, we might be intersested integrating over the posterior, and evaluating the loss of that classifier. This is a loss that cannot be written in the bilinear form that is the formula for $h_1$ above. I wonder if. using more elaborate losses for the posterior, perhaps along the lines of general decision problems as in (<a href=\"http://proceedings.mlr.press/v15/lacoste_julien11a.html?ref=inference.vc\">Lacoste-Julien et al, 2011)</a>, could lead to more interesting update rules which don&apos;t look at all like Bayes rule but are still rational.</p>"
        }
      ]
    }
  ]
}