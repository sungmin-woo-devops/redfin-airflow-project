<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research Blog - Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/blog/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/blog/</link>
	<description></description>
	<lastBuildDate>Thu, 21 Aug 2025 15:44:00 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.8.2</generator>
	<item>
		<title>Applicability vs. job displacement: further notes on our recent research on AI and occupations</title>
		<link>https://www.microsoft.com/en-us/research/blog/applicability-vs-job-displacement-further-notes-on-our-recent-research-on-ai-and-occupations/</link>
		
		<dc:creator><![CDATA[Kiran Tomlinson, Sonia Jaffe, Will Wang, Scott Counts, Siddharth Suri]]></dc:creator>
		<pubDate>Thu, 21 Aug 2025 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1148193</guid>

					<description><![CDATA[<p>Recently, we released a paper Working with AI: Measuring the Occupational Implications of Generative AI that studied what occupations might find AI chatbots useful, and to what degree. The paper sparked significant discussion, which is no surprise since people care deeply about the future of AI and jobs--that’s part of why we think it’s important to study these topics.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/applicability-vs-job-displacement-further-notes-on-our-recent-research-on-ai-and-occupations/">Applicability vs. job displacement: further notes on our recent research on AI and occupations</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1.jpg" alt="Three white icons on a gradient background transitioning from blue to green. From left to right: a network structure with connected circles, an upward-trending line graph with bars and an arrow, and a checklist with horizontal lines and checkmarks." class="wp-image-1148296" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>Recently, we released a paper&nbsp;(<em><a href="https://www.microsoft.com/en-us/research/publication/working-with-ai-measuring-the-occupational-implications-of-generative-ai/">Working with AI: Measuring the Occupational Implications of Generative AI</a></em>)&nbsp;that studied what occupations might&nbsp;find&nbsp;AI chatbots&nbsp;useful, and to what degree.&nbsp;The paper sparked significant discussion,&nbsp;which is no&nbsp;surprise&nbsp;since&nbsp;people care&nbsp;deeply&nbsp;about&nbsp;the future of AI and&nbsp;jobs&#8211;that’s part of why we think&nbsp;it’s&nbsp;important to study these&nbsp;topics.</p>



<p>Unfortunately, not all the&nbsp;discussion&nbsp;was&nbsp;accurate&nbsp;in its portrayal of the&nbsp;study’s scope or conclusions.&nbsp;Specifically, our&nbsp;study&nbsp;does not&nbsp;draw any conclusions about jobs being eliminated; in the paper,&nbsp;we&nbsp;explicitly&nbsp;cautioned&nbsp;against using our findings to make that conclusion.&nbsp;</p>



<p>Given the importance&nbsp;of this&nbsp;topic, we&nbsp;want&nbsp;to&nbsp;clarify any misunderstandings and&nbsp;provide&nbsp;a more digestible summary of the paper,&nbsp;our&nbsp;methodology,&nbsp;and its limitations.&nbsp;</p>



<h2 class="wp-block-heading" id="what-did-our-research-find">What&nbsp;did our research find?</h2>



<p>We set out to better understand how people are using AI, <strong>highlighting where AI might be useful in different occupations</strong>. To do this, we analyzed how people currently use generative AI—specifically Microsoft Bing Copilot (now Microsoft Copilot)—to assist with tasks. We then compared these sets of tasks against the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.onetcenter.org/overview.html" target="_blank" rel="noreferrer noopener">O*NET database<span class="sr-only"> (opens in new tab)</span></a>, a widely used occupational classification system, to understand potential applicability to various occupations.</p>



<p>We found&nbsp;that AI&nbsp;is most&nbsp;useful&nbsp;for&nbsp;tasks related to knowledge work and communication, particularly tasks such as writing, gathering information, and learning.</p>



<p>Those in occupations with these tasks&nbsp;may benefit by&nbsp;considering&nbsp;how AI&nbsp;can be used&nbsp;as a tool to help improve their workflows. On the&nbsp;flip side,&nbsp;it’s&nbsp;not surprising that physical tasks like performing surgeries or moving objects had less&nbsp;direct&nbsp;AI&nbsp;chatbot applicability.</p>



<p>So, to summarize, our paper is about&nbsp;identifying&nbsp;the occupations where&nbsp;AI may be most useful,&nbsp;by&nbsp;assisting&nbsp;or performing subtasks.&nbsp;&nbsp;Our data do&nbsp;not&nbsp;indicate, nor&nbsp;did&nbsp;we&nbsp;suggest, that certain jobs will be replaced by AI.</p>



<h2 class="wp-block-heading" id="methodological-limitations-are-acknowledged-and-important">Methodological limitations are acknowledged—and important</h2>



<p>The paper is transparent about the limitations of our approach.&nbsp;&nbsp;</p>



<p>We analyzed&nbsp;anonymized&nbsp;Bing Copilot conversations to see what&nbsp;activities&nbsp;users are seeking AI&nbsp;assistance&nbsp;with and what activities AI can perform when mapped to the O*NET database.&nbsp;While O*NET provides a structured list of&nbsp;activities&nbsp;associated with various occupations, it does&nbsp;<strong>not</strong>&nbsp;capture the full spectrum of skills, context, and nuance&nbsp;required&nbsp;in the real&nbsp;world.&nbsp;&nbsp;<strong>A job is far more than the collection of tasks that make&nbsp;it up.</strong></p>



<p>For example, a task might involve “writing reports,” but O*NET&nbsp;won’t&nbsp;reflect the interpersonal judgment, domain&nbsp;expertise, or ethical considerations that go into doing that well. The paper acknowledges this gap and warns against over-interpreting the AI applicability scores as measures of AI’s ability to perform an occupation.</p>



<p>Additionally, the dataset is based on user queries from Bing Copilot (from January – September 2024), which may be influenced by factors like awareness, access, or comfort with AI tools.&nbsp;&nbsp;Different people use different LLMs for different purposes and it also is&nbsp;very difficult&nbsp;(or&nbsp;nearly impossible) to&nbsp;determine&nbsp;what conversations are performed in a work context or for leisure.&nbsp;</p>



<p>Finally, we only evaluated AI chatbot usage, so this study does not evaluate the impact or applicability of other forms of AI.</p>



<h2 class="wp-block-heading" id="where-do-we-go-from-here-1">Where do we go from here?</h2>



<p>Given the intense interest in how AI will shape our collective future,&nbsp;it&#8217;s&nbsp;important we continue to study and better understand its societal and economic impact. As with&nbsp;all&nbsp;research on this topic,&nbsp;the findings&nbsp;are&nbsp;nuanced, and&nbsp;it’s&nbsp;important to pay attention to this nuance.&nbsp;</p>



<p>The public interest in our research is based, in large part, on the&nbsp;topic&nbsp;of AI&nbsp;and job displacement.&nbsp;However,&nbsp;our current&nbsp;methodology&nbsp;for this study&nbsp;is unlikely to lead to firm conclusions about this.&nbsp;&nbsp;AI may prove to be a useful tool for many occupations, and we believe the right balance lies in finding how to use the technology in a way that&nbsp;leverages&nbsp;its abilities while complementing human strengths and accounting for people&#8217;s preferences.&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p>For more information from Microsoft on the future of work and AI skilling, check out Microsoft’s Annual&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/annual-work-trend-index-2025/" target="_blank" rel="noreferrer noopener">Work Trend Index<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://blogs.microsoft.com/on-the-issues/2025/07/09/elevate/" target="_blank" rel="noreferrer noopener">Microsoft Elevate<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/applicability-vs-job-displacement-further-notes-on-our-recent-research-on-ai-and-occupations/">Applicability vs. job displacement: further notes on our recent research on AI and occupations</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation</title>
		<link>https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/</link>
		
		<dc:creator><![CDATA[Yuncong Yang, Reuben Tan, Swadheen Shukla, Jianfeng Gao]]></dc:creator>
		<pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1147961</guid>

					<description><![CDATA[<p>MindJourney can enable AI to navigate and interpret 3D environments from limited visual input, potentially improving performance in navigation, planning, and safety-critical tasks.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/">MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg" alt="Three white line icons on a gradient background transitioning from blue to pink. From left to right: a network or molecule structure with a central circle and six surrounding nodes, a 3D cube, and an open laptop with an eye symbol above it." class="wp-image-1147994" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>A new research framework helps AI agents explore three-dimensional spaces they can’t directly detect. Called <a href="https://www.microsoft.com/en-us/research/publication/mindjourney-test-time-scaling-with-world-models-for-spatial-reasoning/" target="_blank" rel="noreferrer noopener">MindJourney</a>, the approach addresses a key limitation in vision-language models (VLMs), which give AI agents their ability to interpret and describe visual scenes.  </p>



<p>While VLMs&nbsp;are strong&nbsp;at identifying objects in&nbsp;static&nbsp;images,&nbsp;they struggle to&nbsp;interpret&nbsp;the interactive 3D world behind 2D images.&nbsp;This&nbsp;gap shows up&nbsp;in spatial&nbsp;questions&nbsp;like&nbsp;“If I sit on the couch&nbsp;that is on my right&nbsp;and face the chairs, will the kitchen be to my right or left?”—tasks that require an agent to&nbsp;interpret&nbsp;its&nbsp;position and movement through space.&nbsp;</p>



<p>People&nbsp;overcome this challenge by mentally exploring a space,&nbsp;imagining moving through it and combining those mental snapshots to work out where objects are.&nbsp;MindJourney&nbsp;applies the same process&nbsp;to&nbsp;AI agents,&nbsp;letting&nbsp;them roam a virtual&nbsp;space before answering spatial questions.&nbsp;</p>



<h2 class="wp-block-heading" id="how-mindjourney-navigates-3d-space">How&nbsp;MindJourney&nbsp;navigates 3D space</h2>



<p>To perform this type of spatial navigation,&nbsp;MindJourney&nbsp;uses a&nbsp;<em>world model</em>—in this case,&nbsp;a video generation system trained on a large collection of videos captured from a single moving viewpoint, showing actions such as going forward and turning left of right, much like a 3D cinematographer. From this, it learns to predict how a new scene would appear from different perspectives.</p>



<p>At inference time, the model can generate photo-realistic images of a scene based on possible movements from the agent’s current position. It generates multiple possible views of a scene while the VLM acts as a filter, selecting the constructed perspectives that are most likely to answer the user&#8217;s question.</p>



<p>These are kept and expanded in the next iteration, while less promising paths are discarded. This process, shown in Figure 1, avoids the need to generate and evaluate thousands of possible movement sequences by focusing only on the most informative perspectives.</p>



<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1400" height="854" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg" alt="Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM's spatial interpretation through generated observations when encountering a new  challenges. " class="wp-image-1147968" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-300x183.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-1024x625.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-768x468.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-240x146.jpg 240w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM&#8217;s spatial interpretation through generated observations when encountering new challenges.<em>&nbsp;</em></figcaption></figure>



<p>&nbsp;</p>



<p>To make its search through a simulated space both effective and efficient, MindJourney uses a <em>spatial beam search</em>—an algorithm that prioritizes the most promising paths. It works within a fixed number of steps, each representing a movement. By balancing breadth with depth, spatial beam search enables MindJourney to gather strong supporting evidence. This process is illustrated in Figure 2.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg" alt="MindJourney pipeline diagram" class="wp-image-1147897" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 2. The MindJourney workflow starts with a spatial beam search for a set number of steps before answering the query. The world model interactively generates new observations, while a VLM interprets the generated images, guiding the search throughout the process.</figcaption></figure>



<p class="has-text-align-left">By iterating through simulation, evaluation, and integration, MindJourney can reason about spatial relationships far beyond what any single 2D image can convey, all without the need for additional training. On the Spatial Aptitude Training (SAT) benchmark, it improved the accuracy of VLMs by 8% over their baseline performance.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p id="microsoft-research-newsletter" class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-describedby="microsoft-research-newsletter" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="MindJourney: Test-Time Scaling with World Models for Spatial Reasoning" width="500" height="375" src="https://www.youtube-nocookie.com/embed/Z4-5NZmdV44?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<h2 class="wp-block-heading" id="building-smarter-agents">Building&nbsp;smarter agents&nbsp;&nbsp;</h2>



<p>MindJourney&nbsp;showed&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/mindjourney-test-time-scaling-with-world-models-for-spatial-reasoning/" target="_blank" rel="noreferrer noopener">strong performance</a>&nbsp;on multiple 3D spatial-reasoning benchmarks, and even advanced VLMs&nbsp;improved&nbsp;when paired with its imagination loop. This suggests that the spatial patterns that world models learn from raw images, combined with the symbolic&nbsp;capabilities&nbsp;of VLMs, create a more complete spatial capability&nbsp;for agents. Together, they enable agents to infer what lies beyond the visible frame and&nbsp;interpret&nbsp;the physical world&nbsp;more accurately.&nbsp;</p>



<p>It also demonstrates that pretrained VLMs and trainable world models can work together in 3D without retraining either one—pointing toward general-purpose agents capable of interpreting and acting in real-world environments. This opens the way to possible applications in autonomous robotics, smart home technologies, and accessibility tools for people with visual impairments. </p>



<p>By converting systems that simply describe static images into active agents that continually evaluate where to look next,&nbsp;MindJourney&nbsp;connects computer vision with planning. Because exploration occurs entirely within the model’s latent space—its internal representation of the scene—robots would be able to test multiple viewpoints before determining their next move,&nbsp;potentially&nbsp;reducing wear, energy use, and collision risk.&nbsp;</p>



<p>Looking ahead, we plan to extend the framework to&nbsp;use&nbsp;world models that&nbsp;not only&nbsp;predict&nbsp;new viewpoints&nbsp;but also forecast&nbsp;how the scene might change over time.&nbsp;We envision&nbsp;MindJourney&nbsp;working&nbsp;alongside VLMs that interpret&nbsp;those predictions&nbsp;and use&nbsp;them to&nbsp;plan&nbsp;what to do&nbsp;next. This&nbsp;enhancement could enable&nbsp;agents&nbsp;more accurately&nbsp;interpret&nbsp;spatial relationships and physical dynamics, helping them to operate effectively&nbsp;in changing environments.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/">MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Dion: the distributed orthonormal update revolution is here</title>
		<link>https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/</link>
		
		<dc:creator><![CDATA[Kwangjun Ahn, John Langford]]></dc:creator>
		<pubDate>Tue, 12 Aug 2025 20:09:21 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1146334</guid>

					<description><![CDATA[<p>Dion is a new AI model optimization method that boosts scalability and performance over existing leading methods by orthonormalizing only a top rank subset of singular vectors, enabling more efficient training of large models such as LLaMA-3 with reduced overhead.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/">Dion: the distributed orthonormal update revolution is here</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2560" height="1441" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-scaled.jpg" alt="Three white icons on a gradient background transitioning from blue to green. From left to right: a network of interconnected nodes, a speedometer with the needle pointing right, and a flowchart with squares and a diamond shape." class="wp-image-1147793" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1920x1080.jpg 1920w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /></figure>



<p>Training AI models requires choosing an optimizer and for nearly a decade, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/1412.6980" target="_blank" rel="noreferrer noopener">Adam(<span class="sr-only"> (opens in new tab)</span></a>&#8211;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/1711.05101" target="_blank" rel="noreferrer noopener">W)<span class="sr-only"> (opens in new tab)</span></a> has been the optimizer of choice. Given that durability and success, it was fair to doubt that any further improvement was possible. And yet, last December, a new optimizer called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://kellerjordan.github.io/posts/muon/" target="_blank" rel="noreferrer noopener">Muon<span class="sr-only"> (opens in new tab)</span></a> showed serious promise by powering a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/KellerJordan/modded-nanogpt/tree/master" target="_blank" rel="noreferrer noopener">nanoGPT speedrun<span class="sr-only"> (opens in new tab)</span></a>. This proved out, with multiple AI labs (e.g., <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2502.16982" target="_blank" rel="noreferrer noopener">Kimi-AI<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2505.02222" target="_blank" rel="noreferrer noopener">Essential-AI<span class="sr-only"> (opens in new tab)</span></a>) reporting 2x scale improvements and the release of the 1T parameter <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://moonshotai.github.io/Kimi-K2/" target="_blank" rel="noreferrer noopener">Kimi K2<span class="sr-only"> (opens in new tab)</span></a> model.&nbsp;Restated: you can train a model to similar performance with half as many GPUs.</p>



<p>There’s one fly in the ointment: Muon requires large matrix multiplications in the optimizer, which requires heavy communication in large models at the scale where <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2304.11277" target="_blank" rel="noreferrer noopener">FSDP</a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noreferrer noopener">TP</a> parallelization becomes desirable.&nbsp;Going back to the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jeremybernste.in/writing/deriving-muon" target="_blank" rel="noreferrer noopener">inspiration for Muon,</a> the key idea is an orthonormal update, which sparked the search&nbsp;for more scalable alternative linear algebras realizing the same goal. That’s exactly what <a href="https://www.microsoft.com/en-us/research/publication/dion-distributed-orthonormalized-updates/" target="_blank" rel="noreferrer noopener">Dion</a> is. We have open-sourced this new optimizer to enable anyone to train large models more efficiently at scale. &nbsp;</p>



<h2 class="wp-block-heading" id="what-s-an-orthonormal-update">What’s an orthonormal update?</h2>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1422" height="515" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion.png" alt="Illustration of matrix parameters" class="wp-image-1146808" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion.png 1422w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion-300x109.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion-1024x371.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion-768x278.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion-240x87.png 240w" sizes="auto, (max-width: 1422px) 100vw, 1422px" /><figcaption class="wp-element-caption">Figure1. Illustration of matrix parameters</figcaption></figure>



<p>At the core of Transformers, a set of input activations is multiplied by a learned weight matrix to produce a new set of output activations. When the weight matrix is updated during training, the resulting change in the output activations generally depends on the direction of the input activations. As a result, the learning rate must be chosen conservatively to accommodate the input direction that induces the largest change. Orthonormalized updates alter this behavior by (approximately) making the change in output activations invariant to the direction of the input. This is achieved by enforcing <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://en.wikipedia.org/wiki/Orthonormality" target="_blank" rel="noreferrer noopener">orthonormality<span class="sr-only"> (opens in new tab)</span></a> on the update matrix, thereby equalizing its effect across all input directions.</p>



<h2 class="wp-block-heading" id="what-is-dion">What is Dion?</h2>



<p>While Muon has shown strong empirical results, scaling it to very large models poses challenges. As reported by <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.essential.ai/blog/infra" target="_blank" rel="noreferrer noopener">Essential AI<span class="sr-only"> (opens in new tab)</span></a>, applying Muon to large architectures like LLaMA-3 becomes <em>compute-bound</em>—and potentially <em>communication-bound</em>—due to the cost of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://docs.modula.systems/algorithms/newton-schulz/" target="_blank" rel="noreferrer noopener">Newton–Schulz orthonormalization steps<span class="sr-only"> (opens in new tab)</span></a>.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1685" height="857" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion.png" alt="Pseudocode of the centralized version of Dion" class="wp-image-1146810" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion.png 1685w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-300x153.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-1024x521.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-768x391.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-1536x781.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-240x122.png 240w" sizes="auto, (max-width: 1685px) 100vw, 1685px" /><figcaption class="wp-element-caption">Figure 2. Pseudocode of the centralized version of Dion</figcaption></figure>



<p>This is where <strong>Dion</strong> enters. At a high level, Dion introduces a new axis for scalability: the <strong>rank</strong>. Specifically, for a given rank r, Dion orthonormalizes only the top r of the singular vector space, reducing communication and compute overhead while preserving performance.&nbsp;Empirically, we observe that the necessary rank for good performance grows much more slowly than the number of parameters in larger models.</p>



<div class="annotations " data-bi-aN="margin-callout">
	<article class="annotations__list card depth-16 bg-body p-4 annotations__list--right">
		<div class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Download</span>
			<a href="https://github.com/microsoft/dion/" target="_blank" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="Dion optimizer" data-bi-aN="margin-callout" data-bi-cN="Dion optimizer">
				Dion optimizer&nbsp;<span class="glyph-append glyph-append-share glyph-append-xsmall"></span>
			</a>
					</div>
	</article>
</div>



<p>Dion implements orthonormalization using <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/1905.13727" target="_blank" rel="noreferrer noopener"><em>amortized power iteration</em><span class="sr-only"> (opens in new tab)</span></a><em>.&nbsp;</em>Power iteration typically pulls out the largest singular value by repeated matrix multiplication.&nbsp;By amortizing this process over optimization steps—applied to the slowly-evolving momentum matrix—we reduce the cost to just two matrix multiplications per step. Incorporating a QR decomposition allows us to extract an approximate orthonormal basis spanning the top singular directions, rather than just the leading one.&nbsp;This amortized power iteration is fully compatible with standard distributed training techniques such as <strong>FSDP</strong> and <strong>tensor parallelism</strong>.&nbsp;Here, we show a simple centralized version, but the technique works for more complex forms of parallelization as presented in the paper. In other words, we can orthogonalize a matrix <em>without ever seeing a full row or column of it</em>.&nbsp;</p>



<p>Low-rank approximation would ordinarily introduce error, but Dion overcomes this through an error feedback mechanism. This keeps the residual of low rank approximation in the momentum matrix so that any systematic gradient structure not initially captured accumulates to eventually be applied in a future update.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144028">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-label="The AI Revolution in Medicine, Revisited" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Episode7-PeterBillSebastien-AIRevolution_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshot of Bill Gates, Peter Lee, and Sébastien Bubeck" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">The AI Revolution in Medicine, Revisited</h2>
				
								<p id="the-ai-revolution-in-medicine-revisited" class="large">Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-describedby="the-ai-revolution-in-medicine-revisited" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="how-does-it-work">How does it work?</h2>



<p>Something very strange happened in our experiments. Usually, adding an extra constraint on the way an algorithm works can be expected to <em>decrease</em> overall performance. And indeed, at the 120M parameter scale of the speedrun, we see Dion’s update taking more time than Muon, while not yielding any significant gains. But at larger scales, we observed a different trend: Dion began to outperform Muon.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="699" height="414" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure-3_Dion.png" alt="Wall-clock time speedup of Dion for 3B model training" class="wp-image-1146815" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure-3_Dion.png 699w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure-3_Dion-300x178.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure-3_Dion-240x142.png 240w" sizes="auto, (max-width: 699px) 100vw, 699px" /><figcaption class="wp-element-caption">Figure 3. Wall-clock time speedup of Dion for 3B model training</figcaption></figure>



<p>Why would adding a constraint <em>improve</em> the update rule? The answer lies in what the constraint enforces. Dion achieves a much closer approximation to true orthonormalization than Muon. This precision, initially subtle, becomes increasingly important as the number of singular vectors grows. Over increasing model scale and training steps, this small advantage accumulates—leading to a measurable improvement in performance.</p>



<p>This edge further grows with batch size—with larger batches the update quality tends to degrade, but notably more slowly with Dion than Muon (and Muon is already a significant improvement over AdamW).</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="786" height="637" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion.png" alt="Scaling of Dion across different batch sizes" class="wp-image-1146818" style="width:596px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion.png 786w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion-300x243.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion-768x622.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion-222x180.png 222w" sizes="auto, (max-width: 786px) 100vw, 786px" /><figcaption class="wp-element-caption">Figure 4. Scaling of Dion across different batch sizes</figcaption></figure>



<p>Here you can see how the number of steps to reach a pretraining loss compared to AdamW varies as batch size grows with full rank and ¼ rank Dion (in orange) and Muon (in blue).&nbsp;&nbsp;&nbsp;</p>



<p>In our experiments, these benefits extend to various post-training regimes as well.</p>



<p>We also experimented with rank, discovering empirically that larger models tolerate smaller rank well.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1893" height="511" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion.png" alt="Low-rank Dion across different model sizes" class="wp-image-1146821" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion.png 1893w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-300x81.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-1024x276.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-768x207.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-1536x415.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-240x65.png 240w" sizes="auto, (max-width: 1893px) 100vw, 1893px" /><figcaption class="wp-element-caption">Figure 5. Low-rank Dion across different model sizes</figcaption></figure>



<p>Projecting this trend out to the scale of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2407.21783" target="_blank" rel="noreferrer noopener">LLaMA-3<span class="sr-only"> (opens in new tab)</span></a> 405B parameter models suggests that Dion is fully effective even with <strong>rank fractions as low as 1/16 or 1/64</strong> for large dense models like LLaMA-3.&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p>Using hardware timings of the individual update steps suggests a story that looks this:</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1075" height="645" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6.png" alt="Estimated wall-clock time of each optimizer step for Llama 3 405B. Lower is better. Muon is highlighted in orange as our baseline, next to Dion with varying rank fractions. Suggested rank fractions for a 405B parameter model are shown in blue. Using Dion with rank fraction 1/16 or lower offers an order-of-magnitude speedup over Muon." class="wp-image-1147684" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6.png 1075w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6-300x180.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6-1024x614.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6-768x461.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6-240x144.png 240w" sizes="auto, (max-width: 1075px) 100vw, 1075px" /><figcaption class="wp-element-caption">Figure 6. Estimated wall-clock time of each optimizer step for Llama 3 405B. Lower is better. Muon is highlighted in orange as our baseline, next to Dion with varying rank fractions. Suggested rank fractions for a 405B parameter model are shown in blue. Using Dion with rank fraction 1/16 or lower offers an order-of-magnitude speedup over Muon.</figcaption></figure>



<p>We’ve open-sourced a PyTorch FSDP2 + Tensor Parallel (TP) implementation of <strong>Dion</strong>, available via a simple pip install. Our goal is to make faster training with Dion accessible to everyone. As a bonus, the repository also includes a PyTorch FSDP2 implementation of <strong>Muon.</strong></p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-fill-github"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/dion/">Dion optimizer</a></div>
</div>



<h2 class="wp-block-heading" id="acknowledgements">Acknowledgements</h2>



<p>We thank Riashat Islam and Pratyusha Sharma for their helpful feedback on the writing and presentation.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/">Dion: the distributed orthonormal update revolution is here</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Self-adaptive reasoning for science</title>
		<link>https://www.microsoft.com/en-us/research/blog/self-adaptive-reasoning-for-science/</link>
		
		<dc:creator><![CDATA[Newman Cheng, Gordon Broadbent, Steven Truitt, William Chappell]]></dc:creator>
		<pubDate>Wed, 06 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1145083</guid>

					<description><![CDATA[<p>Microsoft is pioneering a vision for a self-adapting AI system that can adapt to the dynamic nature of scientific discovery, promoting deeper, more refined reasoning in complex scientific domains.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/self-adaptive-reasoning-for-science/">Self-adaptive reasoning for science</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1.jpg" alt="A gradient background transitioning from blue to pink with three white icons: a DNA double helix, a light bulb with rays, and a stylized path with arrows and nodes." class="wp-image-1146704" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<h2 class="wp-block-heading" id="unlocking-self-adaptive-cognitive-behavior-that-is-more-controllable-and-explainable-than-reasoning-models-in-challenging-scientific-domains">Unlocking self-adaptive cognitive behavior that is more controllable and explainable than reasoning models in challenging scientific domains</h2>



<p>Long-running LLM agents equipped with strong reasoning, planning, and execution skills have the potential to transform scientific discovery with high-impact advancements, such as developing new materials or pharmaceuticals. As these agents become more autonomous, ensuring effective human oversight and clear accountability becomes increasingly important, presenting challenges that must be addressed to unlock their full transformative power. Today’s approaches to long-term reasoning are established during the post-training phase, prior to end-user deployment and typically by the model provider. As a result, the expected actions of these agents are pre-baked by the model developer, offering little to no control from the end user.</p>



<p>At Microsoft, we are pioneering a vision for a continually steerable virtual scientist. In line with this vision, we created the ability to have a non-reasoning model develop thought patterns that allow for control and customizability by scientists. Our approach, a cognitive loop via in-situ optimization (CLIO), does not rely on reinforcement learning post-training to develop reasoning patterns yet still yields equivalent performance as demonstrated through our evaluation on Humanity’s Last Exam (HLE). Notably, we increased OpenAI GPT-4.1’s base model accuracy on text-only biology and medicine from <strong>8.55%</strong> to <strong>22.37%</strong>, an absolute increase of<strong> 13.82%</strong> (<strong>161.64% </strong>relative), surpassing o3 (high). This demonstrates that an optimization-based, self-adaptive AI system developed without further post-training can rival post-trained models in domains where adaptability, explainability, and control matter most.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1911" height="1070" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Figure-1_Updated.png" alt="Bar chart that represents the Head-to-head comparison of OpenAI’s GPT-4.1 with CLIO, o3, and GPT-4.1 with no tools on HLE biology and medicine questions" class="wp-image-1146748" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Figure-1_Updated.png 1911w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Figure-1_Updated-300x168.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Figure-1_Updated-1024x573.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Figure-1_Updated-768x430.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Figure-1_Updated-1536x860.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Figure-1_Updated-655x368.png 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Figure-1_Updated-240x134.png 240w" sizes="auto, (max-width: 1911px) 100vw, 1911px" /><figcaption class="wp-element-caption">Figure 1. Head-to-head comparison of OpenAI’s GPT-4.1 with CLIO, o3, and GPT-4.1 with no tools on HLE biology and medicine questions</figcaption></figure>



<h3 class="wp-block-heading" id="in-situ-optimization-with-internal-self-reflection-to-enable-self-adaptive-reasoning">In-situ optimization with internal self-reflection to enable self-adaptive reasoning</h3>



<p>Model development has advanced from using reinforcement learning human feedback (RLHF) for answer alignment to external grading in reinforcement learning (RLVR). Recent approaches show promise in the utilization of intrinsic rewards for training reasoning models (RLIR). Traditionally, these reasoning processes are learned during the post-training process before any user interaction. While today’s reasoning models require additional data in the training phase and limit user control during the reasoning generation process, CLIO’s approach enables users to steer reasoning from scratch without additional data. Rather, CLIO generates its own necessary data by creating reflection loops at runtime. These reflection loops are utilized for a wide array of activities that CLIO self-defines, encompassing idea exploration, memory management, and behavior control. Most interesting is CLIO’s ability to leverage prior inferences to adjust future behaviors, handling uncertainties and raising flags for correction when necessary. Through this open architecture approach to reasoning, we alleviate the necessity for further model post-training to achieve desired reasoning behavior. Performing novel scientific discoveries often has no prior established patterns for reasoning, much less a large enough corpus of high-quality data to train on.&nbsp;</p>



<p>CLIO reasons by continuously reflecting on progress, generating hypotheses, and evaluating multiple discovery strategies. For the HLE test, CLIO was specifically steered to follow the scientific method as a guiding framework. Our research shows that equipping language models with self-adapting reasoning enhances their problem-solving ability. It provides a net benefit in quality for science questions, as well as providing exposure and control to the end user.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1169" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_FIG2-2-scaled.png" alt="Figure 2. CLIO can raise key areas of uncertainty within its self-formulated reasoning process, balancing multiple different viewpoints using graph structures." class="wp-image-1146740" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_FIG2-2-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_FIG2-2-300x137.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_FIG2-2-1024x468.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_FIG2-2-768x351.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_FIG2-2-1536x702.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_FIG2-2-2048x935.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_FIG2-2-240x110.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 2. CLIO can raise key areas of uncertainty within its self-formulated reasoning process, balancing multiple different viewpoints using graph structures.</figcaption></figure>



<h3 class="wp-block-heading" id="control-over-uncertainty-building-trust-in-ai">Control over uncertainty: Building trust in AI&nbsp;</h3>



<p>Orchestrated reasoning systems like CLIO are valuable for scientific discovery, as they provide features beyond accuracy alone. Capabilities such as explaining the outcomes of internal reasoning are standard in the scientific field and are present in current reasoning model approaches. However, elements like displaying complete work, including final outcomes, internal thought processes, and uncertainty thresholds to support reproducibility or correction, as well as indicating uncertainty, are not yet universally implemented. Current models and systems do not have this same innate humility.&nbsp; Rather, we are left with models that produce confident results, whether correct or incorrect. When correct, it is valuable. When incorrect, it is dangerous to the scientific process. Hence, understanding a model or system’s uncertainty is a crucial aspect that we have developed natively into CLIO.</p>



<p>On the other end of the spectrum, orchestrated reasoning systems tend to oversaturate the user by raising too many flags. We enable prompt-free control knobs within CLIO to set thresholds for raising uncertainty flags. This allows CLIO to flag uncertainty for itself and the end user at the proper point in time. This also enables scientists to revisit CLIO’s reasoning path with critiques, edit beliefs during the reasoning process, and re-execute them from the desired point in time. Ultimately, this builds a foundational level of trust with scientists to use them in a scientifically defensible and rigorous way.&nbsp;</p>



<h3 class="wp-block-heading" id="how-does-clio-perform">How does&nbsp;CLIO&nbsp;perform?&nbsp;</h3>



<p>We evaluate CLIO against text-based biology and medicine questions from HLE. For this domain, we demonstrate a <strong>61.98%</strong> relative increase or an <strong>8.56%</strong> net increase<strong> </strong>in accuracy over OpenAI’s o3 and substantially outperform base completion models like OpenAI’s GPT-4.1, while enabling the requisite explainability and control. This technique applies to all models, showing similar increases in OpenAI’s GPT-4o model, which we observe performs poorly on HLE-level questions. On average, GPT-4.1 is not considered competent for HLE scale questions (<9%), and GPT-4o is natively at less than 2%. By utilizing CLIO, we bring these to near state-of-the-art performance against top reasoning models. CLIO&#8217;s recursive nature enables the system to think broader and more deeply, ensuring coverage of the question when answered. In GPT-4.1, we see an increase of 5.92% in accuracy for overall performance using just the cognitive loop recursion. To think more deeply, we allow CLIO to ensemble different evolutions and intelligently choose from the best approach using <a href="https://www.microsoft.com/en-us/research/project/graphrag/" target="_blank" rel="noreferrer noopener">GraphRAG</a>. This extension of the cognition pattern provides a further 7.90% over a non-ensembled approach. &nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1915" height="1074" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Fig3.png" alt="Waterfall chart that demonstrates the impact of thinking effort on CLIO’s effectiveness." class="wp-image-1146714" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Fig3.png 1915w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Fig3-300x168.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Fig3-1024x574.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Fig3-768x431.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Fig3-1536x861.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Fig3-655x368.png 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Fig3-240x135.png 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MDQ_Fig3-640x360.png 640w" sizes="auto, (max-width: 1915px) 100vw, 1915px" /><figcaption class="wp-element-caption">Figure 3. The impact of thinking effort on CLIO’s effectiveness.</figcaption></figure>



<p>Furthermore, CLIO’s design offers different knobs of control, for example, how much time to think and which technique to utilize for a given problem. In Figure 3, we demonstrate these knobs of control and their increase on GPT-4.1 and GPT-4o&#8217;s performance. In this case, we analyze performance for a subset of biomedical questions, those focused on immunology. CLIO increases GPT-4o&#8217;s base performance to be at par with the best reasoning models for immunology questions. We observe a <strong>13.60%</strong> improvement over the base model, GPT-4o. This result shows CLIO to be model agnostic, similar to <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.ai/new/the-path-to-medical-superintelligence/" target="_blank" rel="noreferrer noopener">Microsoft AI Diagnostic Orchestrator&#8217;s (MAI-DxO)<span class="sr-only"> (opens in new tab)</span></a>&#8216;s approach and corresponding performance boost.&nbsp;</p>



<h3 class="wp-block-heading" id="implications-for-science-and-trustworthy-discovery">Implications for science and trustworthy discovery</h3>



<p>The future of scientific discovery demands more than reasoning over knowledge and raw computational power alone. Here, we demonstrate how CLIO not only increases model performance but establishes new layers of control for scientists. In our upcoming work, we will demonstrate how CLIO increases tool utility for highly valuable scientific questions in the drug discovery space which requires precise tools designed for the language of science. While our experiments focus on scientific discovery, we believe CLIO can apply in a domain-agnostic fashion. Experts tackling problems in domains such as financial analysis, engineering, and legal services could potentially benefit from AI systems with a transparent, steerable reasoning approach. Ultimately, we envision CLIO as an enduring control-layer in hybrid AI stacks that combine traditional completion and reasoning models, with external memory systems, and advanced tool calling. These continuous checks and balances that CLIO enables will continue to remain valuable even as components within the AI stacks evolve. This combination of intelligent and steerable scientific decision making and tool optimization is the basis of the recently announced <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://azure.microsoft.com/en-us/blog/transforming-rd-with-agentic-ai-introducing-microsoft-discovery/?msockid=394581ce06c567df2171946b073d6601" target="_blank" rel="noreferrer noopener">Microsoft Discovery platform<span class="sr-only"> (opens in new tab)</span></a>.</p>



<p>At Microsoft, we’re committed to advancing AI research that earns the trust of scientists, empowering them to discover new frontiers of knowledge. Our work is a testament to what’s possible when we blend innovation with trustworthiness and a human-centered vision for the future of AI-assisted scientific discovery. We invite the research and scientific community to join us in shaping that future.</p>



<p><strong>Further information:</strong></p>



<p>To learn more details about our approach, please read our <a href="https://www.microsoft.com/en-us/research/publication/cognitive-loop-via-in-situ-optimization-self-adaptive-reasoning-for-science/">pre-print paper</a> published alongside this blog. We are in the process of submitting this work for external peer review and encourage partners to explore the utilization of CLIO in Microsoft Discovery. To learn more about Microsoft’s research on this or contact our team, please reach out to <a href="mailto:discoverylabs@microsoft.com" target="_blank" rel="noreferrer noopener">discoverylabs@microsoft.com</a>. </p>



<h2 class="wp-block-heading" id="acknowledgements">Acknowledgements</h2>



<p>We are grateful for Jason Zander and Nadia Karim’s support. We extend our thanks to colleagues both inside and outside Microsoft Discovery and Quantum for sharing their insights and feedback, including Allen Stewart, Yasser Asmi, David Marvin, Harsha Nori, Scott Lundberg, and Phil Waymouth.&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/self-adaptive-reasoning-for-science/">Self-adaptive reasoning for science</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Project Ire autonomously identifies malware at scale</title>
		<link>https://www.microsoft.com/en-us/research/blog/project-ire-autonomously-identifies-malware-at-scale/</link>
		
		<dc:creator><![CDATA[Brian Caswell, Dustin Fraze, Sarah Smith, Rodrigo Racanicci, Tim Middleton-Sally, Shelby Hayes, Stanley He, Katy Smith, Bhakta Pradhan, Mike Walker]]></dc:creator>
		<pubDate>Tue, 05 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1145100</guid>

					<description><![CDATA[<p>Designed to classify software without context, Project Ire replicates the gold standard in malware analysis through reverse engineering. It streamlines a complex, expert-driven process, making large-scale malware detection faster &#038; more consistent.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/project-ire-autonomously-identifies-malware-at-scale/">Project Ire autonomously identifies malware at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1.jpg" alt="Stylized digital illustration of a multi-layered circuit board. A glowing blue microchip sits at the top center, with intricate circuitry radiating outward. Beneath it, four stacked layers transition in color from blue to orange, each featuring circuit-like patterns. Smaller rectangular and circular components are connected around the layers, all set against a dark background with scattered geometric shapes." class="wp-image-1145541" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Today, we are excited to introduce an autonomous AI agent that can analyze and classify software without assistance, a step forward in cybersecurity and malware detection. The prototype, <a href="https://www.microsoft.com/en-us/research/project/project-ire/">Project Ire</a>, automates what is considered the gold standard in malware classification: fully reverse engineering a software file without any clues about its origin or purpose. It uses decompilers and other tools, reviews their output, and determines whether the software is malicious or benign.</p>



<p>Project Ire&nbsp;emerged&nbsp;from a collaboration&nbsp;between&nbsp;Microsoft Research, Microsoft Defender Research, and Microsoft Discovery & Quantum, bringing together security&nbsp;expertise, operational knowledge, data from global malware telemetry, and AI research. It is built on the same collaborative and agentic foundation behind&nbsp;<a href="https://www.microsoft.com/en-us/research/project/graphrag/" target="_blank" rel="noreferrer noopener">GraphRAG<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://azure.microsoft.com/en-us/blog/transforming-rd-with-agentic-ai-introducing-microsoft-discovery/#:~:text=Get%20started%20today%20by%20using%20Azure%20HPC%20and%20Azure%20AI%20Foundry%20infrastructure.&text=Microsoft%20Discovery%20is%20built%20on,make%20any%20adjustments%20as%20needed." target="_blank" rel="noreferrer noopener">Microsoft Discovery<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;The system&nbsp;uses advanced language models and a suite of callable reverse engineering and binary analysis tools to drive investigation and adjudication.</p>



<p>As of this writing, Project Ire has achieved a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank" rel="noreferrer noopener">precision<span class="sr-only"> (opens in new tab)</span></a> of 0.98 and a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank" rel="noreferrer noopener">recall<span class="sr-only"> (opens in new tab)</span></a> of 0.83 using public datasets of Windows drivers. It was the first reverse engineer at Microsoft, human or machine, to author a conviction case—a detection strong enough to justify automatic blocking—for a specific advanced persistent threat (APT) malware sample, which has since been identified and blocked by Microsoft Defender.&nbsp;</p>



<h2 class="wp-block-heading" id="malware-classification-at-a-global-scale">Malware classification at a global scale</h2>



<p>Microsoft’s Defender platform scans more than <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" target="_blank" href="https://news.microsoft.com/apac/2020/03/17/windows-10-powering-the-world-with-one-billion-monthly-active-devices/">one billion monthly<span class="sr-only"> (opens in new tab)</span></a> active devices through the company’s Defender suite of products, which routinely require manual review of software by experts.</p>



<p>This kind of work is challenging. Analysts often face error and alert fatigue, and there’s no easy way to compare and standardize how different people review and classify threats over time. For both of these reasons, today&#8217;s overloaded experts are vulnerable to <a href="https://www.microsoft.com/en-us/security/blog/2020/08/31/microsoft-security-cultivate-diverse-cybersecurity-team/" target="_blank" rel="noreferrer noopener">burnout</a>, a well-documented issue in the field.</p>



<p>Unlike other AI applications in security, malware classification lacks a computable <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.blackhat.com/us-25/briefings/schedule/#ai-agents-for-offsec-with-zero-false-positives-46559" target="_blank" rel="noreferrer noopener">validator<span class="sr-only"> (opens in new tab)</span></a>. The AI must make judgment calls without definitive validation beyond expert review. Many behaviors found in software, like reverse engineering protections, don’t clearly indicate whether a sample is malicious or benign.&nbsp;</p>



<p>This ambiguity requires analysts to investigate each sample incrementally, building enough evidence to determine whether it’s malicious or benign despite opposition from adaptive, active adversaries. This&nbsp;has long made it difficult to automate and scale what is inherently a complex and expensive process.</p>



<h2 class="wp-block-heading" id="technical-foundation">Technical foundation</h2>



<p>Project Ire attempts to address these challenges by acting as an autonomous system that uses specialized tools to reverse engineer software. The system’s architecture allows for reasoning at multiple levels, from low-level binary analysis to control flow reconstruction and high-level interpretation of code behavior.</p>



<p>Its tool-use API enables the system to update its understanding of a file using a wide range of reverse engineering tools, including Microsoft memory analysis sandboxes based on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/freta/intro" target="_blank" rel="noreferrer noopener">Project Freta<span class="sr-only"> (opens in new tab)</span></a>, custom and open-source tools, documentation search, and multiple decompilers.&nbsp;&nbsp;</p>



<h2 class="wp-block-heading" id="reaching-a-verdict">Reaching a verdict&nbsp;</h2>



<p>The evaluation process begins with a triage, where automated reverse engineering tools identify the file type, its structure, and potential areas of interest. From there, the system reconstructs the software’s control flow graph using frameworks such as <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://angr.io/" target="_blank" rel="noreferrer noopener">angr<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/NationalSecurityAgency/ghidra" target="_blank" rel="noreferrer noopener">Ghidra<span class="sr-only"> (opens in new tab)</span></a>, building a graph that forms the backbone of Project Ire’s memory model and guides the rest of the analysis.&nbsp;&nbsp;</p>



<p>Through iterative function analysis, the LLM calls specialized tools through an API to identify and summarize key functions. Each result feeds into a “chain of evidence,” a detailed, auditable trail that shows how the system reached its conclusion. This traceable evidence log supports secondary review by security teams and helps refine the system in cases of misclassification.&nbsp;&nbsp;</p>



<p>To verify its findings, Project Ire can invoke a validator tool that cross-checks claims in the report against the chain of evidence. This tool draws on expert statements from malware reverse engineers on the Project Ire team. Drawing on this evidence and its internal model, the system creates a final report and classifies the sample as malicious or benign.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p id="microsoft-research-newsletter" class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-describedby="microsoft-research-newsletter" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="preliminary-testing-shows-promise">Preliminary testing shows promise&nbsp;</h2>



<p>Two early evaluations tested Project Ire’s effectiveness as an autonomous malware classifier. In the first, we assessed Project Ire on a dataset of publicly accessible Windows drivers, some known to be malicious, others benign. Malicious samples came from the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/magicsword-io/LOLDrivers/blob/e2e48f15a0885e8b2ad2fb81255089845f5c183c/detections/hashes/samples_malicious.sha256" target="_blank" rel="noreferrer noopener"><em>Living off the Land Drivers</em><span class="sr-only"> (opens in new tab)</span></a> database, which includes a collection of Windows drivers used by attackers to bypass security controls, while known benign drivers were sourced from Windows Update.&nbsp;</p>



<p>This classifier performed well, correctly identifying 90% of all files and flagging only 2% of benign files as threats. It achieved a precision of 0.98 and a recall of 0.83. This low false-positive rate suggests clear potential for deployment in security operations, alongside expert reverse engineering reviews.&nbsp;</p>



<p>For each file it analyzes, Project Ire generates a report that includes an evidence section, summaries of all examined code functions, and other technical artifacts.&nbsp;&nbsp;</p>



<p>Figures 1 and 2 present reports for two successful malware classification cases generated during testing. The first involves a kernel-level rootkit, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.virustotal.com/gui/file/86047bb1969d1db455493955fd450d18c62a3f36294d0a6c3732c88dfbcc4f62" target="_blank" rel="noreferrer noopener"><em>Trojan:Win64/Rootkit.EH!MTB</em><span class="sr-only"> (opens in new tab)</span></a>. The system identified several key features, including jump-hooking, process termination, and web-based command and control. It then correctly flagged the sample as malicious.</p>



<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Figure 1 Analysis</title>
  <style>
    body { font-family: Arial, sans-serif; background: #f8f8f8; }
    .code-block {
      background: #23272e;
      color: #e6e6e6;
      font-family: 'Fira Mono', 'Consolas', 'Monaco', monospace;
      font-size: 1em;
      padding: 24px 28px;
      border-radius: 10px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.10);
      border: 1px solid #444;
      margin: 40px auto;
      max-width: 900px;
      line-height: 1.7;
      word-break: break-word;
    }
    .code-block p {
      margin: 0 0 18px 0;
      text-indent: 0;
    }
  </style>
</head>
<body>
  <div class="code-block">
    <p>The binary contains a function named &#8216;MonitorAndTerminateExplorerThread_16f64&#8217; that runs an infinite loop waiting on synchronization objects and terminates system threads upon certain conditions. It queries system or process information, iterates over processes comparing their names case-insensitively to &#8216;Explorer.exe&#8217;, and manipulates registry values related to &#8216;Explorer.exe&#8217;. This function appears to monitor and potentially terminate or manipulate the &#8216;Explorer.exe&#8217; process, a critical Windows shell process. Such behavior is suspicious and consistent with malware that aims to disrupt or control system processes.</p>
    <p>Another function, &#8216;HttpGetRequestAndResponse_174a4&#8217;, performs HTTP GET requests by parsing URLs, resolving hostnames, opening sockets, sending requests, and reading responses. This network communication capability could be leveraged for command and control or data exfiltration, common in malware.</p>
    <p>The binary also includes a function &#8216;PatchProcessEntryPointWithHook_12b5c&#8217; that patches the entry point of a process by writing a hook or trampoline that redirects execution to a specified address. This technique is commonly used for process injection or hooking, allowing malware to alter process behavior or inject malicious code.</p>
    <p>Other functions related to sending IOCTL requests to device drivers were identified, but their maliciousness could not be conclusively determined without additional context.</p>
    <p>Overall, the binary exhibits multiple indicators of malicious behavior, including process manipulation, network communication, and code injection techniques, suggesting it is likely malware designed to interfere with system processes and communicate with remote servers.</p>
  </div>
</body>
</html>
<figure class="wp-block-video aligncenter"><figcaption class="wp-element-caption">Figure 1. Project Ire report, sample with SHA256: <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.virustotal.com/gui/file/86047bb1969d1db455493955fd450d18c62a3f36294d0a6c3732c88dfbcc4f62" target="_blank" rel="noreferrer noopener">86047bb1969d1db455493955fd450d18c62a3f36294d0a6c3732c88dfbcc4f62<span class="sr-only"> (opens in new tab)</span></a></figcaption></figure>



<p>The second sample, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.virustotal.com/gui/file/b6cb163089f665c05d607a465f1b6272cdd5c949772ab9ce7227120cf61f971a" target="_blank" rel="noreferrer noopener"><em>HackTool:Win64/KillAV!MTB</em><span class="sr-only"> (opens in new tab)</span></a>, was designed to disable antivirus software. Project Ire correctly identified the code that locates and disables antivirus programs, providing evidence that the file was malicious.&nbsp;&nbsp;</p>



<p>In one section of the code, however, the system misidentified a function as anti-debugging behavior. To maintain accuracy, the system used the validator tool to flag the claim as unsupported. The issue was later resolved by updating decompiler rules, but this example illustrates how Project Ire navigates uncertainty during analysis. Figure 2 shows the corresponding report.&nbsp;</p>



<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Figure 2 Analysis</title>
  <style>
    body { font-family: Arial, sans-serif; background: #f8f8f8; }
    .code-block {
      background: #23272e;
      color: #e6e6e6;
      font-family: 'Fira Mono', 'Consolas', 'Monaco', monospace;
      font-size: 1em;
      padding: 24px 28px;
      border-radius: 10px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.10);
      border: 1px solid #444;
      margin: 40px auto;
      max-width: 900px;
      line-height: 1.7;
      word-break: break-word;
    }
    .code-block p {
      margin: 0 0 18px 0;
      text-indent: 0;
    }
  </style>
</head>
<body>
  <div class="code-block">
    <p>The binary contains several functions indicative of malicious intent. The function register_and_log_known_processes_140001000 logs and registers process names associated with antivirus and security software, such as &#8216;avp.exe&#8217;, &#8216;avpui.exe&#8217;, and &#8216;360Tray.exe&#8217;. It calls another function, TerminateProcessesByNameSubstring_1400010f4, which enumerates system processes and terminates those whose names contain specified substrings. This behavior is typical of malware attempting to disable or evade security software by killing their processes.</p>
    <p>Another function, check_and_handle_special_state_14000502c, performs checks on a global variable and triggers software interrupts if certain conditions are not met. While the exact purpose of these interrupts (int 0x29 and int 0x3) is unclear, they could represent an anti-debug or anti-analysis mechanism to detect or interfere with debugging or tampering attempts. However, this assumption could not be fully validated against expert statements.</p>
    <p>Other functions include initialization routines and simple logging wrappers, but the core malicious behavior centers on process termination targeting security software. This indicates the binary is designed to compromise system security by disabling protective processes, a hallmark of malware such as trojans or rootkits.</p>
  </div>
</body>
</html>
<figure class="wp-block-video aligncenter"><figcaption class="wp-element-caption">Figure 2. Project Ire report, sample with SHA256: <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.virustotal.com/gui/file/b6cb163089f665c05d607a465f1b6272cdd5c949772ab9ce7227120cf61f971a" target="_blank" rel="noreferrer noopener">b6cb163089f665c05d607a465f1b6272cdd5c949772ab9ce7227120cf61f971a<span class="sr-only"> (opens in new tab)</span></a></figcaption></figure>



<h2 class="wp-block-heading" id="real-world-evaluation-with-microsoft-defender">Real-world evaluation with Microsoft Defender&nbsp;</h2>



<p>The more demanding test involved nearly 4,000 “hard-target” files not classified by automated systems and slated for manual review by expert reverse engineers.</p>



<p>In this real-world scenario, Project Ire operated fully autonomously on files created after the language models’ training cutoff, files that no other automated tools at Microsoft could classify at the time.</p>



<p>The system achieved a high precision score of 0.89, meaning nearly 9 out of 10 files flagged malicious were correctly identified as malicious. Recall was 0.26, indicating that under these challenging conditions, the system detected roughly a quarter of all actual malware.</p>



<p>The system correctly identified many of the malicious files, with few false alarms, just a 4% false positive rate. While overall performance was moderate, this combination of accuracy and a low error rate suggests real potential for future deployment.</p>



<h2 class="wp-block-heading" id="looking-ahead">Looking ahead&nbsp;</h2>



<p>Based on these early successes, the Project Ire prototype will be leveraged inside Microsoft’s Defender organization as <em>Binary Analyzer</em> for threat detection and software classification.</p>



<p>Our goal is to scale the system’s speed and accuracy so that it can correctly classify files from any source, even on first encounter. Ultimately, our vision is to detect novel malware directly <a href="https://www.microsoft.com/en-us/research/blog/toward-trusted-sensing-for-the-cloud-introducing-project-freta/?lang=fr_ca">in memory,</a> at scale.</p>



<h2 class="wp-block-heading" id="acknowledgements">Acknowledgements&nbsp;</h2>



<p>Project Ire acknowledges the following additional developers that contributed to the results in this publication: Dayenne de Souza, Raghav Pande, Ryan Terry, Shauharda Khadka, and Bob Fleck for their independent review of the system.</p>



<p>The system incorporates multiple tools, including the&nbsp;angr&nbsp;framework developed by&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://emotionlabs.io/" target="_blank" rel="noreferrer noopener">Emotion Labs<span class="sr-only"> (opens in new tab)</span></a>. Microsoft has collaborated extensively with Emotion Labs, a pioneer in cyber autonomy, throughout the development of Project Ire, and thanks them for the innovations and insights that contributed to the successes reported here.&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/project-ire-autonomously-identifies-malware-at-scale/">Project Ire autonomously identifies malware at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>VeriTrail: Detecting hallucination and tracing provenance in multi-step AI workflows</title>
		<link>https://www.microsoft.com/en-us/research/blog/veritrail-detecting-hallucination-and-tracing-provenance-in-multi-step-ai-workflows/</link>
		
		<dc:creator><![CDATA[Dasha Metropolitansky]]></dc:creator>
		<pubDate>Tue, 05 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1145803</guid>

					<description><![CDATA[<p>VeriTrail, new from Microsoft Research, can detect AI-generated content that is not supported by the source text, trace the provenance of content from final output back to the source, and locate where errors were likely introduced.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/veritrail-detecting-hallucination-and-tracing-provenance-in-multi-step-ai-workflows/">VeriTrail: Detecting hallucination and tracing provenance in multi-step AI workflows</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1.jpg" alt="Alt text: Two white icons on a blue-to-green gradient background—one showing a central figure linked to others, representing a network, and the other depicting lines connecting to a document, symbolizing data flow." class="wp-image-1145818" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-16018d1d wp-block-buttons-is-layout-flex">
<div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/video/veritrail-detect-hallucination-and-trace-provenance-in-ai-workflows/">Watch VeriTrail Explainer</a></div>
</div>



<p>Many applications of language models (LMs) involve generating content based on source material, such as answering questions, summarizing information, and drafting documents. A critical challenge for these applications is that LMs may produce content that is not supported by the source text – a phenomenon known as “closed-domain hallucination.”<a id="_ftnref1" href="#_ftn1"><sup>1</sup></a></p>



<p>Existing methods for detecting closed-domain hallucination typically compare a given LM output to the source text, implicitly assuming that there is only a single output to evaluate. However, applications of LMs increasingly involve processes with multiple generative steps: LMs generate intermediate outputs that serve as inputs to subsequent steps and culminate in a final output. Many agentic workflows follow this paradigm (e.g., each agent is responsible for a specific document or sub-task, and their outputs are synthesized into a final response). &nbsp;</p>



<p>In our paper “<a href="https://www.microsoft.com/en-us/research/publication/veritrail-closed-domain-hallucination-detection-with-traceability/" target="_blank" rel="noreferrer noopener">VeriTrail: Closed-Domain Hallucination Detection with Traceability</a>,” we argue that, given the complexity of processes with multiple generative steps, detecting hallucination in the final output is necessary but not sufficient. We also need <strong>traceability</strong>, which has two components:&nbsp;</p>



<ol start="1" class="wp-block-list">
<li><strong>Provenance: </strong>if the final output is supported by the source text, we should be able to trace its path through the intermediate outputs to the source.&nbsp;</li>



<li><strong>Error Localization: </strong>if the final output is not supported by the source text, we should be able to trace where the error was likely introduced.</li>
</ol>



<p>Our paper presents VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for processes with any number of generative steps. We also demonstrate that VeriTrail outperforms baseline methods commonly used for hallucination detection. In this blog post, we provide an overview of VeriTrail’s design and performance.<a id="_ftnref2" href="#_ftn2"><sup>2</sup></a></p>



<h2 class="wp-block-heading" id="veritrail-s-hallucination-detection-process">VeriTrail’s hallucination detection process</h2>



<p>A key idea leveraged by VeriTrail is that a wide range of generative processes can be represented as a <strong>directed acyclic graph (DAG)</strong>. Each <strong>node </strong>in the DAG represents a piece of text (i.e., source material, an intermediate output, or the final output) and each <strong>edge </strong>from node A to node B indicates that A was used as an input to produce B. Each node is assigned a unique ID, as well as a <strong>stage</strong> reflecting its position in the generative process. &nbsp;</p>



<p>An example of a process with multiple generative steps is <a href="https://www.microsoft.com/en-us/research/project/graphrag/" target="_blank" rel="noreferrer noopener">GraphRAG</a>. A DAG representing a GraphRAG run is illustrated in Figure 1, where the boxes and arrows correspond to nodes and edges, respectively.<a id="_ftnref3" href="#_ftn3"><sup>3</sup></a></p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="881" height="542" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-1.jpg" alt="A GraphRAG run is depicted as a directed acyclic graph. The Stage 1 nodes represent source text chunks. Each Stage 1 node has an edge pointing to a Stage 2 node, which corresponds to an entity or a relationship. Entity 3 was extracted from two source text chunks, so its descriptions are summarized. The summarized entity description forms a Stage 3 node. The Stage 2 and 3 nodes have edges pointing to Stage 4 nodes, which represent community reports. The Stage 4 nodes have edges pointing to Stage 5 nodes, which correspond to map-level answers. The Stage 5 nodes each have an edge pointing to the terminal node, which represents the final answer. The terminal node is the only node in Stage 6." class="wp-image-1145841" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-1.jpg 881w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-1-300x185.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-1-768x472.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-1-240x148.jpg 240w" sizes="auto, (max-width: 881px) 100vw, 881px" /><figcaption class="wp-element-caption">Figure 1: GraphRAG splits the source text into chunks (Stage 1). For each chunk, an LM extracts entities and relationships (the latter are denoted by “⭤ “), along with short descriptions (Stage 2). If an entity or a relationship was extracted from multiple chunks, an LM summarizes the descriptions (Stage 3). A knowledge graph is constructed from the final set of entities and relationships, and a community detection algorithm, such as Leiden clustering, groups entities into communities. For each community, an LM generates a “community report” that summarizes the entities and relationships (Stage 4). To answer a user’s question, an LM generates “map-level answers” based on groups of community reports (Stage 5), then synthesizes them into a final answer (Stage 6).</figcaption></figure>



<p>VeriTrail takes as input a DAG representing a completed generative process and aims to determine whether the final output is fully supported by the source text. It begins by extracting claims (i.e., self-contained, verifiable statements) from the final output using <a href="https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/" target="_blank" rel="noreferrer noopener">Claimify</a>. VeriTrail verifies claims in the reverse order of the generative process: it starts from the final output and moves toward the source text. Each claim is verified separately. Below, we include two case studies that illustrate how VeriTrail works, using the DAG from Figure 1. </p>



<h3 class="wp-block-heading" id="case-study-1-a-fully-supported-claim">Case study 1: A “Fully Supported” claim</h3>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1280" height="720" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2.jpg" alt="An example of VeriTrail's claim verification process where the claim is found “Fully Supported.” A claim extracted from the terminal node, Node 17, is “Legislative efforts have been made to address the high cost of diabetes-related supplies in the US.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. The sentence “The general assembly in North Carolina is considering legislation to set a cap on insulin prices, which indicates that high insulin prices are a contributing factor to the high cost of diabetes-related supplies in the US” is selected as evidence from Node 15. The tentative verdict is “Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12 and 13, which are the source nodes of Node 15. The sentence “The General Assembly in North Carolina is considering legislation to set a cap on insulin prices” is selected as evidence from Node 13. The verdict remains “Fully Supported.” In Iteration 3, VeriTrail checks Nodes 4, 5, and 11, which are the source nodes of Node 13. The sentence “The General Assembly is the legislative body in North Carolina considering legislation to cap insulin prices” is selected as evidence from Node 4. The verdict is still “Fully Supported.” In Iteration 4, VeriTrail checks Node 1, which is the source node of Node 4. The selected evidence is “‘There’s actually legislation in North Carolina at the General Assembly to set a cap on insulin…’ Stein said.” The corresponding verdict is “Fully Supported.” Since Node 1 represents a raw text chunk, it does not have any source nodes to check. Therefore, verification terminates and the “Fully Supported” verdict is deemed final." class="wp-image-1145843" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2-960x540.jpg 960w" sizes="auto, (max-width: 1280px) 100vw, 1280px" /><figcaption class="wp-element-caption">Figure 2: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Fully Supported” claim.</figcaption></figure>



<p>Figure 2 shows an example of a claim that VeriTrail determined was not hallucinated: </p>



<ul class="wp-block-list">
<li>In Iteration 1, VeriTrail identified the nodes that were used as inputs for the final answer: Nodes 15 and 16. Each identified node was split into sentences, and each sentence was programmatically assigned a unique ID.
<ul class="wp-block-list">
<li>An LM then performed <strong>Evidence Selection</strong>, selecting all sentence IDs that strongly implied the truth or falsehood of the claim. The LM also generated a summary of the selected sentences (not shown in Figure 2). In this example, a sentence was selected from Node 15.</li>



<li>Next, an LM performed <strong>Verdict Generation</strong>. If no sentences had been selected in the Evidence Selection step, the claim would have been assigned a “Not Fully Supported” verdict. Instead, an LM was prompted to classify the claim as “Fully Supported,” “Not Fully Supported,” or “Inconclusive” based on the evidence. In this case, the verdict was “Fully Supported.”</li>
</ul>
</li>



<li>Since the verdict in Iteration 1 was “Fully Supported,” VeriTrail proceeded to Iteration 2. It considered the nodes from which at least one sentence was selected in the latest Evidence Selection step (Node 15) and identified their input nodes (Nodes 12 and 13). VeriTrail repeated Evidence Selection and Verdict Generation for the identified nodes. Once again, the verdict was “Fully Supported.” This process – identifying candidate nodes, performing Evidence Selection and Verdict Generation – was repeated in Iteration 3, where the verdict was still “Fully Supported,” and likewise in Iteration 4. </li>



<li>In Iteration 4, a single source text chunk was verified. Since the source text, by definition, does not have any inputs, verification terminated and the verdict was deemed final.</li>
</ul>



<h3 class="wp-block-heading" id="case-study-2-a-not-fully-supported-claim">Case study 2: A “Not Fully Supported” claim</h3>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1280" height="515" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3.jpg" alt="An example of VeriTrail's claim verification process where the claim is found “Not Fully Supported.” We assume that the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. A claim extracted from the terminal node, Node 17, is “Challenges related to electric vehicle battery repairability contribute to sluggish retail auto sales in China.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. Two sentences are selected as evidence. The first sentence is “Challenges with electric vehicle (EV) battery disposal and repair may also contribute to the sluggishness in retail auto sales.” The second sentence is “Junkyards are accumulating discarded EV battery packs, while collision shops face limitations in repairing EV battery packs, which could affect consumer confidence and demand.” These sentences are both from Node 15. The tentative verdict is “Not Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12, 13, and 14. Nodes 12 and 13 are the source nodes of Node 15. Node 14 is the source node of Node 16, which was checked in Iteration 1. The sentence “The electric vehicle market in China is influenced by challenges associated with EV battery disposal and repair” is selected as evidence from Node 12. The verdict remains “Not Fully Supported.” Since two consecutive “Not Fully Supported” verdicts have been reached, which was the maximum, verification terminates and the final verdict is “Not Fully Supported.”" class="wp-image-1145842" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3-300x121.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3-1024x412.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3-768x309.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3-240x97.jpg 240w" sizes="auto, (max-width: 1280px) 100vw, 1280px" /><figcaption class="wp-element-caption">Figure 3: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Not Fully Supported” claim, where the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. </figcaption></figure>



<p>Figure 3 provides an example of a claim where VeriTrail identified hallucination:</p>



<ul class="wp-block-list">
<li>In Iteration 1, VeriTrail identified the nodes used as inputs for the final answer: Nodes 15 and 16. After Evidence Selection and Verdict Generation, the verdict was “Not Fully Supported.” Users can configure the maximum number of consecutive “Not Fully Supported” verdicts permitted. If the maximum had been set to 1, verification would have terminated here, and the verdict would have been deemed final. Let’s assume the maximum was set to 2, meaning that VeriTrail had to perform at least one more iteration.</li>



<li>Even though evidence was selected only from Node 15 in Iteration 1, VeriTrail checked the input nodes for both Node 15 and Node 16 (i.e., Nodes 12, 13, and 14) in Iteration 2. Recall that in Case Study 1 where the verdict was “Fully Supported,” VeriTrail only checked the input nodes for Node 15. Why was the “Not Fully Supported” claim handled differently? If the Evidence Selection step overlooked relevant evidence, the “Not Fully Supported” verdict might be incorrect. In this case, continuing verification based solely on the selected evidence (i.e., Node 15) would propagate the mistake, defeating the purpose of repeated verification.</li>



<li>In Iteration 2, Evidence Selection and Verdict Generation were repeated for Nodes 12, 13, and 14. Once again, the verdict was “Not Fully Supported.” Since this was the second consecutive “Not Fully Supported” verdict, verification terminated and the verdict was deemed final.</li>
</ul>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p id="microsoft-research-newsletter" class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-describedby="microsoft-research-newsletter" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="providing-traceability">Providing traceability</h2>



<p>In addition to assigning a final “Fully Supported,” “Not Fully Supported,” or “Inconclusive” verdict to each claim, VeriTrail returns (a) all Verdict Generation results and (b) an evidence trail composed of all Evidence Selection results: the selected sentences, their corresponding node IDs, and the generated summaries. Collectively, these outputs provide traceability:&nbsp;</p>



<ol start="1" class="wp-block-list">
<li><strong>Provenance: </strong>For “Fully Supported” and “Inconclusive” claims, the evidence trail traces a path from the source material to the final output, helping users understand how the output may have been derived. For example, in Case Study 1, the evidence trail consists of Sentence 8 from Node 15, Sentence 11 from Node 13, Sentence 26 from Node 4, and Sentence 79 from Node 1.</li>



<li><strong>Error Localization: </strong>For “Not Fully Supported” claims, VeriTrail uses the Verdict Generation results to identify the stage(s) of the process where the unsupported content was likely introduced. For instance, in Case Study 2, where none of the verified intermediate outputs supported the claim, VeriTrail would indicate that the hallucination occurred in the final answer (Stage 6). Error stage identification helps users address hallucinations and understand where in the process they are most likely to occur.&nbsp;</li>
</ol>



<p>The evidence trail also helps users verify the verdict: instead of reading through all nodes – which may be infeasible for processes that generate large amounts of text – users can simply review the evidence sentences and summaries. </p>



<h2 class="wp-block-heading" id="key-design-features">Key design features</h2>



<p>VeriTrail’s design prioritizes reliability, efficiency, scalability, and user agency. Notable features include: </p>



<ul class="wp-block-list">
<li>During Evidence Selection (introduced in Case Study 1), the sentence IDs returned by the LM are checked against the programmatically assigned IDs. If a returned ID does not match an assigned ID, it is discarded; otherwise, it is mapped to its corresponding sentence. This approach <strong>guarantees that the sentences included in the evidence trail are not hallucinated</strong>.</li>



<li>After a claim is assigned an interim “Fully Supported” or “Inconclusive” verdict (as in Case Study 1), VeriTrail verifies the input nodes of only the nodes from which evidence was previously selected – not all possible input nodes. By progressively narrowing the search space, VeriTrail limits the number of nodes the LM must evaluate. In particular, since VeriTrail starts from the final output and moves toward the source text, it tends to verify a smaller proportion of nodes as it approaches the source text. Nodes closer to the source text tend to be larger (e.g., a book chapter should be larger than its summary), so verifying fewer of them helps <strong>reduce computational cost</strong>.</li>



<li>VeriTrail is designed to <strong>handle input graphs with any number of nodes</strong>, regardless of whether they fit in a single prompt. Users can specify an input size limit per prompt. For Evidence Selection, inputs that exceed the limit are split across multiple prompts. If the resulting evidence exceeds the input size limit for Verdict Generation, VeriTrail reruns Evidence Selection to compress the evidence further. Users can configure the maximum number of Evidence Selection reruns.  </li>



<li>The configurable maximum number of consecutive “Not Fully Supported” verdicts (introduced in Case Study 2) allows the user to find their desired <strong>balance between computational cost and how conservative VeriTrail is in flagging hallucinations</strong>. A lower maximum reduces cost by limiting the number of checks. A higher maximum increases confidence that a flagged claim is truly hallucinated since it requires repeated confirmation of the “Not Fully Supported” verdict. </li>
</ul>



<h2 class="wp-block-heading" id="evaluating-veritrail-s-performance">Evaluating VeriTrail’s performance</h2>



<p>We tested VeriTrail on two datasets covering distinct generative processes (hierarchical summarization<a id="_ftnref4" href="#_ftn4"><sup>4</sup></a> and GraphRAG), tasks (summarization and question-answering), and types of source material (fiction novels and news articles). For the source material, we focused on long documents and large collections of documents (i.e., >100K tokens), where hallucination detection is especially challenging and processes with multiple generative steps are typically most valuable. The resulting DAGs were much more complex than the examples provided above (e.g., in one of the datasets, the average number of nodes was 114,368).</p>



<p>We compared VeriTrail to three types of baseline methods commonly used for closed-domain hallucination detection: Natural Language Inference models (<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aclanthology.org/2023.acl-long.634/" target="_blank" rel="noreferrer noopener">AlignScore</a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aclanthology.org/2024.eacl-long.102/" target="_blank" rel="noreferrer noopener">INFUSE</a>); Retrieval-Augmented Generation; and long-context models (Gemini 1.5 Pro and GPT-4.1 mini). Across both datasets and all language models tested, VeriTrail outperformed the baseline methods in detecting hallucination.<a id="_ftnref5" href="#_ftn5"><sup>5</sup></a></p>



<p>Most importantly, VeriTrail traces claims through intermediate outputs – unlike the baseline methods, which directly compare the final output to the source material. As a result, it can identify where hallucinated content was likely introduced and how faithful content may have been derived from the source. By providing traceability, VeriTrail brings transparency to generative processes, helping users understand, verify, debug, and, ultimately, trust their outputs. &nbsp;</p>



<p>For an in-depth discussion of VeriTrail, please see our paper “<a href="https://www.microsoft.com/en-us/research/publication/veritrail-closed-domain-hallucination-detection-with-traceability/" target="_blank" rel="noreferrer noopener">VeriTrail: Closed-Domain Hallucination Detection with Traceability.</a>”</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><a id="_ftn1" href="#_ftnref1"><sup>1</sup><span class="sr-only"> (opens in new tab)</span></a> The term “closed-domain hallucination” was introduced by OpenAI in the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2303.08774" target="_blank" rel="noreferrer noopener">GPT-4 Technical Report<span class="sr-only"> (opens in new tab)</span></a>.</p>



<p><sup><a id="_ftn2" href="#_ftnref2">2</a></sup> VeriTrail is currently used for research purposes only and is not available commercially.</p>



<p><sup><a id="_ftn3" href="#_ftnref3">3</a></sup> We focus on GraphRAG’s global search method.</p>



<p><sup><a id="_ftn4" href="#_ftnref4">4<span class="sr-only"> (opens in new tab)</span></a></sup> In hierarchical summarization, an LM summarizes each source text chunk individually, then the resulting summaries are repeatedly grouped and summarized until a final summary is produced (<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2109.10862" target="_blank" rel="noreferrer noopener">Wu et al., 2021<span class="sr-only"> (opens in new tab)</span></a>; <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2310.00785" target="_blank" rel="noreferrer noopener">Chang et al., 2023<span class="sr-only"> (opens in new tab)</span></a>).</p>



<p><sup><a id="_ftn5" href="#_ftnref5">5</a></sup> The only exception was the mistral-large-2411 model, where VeriTrail had the highest balanced accuracy, but not the highest macro F1 score.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/veritrail-detecting-hallucination-and-tracing-provenance-in-multi-step-ai-workflows/">VeriTrail: Detecting hallucination and tracing provenance in multi-step AI workflows</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Xinxing Xu bridges AI research and real-world impact at Microsoft Research Asia &#8211; Singapore</title>
		<link>https://www.microsoft.com/en-us/research/blog/xinxing-xu-bridges-ai-research-and-real-world-impact-at-microsoft-research-asia-singapore/</link>
		
		<dc:creator><![CDATA[Xinxing Xu]]></dc:creator>
		<pubDate>Thu, 24 Jul 2025 01:30:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1145385</guid>

					<description><![CDATA[<p>Xinxing Xu is helping shape the work of Microsoft Research Asia – Singapore by turning advanced AI research into real-world solutions. Learn how he collaborates across sectors and disciplines to drive responsible innovation throughout Southeast Asia.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/xinxing-xu-bridges-ai-research-and-real-world-impact-at-microsoft-research-asia-singapore/">Xinxing Xu bridges AI research and real-world impact at Microsoft Research Asia &#8211; Singapore</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p>AI has made remarkable progress in recent years, but turning experimental models into tools that work in the real world is still a major challenge. Bridging this gap between innovation and application has shaped the career of Xinxing Xu, principal researcher at <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" target="_blank" href="https://news.microsoft.com/source/asia/2025/07/24/microsoft-research-asia-launches-singapore-lab-to-drive-ai-innovation-industrial-transformation-and-talent-development/?msockid=3b78cf39416866772e40db2040e7673b">Microsoft Research Asia – Singapore<span class="sr-only"> (opens in new tab)</span></a>, and underpins the mission of the lab’s newly established presence in the region.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="960" height="540" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-Xu-1-1.jpg" alt="photo of Xinxing Xu standing against a gray background" class="wp-image-1145600" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-Xu-1-1.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-Xu-1-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-Xu-1-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-Xu-1-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-Xu-1-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-Xu-1-1-640x360.jpg 640w" sizes="auto, (max-width: 960px) 100vw, 960px" /><figcaption class="wp-element-caption">Xinxing Xu, Principal Researcher, Microsoft Research Asia – Singapore</figcaption></figure>



<p>“Innovative algorithms can only demonstrate their true value when tested with real-world data and in actual scenarios, where they can be continuously optimized through iteration,” he says.</p>



<p>Xu’s commitment to balancing algorithmic innovation with practical application has shaped his entire career. During his PhD studies at Nanyang Technological University, Singapore, Xu focused on emerging technologies like multiple kernel learning methods and multimodal machine learning. Today he’s applying these techniques to real-world use cases like image recognition and video classification.</p>



<p>After completing his doctorate, he joined the Institute of High Performance Computing at Singapore’s Agency for Science, Technology and Research (A*STAR), where he worked on interdisciplinary projects ranging from medical image recognition to AI systems for detecting defects on facade of buildings. These experiences broadened his perspective and deepened his passion for translating AI into real-world impact.</p>



<p>In 2024, Xu joined Microsoft Research Asia where he began a new chapter focused on bridging between academic research and real-world AI applications.</p>



<p>“Microsoft Research Asia is committed to integrating scientific exploration with real-world applications, which creates a unique research environment,” Xu says. “It brings together top talent and resources, and Microsoft&#8217;s engineering and product ecosystem strongly supports turning research into impactful technology. The lab’s open and inclusive culture encourages innovation with broader societal impact. It reflects the approach to research I’ve always hoped to contribute to.”</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144028">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-label="The AI Revolution in Medicine, Revisited" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Episode7-PeterBillSebastien-AIRevolution_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshot of Bill Gates, Peter Lee, and Sébastien Bubeck" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">The AI Revolution in Medicine, Revisited</h2>
				
								<p id="the-ai-revolution-in-medicine-revisited" class="large">Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-describedby="the-ai-revolution-in-medicine-revisited" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="bringing-cross-domain-expertise-to-ai-s-real-world-frontiers">Bringing cross-domain expertise to AI’s real-world frontiers</h2>



<p>As a key hub in Microsoft Research’s network across Asia, the Singapore lab is guided by a three-part mission: to drive industry-transforming AI deployment, pursue fundamental breakthroughs in the field, and promote responsible, socially beneficial applications of the technology.</p>



<p>To reach these goals, Xu and his colleagues are working closely with local collaborators, combining cross-disciplinary expertise to tackle complex, real-world challenges.</p>



<p>To deliver on that mission, Xinxing Xu and his colleagues are working closely with local collaborators, drawing on cross-disciplinary expertise to solve real-world problems. One key focus is healthcare, where Xu leads a collaboration with Singapore’s SingHealth to explore how AI can support precision medicine. By combining SingHealth’s clinical data with advanced AI models, the team aims to deliver more personalized analyses and sharper diagnostic tools—laying the groundwork for improved patient outcomes.&nbsp;</p>



<p>Beyond healthcare, the team is also targeting key sectors like finance and logistics. By developing domain-specific foundation models and AI agents, they aim to support smarter decision-making and accelerate digital transformation across industries. “Singapore has a strong foundation in these sectors,” Xu notes, “making it an ideal environment for technology validation and iteration.”</p>



<p>The team is also partnering with leading academic institutions, including the National University of Singapore (NUS) and Nanyang Technological University, Singapore (NTU Singapore), to advance the field of spatial intelligence. Their goal is to develop embodied intelligence systems capable of carrying out complex tasks in smart environments.</p>



<p>As AI becomes more deeply embedded in everyday life, researchers at the Singapore lab are also increasingly focused on what they call “societal AI”—building AI systems that are culturally relevant and trustworthy within Southeast Asia’s unique cultural and social contexts. In collaboration with global colleagues, they’re helping to advance a more culturally grounded and responsible approach to AI research in the region.</p>



<h2 class="wp-block-heading" id="microsoft-research-asia-singapore-expanding-global-reach-connecting-regional-innovation">Microsoft Research Asia – Singapore: Expanding global reach, connecting regional innovation&nbsp;</h2>



<p>Realizing AI’s full potential requires more than technical breakthroughs. It also depends on collaboration—across industries, academia, and policy. Only through this intersection of forces can AI move beyond the lab to deliver meaningful societal value.&nbsp;</p>



<p>Singapore’s strengths in science, engineering, and digital governance make it an ideal setting for this kind of work. Its collaborative culture, robust infrastructure, international talent pool, and strong policy support for science and technology make it fertile ground for interdisciplinary research.&nbsp;</p>



<p>This is why Microsoft Research Asia continues to collaborate closely with Singapore’s top universities, research institutions, and industry partners. These partnerships support joint research, talent development, and technical exchange. Building on this foundation, Microsoft Research Asia – Singapore will further deepen its collaboration with NUS, NTU Singapore, and Singapore Management University (SMU) to advance both fundamental and applied research, while equipping the next generation of researchers with real-world experience. In addition, Microsoft Research Asia is fostering academic exchange and strengthening the research ecosystem through summer schools and joint workshops with NUS, NTU Singapore, and SMU.&nbsp;</p>



<p>The launch of the Singapore lab further marks an important step in expanding the company’s global research footprint, serving as a bridge between regional innovation and Microsoft’s global ecosystem. Through its integrated lab network, Microsoft Research fosters the sharing of technologies, methods, and real-world insights, creating a virtuous cycle of innovation.</p>



<p>“We aim to build a research hub in Singapore that is globally connected and deeply rooted in the local ecosystem,” Xu says. “Many breakthroughs come from interdisciplinary and cross-regional collaboration. By breaking boundaries—across disciplines, industries, and geographies—we can drive research that has lasting impact.”</p>



<p>As AI becomes more deeply woven into industry and everyday life, Xu believes that meaningful research must be closely connected to regional development and social well-being. “Microsoft Research Asia – Singapore is a future-facing lab,” he says. “While we push technological frontiers, we’re equally committed to the responsibility of technology—ensuring AI can help address society’s most pressing challenges.”</p>



<p>In a world shaped by global challenges, Xu sees collaboration and innovation as essential to real progress. With Singapore as a launchpad, he and his team are working to extend AI’s impact and value across Southeast Asia and beyond.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1024" height="768" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-with-colleagues.jpg" alt="Xingxing Xu (center) with colleagues at Microsoft Research Asia - Singapore " class="wp-image-1145598" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-with-colleagues.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-with-colleagues-300x225.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-with-colleagues-768x576.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-with-colleagues-80x60.jpg 80w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-with-colleagues-240x180.jpg 240w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">Xingxing Xu (center) with colleagues at Microsoft Research Asia &#8211; Singapore&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="three-essential-strengths-for-the-next-generation-of-ai-researchers">Three essential strengths for the next generation of AI researchers</h2>



<p>AI’s progress depends not only on technical breakthroughs but also on the growth and dedication of talent. At Microsoft Research Asia, there is a strong belief that bringing research into the real world requires more than technical coordination—it depends on unlocking the full creativity and potential of researchers.</p>



<p>In Singapore—a regional innovation hub that connects Southeast Asia—Xu and his colleagues are working to push AI beyond the lab and into fields like healthcare, finance, and manufacturing. For young researchers hoping to shape the future of AI, this is a uniquely powerful stage.</p>



<p>To help guide the next generation, Xu shares three pieces of advice:</p>



<ul class="wp-block-list">
<li><strong>Build a strong foundation</strong> – “Core knowledge in machine learning, linear algebra, and probability and statistics is the bedrock of AI research,” Xu says. “A solid theoretical base is essential to remain competitive in a rapidly evolving field. Even today’s hottest trends in generative AI rely on longstanding principles of optimization and model architecture design.” While code generation tools are on the rise, Xu emphasizes that mathematical fundamentals remain essential for understanding and innovating in AI.</li>



<li><strong>Understand real-world applications</strong> – Technical skills alone aren’t enough. Xu encourages young researchers to deeply engage with the problems they’re trying to solve. Only by tightly integrating technology with its context can researchers create truly valuable solutions.<br><br>“In healthcare, for example, researchers may need to follow doctors in clinics to gain a true understanding of clinical workflows. That context helps identify the best entry points for AI deployment. Framing research problems around real-world needs is often more impactful than just tuning model parameters,” Xu says.</li>



<li><strong>Develop interdisciplinary thinking</strong> – Cross-disciplinary collaboration is becoming essential to AI innovation. Xu advises young researchers to learn how to work with experts from other fields to explore new directions together. “These kinds of interactions often spark fresh, creative ideas,” he says.<br><br>Maintaining curiosity is just as important. “Being open to new technologies and fields is what enables researchers to continually break new ground and produce original results.”</li>
</ul>



<p>Xu extends an open invitation to aspiring researchers from all backgrounds to join Microsoft Research Asia – Singapore. “We offer a unique platform that blends cutting-edge research with real-world impact,” he says. “It’s a place where you can work on the frontiers of AI—and see how your work can help transform industries and improve lives.”</p>



<p>To learn more about current openings at the Singapore lab, please visit our <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jobs.careers.microsoft.com/global/en/job/1849717/Senior-Researcher" target="_blank" rel="noreferrer noopener">careers page<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/xinxing-xu-bridges-ai-research-and-real-world-impact-at-microsoft-research-asia-singapore/">Xinxing Xu bridges AI research and real-world impact at Microsoft Research Asia &#8211; Singapore</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Technical approach for classifying human-AI interactions at scale</title>
		<link>https://www.microsoft.com/en-us/research/blog/technical-approach-for-classifying-human-ai-interactions-at-scale/</link>
		
		<dc:creator><![CDATA[Amber Hoak, David Tittsworth, Kate Lytvynets, Scott Counts, Weiwei Yang, Ben Cutler, Jonathan McLean]]></dc:creator>
		<pubDate>Wed, 23 Jul 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1144422</guid>

					<description><![CDATA[<p>Semantic Telemetry helps LLMs run efficiently, reliably, and in near real-time. Learn about the engineering behind that system, including the trade-offs and lessons learned along the way—from batching strategies to token optimization and orchestration.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/technical-approach-for-classifying-human-ai-interactions-at-scale/">Technical approach for classifying human-AI interactions at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1.jpg" alt="The image features four white icons on a gradient background that transitions from blue on the left to green on the right. The first icon is a network or molecule structure with interconnected nodes. The second icon shows a stylized person in front of a computer screen. The third icon shows an organization tree with one main node and three nodes branching out side by side below it." class="wp-image-1144473" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>As large language models (LLMs) become foundational to modern AI systems, the ability to run them at scale—efficiently, reliably, and in near real-time—is no longer a nice-to-have. It’s essential. The <a href="https://www.microsoft.com/en-us/research/project/semantic-telemetry/?msockid=153992cb7df169482b9487167c0968e9">Semantic Telemetry</a> project tackles this challenge by applying LLM-based classifiers to hundreds of millions of sampled, anonymized Bing Chat conversations each week. These classifiers extract signals like user expertise, primary topic, and satisfaction, enabling deeper insight into human-AI interactions and driving continuous system improvement.</p>



<p>But building a pipeline that can handle this volume isn’t just about plugging into an API. It requires a high-throughput, high-performance architecture that can orchestrate distributed processing, manage token and prompt complexity, and gracefully handle the unpredictability of remote LLM endpoints.</p>



<p>In this latest post in our series on Semantic Telemetry, we’ll walk through the engineering behind that system—how we designed for scale from the start, the trade-offs we made, and the lessons we learned along the way. From batching strategies and token optimization and orchestration, we’ll share what it takes to build a real-time LLM classification pipeline.</p>



<p>For additional project background: <a href="https://www.microsoft.com/en-us/research/blog/semantic-telemetry-understanding-how-users-interact-with-ai-systems/">Semantic Telemetry: Understanding how users interact with AI systems</a> and <a href="https://www.microsoft.com/en-us/research/blog/engagement-user-expertise-and-satisfaction-key-insights-from-the-semantic-telemetry-project/">Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project</a>.</p>



<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<div class="annotations " data-bi-aN="citation">
	<article class="annotations__list card depth-16 bg-body p-4 ">
		<div class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Blog</span>
			<a href="https://www.microsoft.com/en-us/research/blog/semantic-telemetry-understanding-how-users-interact-with-ai-systems/" target="_self" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="Semantic Telemetry: Understanding how users interact with AI systems" data-bi-aN="citation" data-bi-cN="Semantic Telemetry: Understanding how users interact with AI systems">
				Semantic Telemetry: Understanding how users interact with AI systems&nbsp;<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span>
			</a>
					</div>
	</article>
</div>
</div>



<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<div class="annotations " data-bi-aN="citation">
	<article class="annotations__list card depth-16 bg-body p-4 ">
		<div class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Blog</span>
			<a href="https://www.microsoft.com/en-us/research/blog/engagement-user-expertise-and-satisfaction-key-insights-from-the-semantic-telemetry-project/" target="_self" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project" data-bi-aN="citation" data-bi-cN="Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project">
				Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project&nbsp;<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span>
			</a>
					</div>
	</article>
</div>
</div>
</div>



<h2 class="wp-block-heading" id="system-architecture-highlights">System architecture highlights</h2>



<p>The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://spark.apache.org/docs/latest/api/python/index.html" target="_blank" rel="noreferrer noopener">Semantic Telemetry pipeline<span class="sr-only"> (opens in new tab)</span></a> is a highly-scalable, highly-configurable, data transformation pipeline. While it follows a familiar ETL structure, several architectural innovations make it uniquely suited for high-throughput LLM integration:</p>



<ul class="wp-block-list">
<li><strong>Hybrid compute engine</strong><br>The pipeline combines the distributed power of PySpark with the speed and simplicity of Polars, enabling it to scale across large datasets or run lightweight jobs in Spark-less environments—without code changes.</li>



<li><strong>LLM-centric transformation layer</strong><br>At the core of the pipeline is a multi-stage transformation process tailored for running across multiple LLM endpoints such that:
<ul class="wp-block-list">
<li>Runs model agnostic. Provides a generic interface for LLMs and adopts model specific interfaces built from a generic interface.</li>



<li>Prompt templates are defined using the Prompty language specification for consistency and reuse, with options for users to include custom prompts.</li>



<li>Parsing and cleaning logic ensures structured, schema-aligned outputs, even when LLM responses are imperfect such as removing extra characters in output, resolving not-exact label matches (i.e. “create” versus “created”) and relabeling invalid classifications.</li>
</ul>
</li>
</ul>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="650" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Semantic-Telemetry-Pipeline-2_1400px.png" alt="Figure 1. Architecture diagram of LLM workflow" class="wp-image-1144472" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Semantic-Telemetry-Pipeline-2_1400px.png 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Semantic-Telemetry-Pipeline-2_1400px-300x139.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Semantic-Telemetry-Pipeline-2_1400px-1024x475.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Semantic-Telemetry-Pipeline-2_1400px-768x357.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Semantic-Telemetry-Pipeline-2_1400px-240x111.png 240w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 1. Architecture diagram</figcaption></figure>



<p>The pipeline supports multiple classification tasks (e.g., user expertise, topic, satisfaction) through modular prompt templates and configurable execution paths—making it easy to adapt to new use cases or environments.</p>



<h2 class="wp-block-heading" id="engineering-challenges-solutions">Engineering challenges & solutions</h2>



<p>Building a high-throughput, LLM-powered classification pipeline at scale introduced a range of engineering challenges—from managing latency and token limits to ensuring system resilience. Below are the key hurdles we encountered and how we addressed them.</p>



<h3 class="wp-block-heading" id="llm-endpoint-latency-variability">LLM endpoint latency & variability</h3>



<p><strong>Challenge</strong>: LLM endpoints, especially those hosted remotely (e.g., Azure OpenAI), introduce unpredictable latency due to model load, prompt complexity, and network variability. This made it difficult to maintain consistent throughput across the pipeline.</p>



<p><strong>Solution</strong>: We implemented a combination of:</p>



<ul class="wp-block-list">
<li><strong>Multiple Azure OpenAI endpoints</strong> in rotation to increase throughput and distribute workload. We can analyze throughput and redistribute as needed.</li>



<li><strong>Saving output in intervals</strong> to write data asynchronously in case of network errors.</li>



<li><strong>Utilizing models with higher tokens per minute (TPM)</strong> such as OpenAI’s GPT-4o mini. GPT-4o mini had a 2M TPM limit which is a 25x throughput increase from GPT-4 (80K TPM -> 2M TPM)</li>



<li><strong>Timeouts and retries</strong> with exponential backoff.</li>
</ul>



<h3 class="wp-block-heading" id="evolving-llm-models-prompt-alignment">Evolving LLM models & prompt alignment</h3>



<p><strong>Challenge</strong>: Each new LLM release—such as Phi, Mistral, DeepSeek, and successive generations of GPT (e.g., GPT-3.5, GPT-4, GPT-4 Turbo, GPT-4o)—brings improvements, but also subtle behavioral shifts. These changes can affect classification consistency, output formatting, and even the interpretation of prompts. Maintaining alignment with baseline expectations across models became a moving target.</p>



<p><strong>Solution</strong>: We developed a model evaluation workflow to test prompt alignment across LLM versions:</p>



<ul class="wp-block-list">
<li><strong>Small-sample testing</strong>: We ran the pipeline on a representative sample using the new model and compared the output distribution to a known baseline.</li>



<li><strong>Distribution analysis</strong>: If the new model’s output aligned closely, we scaled up testing. If not, we iteratively <strong>tuned the prompts</strong> and re-ran comparisons.</li>



<li><strong>Interpretation flexibility</strong>: We also recognized that a shift in distribution isn’t always a regression. Sometimes it reflects a more accurate or nuanced classification, especially as models improve.</li>
</ul>



<p>To support this process, we used tools like <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/sammo" target="_blank" rel="noreferrer noopener">Sammo<span class="sr-only"> (opens in new tab)</span></a>, which allowed us to compare outputs across multiple models and prompt variants. This helped us quantify the impact of prompt changes and model upgrades and make informed decisions about when to adopt a new model or adjust our classification schema.</p>



<h3 class="wp-block-heading" id="dynamic-concurrency-scaling-for-llm-calls">Dynamic concurrency scaling for LLM calls</h3>



<p><strong>Challenge</strong>: LLM endpoints frequently encounter rate limits and inconsistent response times under heavy usage. The models&#8217; speeds can also vary, complicating the selection of optimal concurrency levels. Furthermore, users may choose suboptimal settings due to lack of familiarity, and default concurrency configurations are rarely ideal for every situation. Dynamic adjustments based on throughput, measured in various ways, can assist in determining optimal concurrency levels.</p>



<p><strong>Solution</strong>: We implemented a dynamic concurrency control mechanism that proactively adjusts the number of parallel LLM calls based on real-time system behavior:</p>



<ul class="wp-block-list">
<li><strong>External task awareness</strong>: The system monitors the number of parallel tasks running across the pipeline (e.g., Spark executors or async workers) and uses this to inform the initial concurrency level.</li>



<li><strong>Success/failure rate monitoring</strong>: The system tracks the rolling success and failure rates of LLM calls. A spike in failures triggers a temporary reduction in concurrency, while sustained success allows for gradual ramp-up.</li>



<li><strong>Latency-based feedback loop</strong>: Instead of waiting for rate-limit errors, measure the response time of LLM calls. If latency increases, reduce concurrency; if latency decreases and success rates remain high, cautiously scale up.</li>
</ul>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144028">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-label="The AI Revolution in Medicine, Revisited" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Episode7-PeterBillSebastien-AIRevolution_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshot of Bill Gates, Peter Lee, and Sébastien Bubeck" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">The AI Revolution in Medicine, Revisited</h2>
				
								<p id="the-ai-revolution-in-medicine-revisited" class="large">Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-describedby="the-ai-revolution-in-medicine-revisited" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="optimization-experiments">Optimization experiments</h2>



<p>To further improve throughput and efficiency, we ran a series of optimization experiments. Each approach came with trade-offs that we carefully measured.</p>



<h3 class="wp-block-heading" id="batch-endpoints-azure-openai">Batch endpoints (Azure/OpenAI)</h3>



<p>Batch endpoints are a cost-effective, moderately high-throughput way of executing LLM requests. Batch endpoints process large lists of LLM prompts over a 24-hour period, recording responses in a file. They are about 50% cheaper than non-batch endpoints and have separate token limits, enabling increased throughput when used alongside regular endpoints. However, they require at least 24 hours to complete requests and provide lower overall throughput compared to non-batch endpoints, making them unsuitable for situations needing quick results.</p>



<h3 class="wp-block-heading" id="conversation-batching-in-prompts-during-pipeline-runtime">Conversation batching in prompts during pipeline runtime</h3>



<p>Batching multiple conversations for classification at once can significantly increase throughput and reduce token usage, but it may impact the accuracy of results. In our experiment with a domain classifier, classifying 10 conversations simultaneously led to an average of 15-20% of domain assignments changing between repeated runs of the same prompt. To address this, one mitigation approach is to use a grader LLM prompt: first classify the batch, then have the LLM identify any incorrectly classified conversations, and finally re-classify those as needed. While batching offers efficiency gains, it is important to monitor for potential drops in classification quality.</p>



<h3 class="wp-block-heading" id="combining-classifiers-in-a-single-prompt">Combining classifiers in a single prompt</h3>



<p>Combining multiple classifiers into a single prompt increases throughput by allowing one call to the LLM instead of multiple calls. This not only multiplies the overall throughput by the number of classifiers processed but also reduces the total number of tokens used, since the conversation text is only passed in once. However, this approach may compromise classification accuracy, so results should be closely monitored.</p>



<h3 class="wp-block-heading" id="classification-using-text-embeddings">Classification using text embeddings</h3>



<p>An alternative approach is to train custom neural network models for each classifier using only the text embeddings of conversations. This method delivers both cost and time savings by avoiding making multiple LLM requests for every classifier and conversation—instead, the system only needs to request conversation text embeddings once and can reuse these embeddings across all classifier models.</p>



<p>For example, starting with a set of conversations to validate and test the new model, run these conversations through the original prompt-based classifier to generate a set of golden classifications, then obtain text embeddings (using a tool like text-embedding-3-large) for each conversation. These embeddings and their corresponding classifications are used to train a model such as a multi-layer perceptron. In production, the workflow involves retrieving the text embedding for each conversation and passing it through the trained model; if there is a model for each classifier, a single embedding retrieval per conversation suffices for all classifiers.</p>



<p>The benefits of this approach include significantly increased throughput and cost savings—since it’s not necessary to call the LLM for every classifier and conversation. However, this setup can require GPU compute which can increase costs and infrastructure complexity, and the resulting models may not achieve the same accuracy as prompt-based classification methods.</p>



<h3 class="wp-block-heading" id="prompt-compression">Prompt compression</h3>



<p>Compressing prompts by eliminating unnecessary tokens or by using a tool such as <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/LLMLingua" target="_blank" rel="noreferrer noopener">LLMLingua<span class="sr-only"> (opens in new tab)</span></a> to automate prompt compression can optimize classification prompts either ahead of time or in real-time. This approach increases overall throughput and results in cost savings due to a reduced number of tokens, but there are risks: changes to the classifier prompt or conversation text may impact classification accuracy, and depending on the compression technique, it could even decrease throughput if the compression process takes longer than simply sending uncompressed text to the LLM.</p>



<h3 class="wp-block-heading" id="text-truncation">Text truncation</h3>



<p>Truncating conversations to a specific length limits the overall number of tokens sent through an endpoint, offering cost savings and increased throughput like prompt compression. By reducing the number of tokens per request, throughput rises because more requests can be made before reaching the endpoint’s tokens-per-minute (TPM) limit, and costs decrease due to fewer tokens being processed. However, the ideal truncation length depends on both the classifiers and the conversation content, so it’s important to assess how truncation affects output quality before implementation. While this approach brings clear efficiency benefits, it also poses a risk: long conversations may have their most important content cut off, which can reduce classification accuracy.</p>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>Building a scalable, high-throughput pipeline for LLM-based classification is far from trivial. It requires navigating a constantly shifting landscape of model capabilities, prompt behaviors, and infrastructure constraints. As LLMs become faster, cheaper, and more capable, they’re unlocking new possibilities for real-time understanding of human-AI interactions at scale. The techniques we’ve shared represent a snapshot of what’s working today. But more importantly, they offer a foundation for what’s possible tomorrow.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/technical-approach-for-classifying-human-ai-interactions-at-scale/">Technical approach for classifying human-AI interactions at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>CollabLLM: Teaching LLMs to collaborate with users</title>
		<link>https://www.microsoft.com/en-us/research/blog/collabllm-teaching-llms-to-collaborate-with-users/</link>
		
		<dc:creator><![CDATA[Shirley Wu, Michel Galley, Baolin Peng, Swadheen Shukla, Jianfeng Gao]]></dc:creator>
		<pubDate>Tue, 15 Jul 2025 18:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1144588</guid>

					<description><![CDATA[<p>Recipient of an ICML 2025 Outstanding Paper Award, CollabLLM improves how LLMs collaborate with users, including knowing when to ask questions and how to adapt tone and communication style to different situations. This approach helps move AI toward more user-centric and trustworthy systems.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/collabllm-teaching-llms-to-collaborate-with-users/">CollabLLM: Teaching LLMs to collaborate with users</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update.jpg" alt="CollabLLM blog hero | flowchart diagram starting in the upper left corner with an icon of two overlapping chat bubbles; arrow pointing right to an LLM network node icon; branching down to show three simulated users; right arrow to a "Reward" box" class="wp-image-1144599" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM-BlogHeroFeature-1400x788_Update-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Large language models (LLMs) can solve complex puzzles in seconds, yet they sometimes struggle over simple conversations. When these AI tools make assumptions, overlook key details, or neglect to ask clarifying questions, the result can erode trust and derail real-world interactions, where nuance is everything.</p>



<p>A key reason these models behave this way lies in how they’re trained and evaluated. Most benchmarks use isolated, single-turn prompts with clear instructions. Training methods tend to optimize for the model&#8217;s next response, not its contribution to a successful, multi-turn exchange. But real-world interaction is dynamic and collaborative. It relies on context, clarification, and shared understanding.</p>



<h2 class="wp-block-heading" id="user-centric-approach-to-training">User-centric approach to training&nbsp;</h2>



<p>To address this, we’re exploring ways to train LLMs with users in mind. Our approach places models in simulated environments that reflect the back-and-forth nature of real conversations. Through reinforcement learning, these models improve through trial and error, for example, learning when to ask questions and how to adapt tone and communication style to different situations. This user-centric approach helps bridge the gap between how LLMs are typically trained and how people actually use them.  </p>



<p>This is the concept behind <a href="https://www.microsoft.com/en-us/research/publication/collabllm-from-passive-responders-to-active-collaborators/">CollabLLM<span class="sr-only"> (opens in new tab)</span></a>, recipient of an <a href="https://www.microsoft.com/en-us/research/event/icml-2025/">ICML<span class="sr-only"> (opens in new tab)</span></a> <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://icml.cc/virtual/2025/awards_detail" target="_blank" rel="noreferrer noopener">Outstanding Paper Award<span class="sr-only"> (opens in new tab)</span></a>. This training framework helps LLMs improve through simulated multi-turn interactions, as illustrated in Figure 1. The core insight behind CollabLLM is simple: in a constructive collaboration, the value of a response isn’t just in its immediate usefulness, but in how it contributes to the overall success of the conversation. A clarifying question might seem like a delay but often leads to better outcomes. A quick answer might appear useful but can create confusion or derail the interaction.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1365" height="486" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig1.png" alt="Figure 1 compares two training strategies for Large Language Models: a standard non-collaborative method and our proposed collaborative method (CollabLLM). On the left, the standard method uses a preference/reward dataset with single-turn evaluations, resulting in a model that causes ineffective interactions. The user gives feedback, but the model generates multiple verbose and unsatisfactory responses, requiring many back-and-forth turns. On the right, CollabLLM incorporates collaborative simulation during training, using multi-turn interactions and reinforcement learning. After training, the model asks clarifying questions (e.g., tone preferences), receives focused user input, and quickly generates tailored, high-impact responses." class="wp-image-1144594" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig1.png 1365w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig1-300x107.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig1-1024x365.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig1-768x273.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig1-240x85.png 240w" sizes="auto, (max-width: 1365px) 100vw, 1365px" /><figcaption class="wp-element-caption">Figure 1. Diagram comparing two training approaches for LLMs. (a) The standard method lacks user-agent collaboration and uses single-turn rewards, leading to an inefficient conversation. (b) In contrast, CollabLLM simulates multi-turn user-agent interactions during training, enabling it to learn effective collaboration strategies and produce more efficient dialogues.</figcaption></figure>



<p>CollabLLM puts this collaborative approach into practice with a simulation-based training loop, illustrated in Figure 2. At any point in a conversation, the model generates multiple possible next turns by engaging in a dialogue with a simulated user.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="970" height="438" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig2.png" alt="Figure 2 illustrates the overall training procedure of CollabLLM. For a given conversational input, the LLM and a user simulator are used to sample conversation continuations. The sampled conversations are then scored using a reward model that utilizes various multiturn-aware rewards, which are then in turn used to update parameters of the LLM." class="wp-image-1144593" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig2.png 970w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig2-300x135.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig2-768x347.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig2-240x108.png 240w" sizes="auto, (max-width: 970px) 100vw, 970px" /><figcaption class="wp-element-caption">Figure 2: Simulation-based training process used in CollabLLM</figcaption></figure>



<p>The system uses a sampling method to extend conversations turn by turn, choosing likely responses for each participant (the AI agent or the simulated user), while adding some randomness to vary the conversational paths. The goal is to expose the model to a wide variety of conversational scenarios, helping it learn more effective collaboration strategies.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="999693">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Event Series</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/event/microsoft-research-forum/?OCID=msr_researchforum_MCR_Blog_Promo" aria-label="Microsoft Research Forum" data-bi-cN="Microsoft Research Forum" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Research-Forum-hero_1400x788.jpg" alt="Research Forum | abstract background with colorful hexagons" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Forum</h2>
				
								<p id="microsoft-research-forum" class="large">Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/event/microsoft-research-forum/?OCID=msr_researchforum_MCR_Blog_Promo" aria-describedby="microsoft-research-forum" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Forum" target="_blank">
							Watch on-demand						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<p>To each simulated conversation, we applied multiturn-aware reward (MR) functions, which assess how the model’s response at the given turn influences the entire trajectory of the conversation. We sampled multiple conversational follow-ups from the model, such as statements, suggestions, questions, and used MR to assign a reward to each based on how well the conversation performed in later turns. We based these scores on automated metrics that reflect key factors like goal completion, conversational efficiency, and user engagement.</p>



<p>To score the sampled conversations, we used task-specific metrics and metrics from an LLM-as-a-judge framework, which supports efficient and scalable evaluation. For metrics like engagement, a judge model rates each sampled conversation on a scale from 0 to 1.</p>



<p>The MR of each model response was computed by averaging the scores from the sampled conversations, originating from the model response. Based on the score, the model updates its parameters using established reinforcement learning algorithms like Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO).</p>



<p>We tested CollabLLM through a combination of automated and human evaluations, detailed in the <a href="https://www.microsoft.com/en-us/research/publication/collabllm-from-passive-responders-to-active-collaborators/">paper</a>. One highlight is a user study involving 201 participants in a document co-creation task, shown in Figure 3. We compared CollabLLM to a baseline trained with single-turn rewards and to a second, more proactive baseline prompted to ask clarifying questions and take other proactive steps. CollabLLM outperformed both, producing higher-quality documents, better interaction ratings, and faster task completion times.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1860" height="492" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig3.png" alt="Figure 3 shows the main results of our user study on a document co-creation task, by comparing a baseline, a proactive baseline, and CollabLLM. CollabLLM outperformed the two baselines. Relative to the best baseline, CollabLLM yields improved document quality rating (+0.12), interaction rating (+0.14), and a reduction of average time spent by the user (-129 seconds)." class="wp-image-1144597" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig3.png 1860w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig3-300x79.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig3-1024x271.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig3-768x203.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig3-1536x406.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/CollabLLM_fig3-240x63.png 240w" sizes="auto, (max-width: 1860px) 100vw, 1860px" /><figcaption class="wp-element-caption">Figure 3: Results of the user study in a document co-creation task comparing CollabLLM to a baseline trained with single-turn rewards.</figcaption></figure>



<h2 class="wp-block-heading" id="designing-for-real-world-collaboration">Designing for real-world collaboration</h2>



<p>Much of today’s AI research focuses on fully automated tasks, models working without input from or interaction with users. But many real-world applications depend on people in the loop: as users, collaborators, or decision-makers. Designing AI systems that treat user input not as a constraint, but as essential, leads to systems that are more accurate, more helpful, and ultimately more trustworthy.</p>



<p>This work is driven by a core belief: the future of AI depends not just on intelligence, but on the ability to collaborate effectively. And that means confronting the communication breakdowns in today’s systems.</p>



<p>We see CollabLLM as a step in that direction, training models to engage in meaningful multi-turn interactions, ask clarifying questions, and adapt to context. In doing so, we can build systems designed to work <em>with</em> people—not around them.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/collabllm-teaching-llms-to-collaborate-with-users/">CollabLLM: Teaching LLMs to collaborate with users</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>PadChest-GR: A bilingual grounded radiology reporting benchmark for chest X-rays</title>
		<link>https://www.microsoft.com/en-us/research/blog/padchest-gr-a-bilingual-grounded-radiology-reporting-benchmark-for-chest-x-rays/</link>
		
		<dc:creator><![CDATA[Daniel Coelho de Castro, Javier Alvarez-Valle]]></dc:creator>
		<pubDate>Thu, 26 Jun 2025 16:08:25 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1142540</guid>

					<description><![CDATA[<p>The world’s first multimodal, bilingual radiology dataset could reshape the way radiologists and AI systems make sense of X-rays. PadChest-GR, developed by the University of Alicante with Microsoft Research, has the potential to advance research across the field for years to come.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/padchest-gr-a-bilingual-grounded-radiology-reporting-benchmark-for-chest-x-rays/">PadChest-GR: A bilingual grounded radiology reporting benchmark for chest X-rays</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1.jpg" alt="Alt text: The image features three white icons on a gradient background transitioning from blue on the left to green on the right. The first icon, located on the left, resembles an X-ray of a ribcage enclosed in a square with rounded corners. The middle icon depicts a hierarchical structure with one circle at the top connected by lines to two smaller circles below it. The third icon, positioned on the right, shows the letters "N" and "A" separated by a diagonal line, with a tilde (~) above the "N"." class="wp-image-1142658" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/PadChest-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>In our ever-evolving journey to enhance healthcare through technology, we’re announcing a unique new benchmark for grounded radiology report generation—<strong><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIdbp2401120" target="_blank" rel="noreferrer noopener">PadChest-GR<span class="sr-only"> (opens in new tab)</span></a></strong>. The world’s first multimodal, bilingual sentence-level radiology report dataset, developed&nbsp;by the University of Alicante with Microsoft Research, University Hospital Sant Joan d’Alacant and MedBravo, is set to redefine how AI and radiologists interpret radiological images. Our work demonstrates how collaboration between humans and AI can create powerful feedback loops—where new datasets drive better AI models, and those models, in turn, inspire richer datasets. We&#8217;re excited to share this progress in NEJM AI, highlighting both the clinical relevance and research excellence of this initiative.&nbsp;</p>



<h2 class="wp-block-heading" id="a-new-frontier-in-radiology-report-generation">A new frontier in radiology report generation&nbsp;</h2>



<p>It is estimated that over half of people visiting hospitals have radiology scans that must be interpreted by a clinical professional. Traditional radiology reports often condense multiple findings into unstructured narratives. In contrast, grounded radiology reporting demands that each finding be described and localized individually.</p>



<p>This can mitigate the risk of AI fabrications and enable new interactive capabilities that enhance clinical and patient interpretability. PadChest-GR is the first bilingual dataset to address this need with 4,555 chest X-ray studies complete with Spanish and English sentence-level descriptions and precise spatial (bounding box) annotations for both positive and negative findings. It is the first public benchmark that enables us to evaluate generation of fully grounded radiology reports in chest X-rays.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1516" height="781" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/grounded_report_example.png" alt="Figure 1: A chest X-ray overlaid with numbered bounding boxes, next to a matching list of structured radiological findings in Spanish and English. " class="wp-image-1142582" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/grounded_report_example.png 1516w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/grounded_report_example-300x155.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/grounded_report_example-1024x528.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/grounded_report_example-768x396.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/grounded_report_example-240x124.png 240w" sizes="auto, (max-width: 1516px) 100vw, 1516px" /><figcaption class="wp-element-caption">Figure 1. Example of a grounded report from PadChest-GR. The original free-text report in Spanish was <em>”Motivo de consulta: Preoperatorio. Rx PA tórax: Impresión diagnóstica: Ateromatosis aórtica calcificada. Engrosamiento pleural biapical. Atelectasia laminar basal izquierda. Elongación aórtica. Sin otros hallazgos radiológicos significativos.”</em></figcaption></figure>



<p>This benchmark isn’t standing alone—it plays a critical role in powering our state-of-the-art multimodal report generation model, <strong>MAIRA-2</strong>. Leveraging the detailed annotations of PadChest-GR, MAIRA-2 represents our commitment to building more interpretable and clinically useful AI systems. You can explore our work on MAIRA-2 on our <a href="https://www.microsoft.com/en-us/research/project/project-maira/">project web page</a>, including recent user research conducted with <a href="https://www.microsoft.com/en-us/research/publication/multimodal-healthcare-ai-identifying-and-designing-clinically-relevant-vision-language-applications-for-radiology/" target="_blank" rel="noreferrer noopener">clinicians in healthcare settings</a>.</p>



<p>PadChest-GR is a testament to the power of collaboration. Aurelia Bustos at MedBravo and Antonio Pertusa at the University of Alicante published the original&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://bimcv.cipf.es/bimcv-projects/padchest/" target="_blank" rel="noreferrer noopener">PadChest dataset<span class="sr-only"> (opens in new tab)</span></a> in 2020,&nbsp;with the help of Jose María Salinas from Hospital San Juan de Alicante and María de la Iglesia Vayá from the Center of Excellence in Biomedical Imaging at the Ministry of Health in Valencia, Spain. We started to look at PadChest and were deeply impressed by the scale, depth, and diversity of the data.</p>



<p>As we worked more closely with the dataset, we realized the opportunity to develop this for grounded radiology reporting research and worked with the team at the University of Alicante to determine how to approach this together. Our complementary expertise was a nice fit. At Microsoft Research, our mission is to push the boundaries of medical AI through innovative, data-driven solutions. The University of Alicante, with its deep clinical expertise, provided critical insights that greatly enriched the dataset’s relevance and utility. The result of this collaboration is the PadChest-GR dataset.</p>



<p>A significant enabler of our annotation process was <strong>Centaur Labs</strong>. The team of senior and junior radiologists from the University Hospital Sant Joan d’Alacant, coordinated by Joaquin Galant,&nbsp;used this HIPAA-compliant labeling platform to&nbsp;perform rigorous study-level quality control and bounding box annotations. The annotation protocol implemented ensured that each annotation was accurate and consistent, forming the backbone of a dataset designed for the next generation of grounded radiology report generation models.&nbsp;</p>



<h2 class="wp-block-heading" id="accelerating-padchest-gr-dataset-annotation-with-ai">Accelerating PadChest-GR dataset annotation with AI&nbsp;</h2>



<p>Our approach integrates advanced large language models with comprehensive manual annotation:&nbsp;</p>



<p><strong>Data Selection & Processing:</strong> Leveraging <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" target="_blank" rel="noreferrer noopener">Microsoft Azure OpenAI Service<span class="sr-only"> (opens in new tab)</span></a> with GPT-4, we extracted sentences describing individual positive and negative findings from raw radiology reports, translated them from Spanish to English, and linked each sentence to the existing expert labels from PadChest. This was done for a selected subset of the full PadChest dataset, carefully curated to reflect a realistic distribution of clinically relevant findings.&nbsp;</p>



<p><strong>Manual Quality Control & Annotation:</strong> The processed studies underwent meticulous quality checks on the Centaur Labs platform by radiologist from Hospital San Juan de Alicante. Each positive finding was then annotated with bounding boxes to capture critical spatial information.&nbsp;</p>



<p><strong>Standardization & Integration:</strong> All annotations were harmonized into coherent grounded reports, preserving the structure and context of the original findings while enhancing interpretability.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1752" height="790" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/dataflow.png" alt="Figure 2: A detailed block diagram illustrating the flow of data between various stages of AI processing and manual annotation. " class="wp-image-1142586" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/dataflow.png 1752w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/dataflow-300x135.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/dataflow-1024x462.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/dataflow-768x346.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/dataflow-1536x693.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/dataflow-240x108.png 240w" sizes="auto, (max-width: 1752px) 100vw, 1752px" /><figcaption class="wp-element-caption">Figure 2. Overview of the data curation pipeline.</figcaption></figure>



<h2 class="wp-block-heading" id="impact-and-future-directions">Impact and future directions&nbsp;</h2>



<p>PadChest-GR not only sets a new benchmark for grounded radiology reporting, but also serves as the foundation for our MAIRA-2 model, which already showcases the potential of highly interpretable <a href="https://www.microsoft.com/en-us/industry/blog/healthcare/2025/05/19/developing-next-generation-cancer-care-management-with-multi-agent-orchestration/" target="_blank" rel="noreferrer noopener">AI in clinical settings</a>. While we developed PadChest-GR to help train and validate our own models, we believe the research community will greatly benefit from this dataset for many years to come. We look forward to seeing the broader research community build on this—improving grounded reporting AI models and using PadChest-GR as a standard for evaluation. We believe that by fostering open collaboration and sharing our resources, we can accelerate progress in medical imaging AI and ultimately improve patient care together with the community.</p>



<p>The collaboration between Microsoft Research and the University of Alicante highlights the transformative power of working together across disciplines. With our publication in NEJM-AI and the integral role of PadChest-GR in the development of <a href="https://www.microsoft.com/en-us/research/publication/maira-2-grounded-radiology-report-generation/" target="_blank" rel="noreferrer noopener">MAIRA-2<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/radfact" target="_blank" rel="noreferrer noopener">RadFact<span class="sr-only"> (opens in new tab)</span></a>, we are excited about the future of AI-empowered radiology. We invite researchers and industry experts to explore PadChest-GR and MAIRA-2, contribute innovative ideas, and join us in advancing the field of grounded radiology reporting.&nbsp;</p>



<p>Papers already using PadChest-GR:</p>



<ul class="wp-block-list">
<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" target="_blank" href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Farxiv.org%2Fabs%2F2406.04449&data=05%7C02%7Cv-ammelfi%40microsoft.com%7C3a510e2a628c41f431e608ddb23acd37%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638862687830178131%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=BI0coh1EYtLPEme8ygYDgaY8OLiLxA7kJj0dj3KXvNM%3D&reserved=0">[2406.04449] MAIRA-2: Grounded Radiology Report Generation<span class="sr-only"> (opens in new tab)</span></a></li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" target="_blank" href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Farxiv.org%2Fabs%2F2502.03333&data=05%7C02%7Cv-ammelfi%40microsoft.com%7C3a510e2a628c41f431e608ddb23acd37%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638862687830198918%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=udnptjC4kQA22qIDmTCwoSJI4ol%2Fp95%2FOsidJdZ4CWc%3D&reserved=0">RadVLM: A Multitask Conversational Vision-Language Model for Radiology<span class="sr-only"> (opens in new tab)</span></a></li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" target="_blank" href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.03278&data=05%7C02%7Cv-ammelfi%40microsoft.com%7C3a510e2a628c41f431e608ddb23acd37%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638862687830212221%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=taRcdqfs7Dis0FxmUDJDAr7DGmggLDyf9et2pYu0mm8%3D&reserved=0">Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions<span class="sr-only"> (opens in new tab)</span></a></li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" target="_blank" href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3D0Jn1d4gYRS&data=05%7C02%7Cv-ammelfi%40microsoft.com%7C3a510e2a628c41f431e608ddb23acd37%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638862687830225142%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=QcvL%2FgUDBqxkr5Zxtx9PwSLZsYwEKWdoGaC9LHKxr7Q%3D&reserved=0">Visual Prompt Engineering for Vision Language Models in Radiology<span class="sr-only"> (opens in new tab)</span></a></li>
</ul>



<p>For further details or to download PadChest-GR, please visit the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://bimcv.cipf.es/bimcv-projects/padchest-gr/" target="_blank" rel="noreferrer noopener">BIMCV PadChest-GR Project<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>



<p>Models in the Azure Foundry that can do Grounded Reporting:&nbsp;</p>



<ul class="wp-block-list">
<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" target="_blank" href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fhow-to%2Fhealthcare-ai%2Fdeploy-cxrreportgen&data=05%7C02%7Cv-ammelfi%40microsoft.com%7C3a510e2a628c41f431e608ddb23acd37%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638862687830239988%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=dvRiCJL5l9vOk89pdPgmjPBVOtiHIzK5DZ7uhGbRk0Q%3D&reserved=0">How to deploy and use CXRReportGen healthcare AI model with Azure AI Foundry &#8211; Azure AI Foundry | Microsoft Learn<span class="sr-only"> (opens in new tab)</span></a></li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" target="_blank" href="https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fhealth-bot%2Fcopilot%2Forchestrator&data=05%7C02%7Cv-ammelfi%40microsoft.com%7C3a510e2a628c41f431e608ddb23acd37%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C638862687830255286%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=t3r8%2BHHR1KvoLZi4YnI44DYT855MbYZisNc4f5f1OTg%3D&reserved=0">Healthcare Orchestrator &#8211; Healthcare agent service | Microsoft Learn<span class="sr-only"> (opens in new tab)</span></a></li>
</ul>



<h2 class="wp-block-heading" id="acknowledgement">Acknowledgement</h2>



<ul class="wp-block-list">
<li>Authors: <a href="https://www.microsoft.com/en-us/research/people/dacoelh/">Daniel C. Castro<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/search/cs?searchtype=author&query=Bustos,+A" target="_blank" rel="noreferrer noopener">Aurelia Bustos<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/people/shbannur/">Shruthi Bannur<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/people/sthyland/">Stephanie L. Hyland<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/people/kenzabouzid/">Kenza Bouzid<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/search/cs?searchtype=author&query=Wetscherek,+M+T" target="_blank" rel="noreferrer noopener">Maria Teodora Wetscherek<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/search/cs?searchtype=author&query=S%C3%A1nchez-Valverde,+M+D" target="_blank" rel="noreferrer noopener">Maria Dolores Sánchez-Valverde<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/search/cs?searchtype=author&query=Jaques-P%C3%A9rez,+L" target="_blank" rel="noreferrer noopener">Lara Jaques-Pérez<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/search/cs?searchtype=author&query=P%C3%A9rez-Rodr%C3%ADguez,+L" target="_blank" rel="noreferrer noopener">Lourdes Pérez-Rodríguez<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/people/kenjitak/" target="_blank" rel="noreferrer noopener">Kenji Takeda<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/search/cs?searchtype=author&query=Salinas,+J+M" target="_blank" rel="noreferrer noopener">José María Salinas<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/people/jaalvare/">Javier Alvarez-Valle<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/search/cs?searchtype=author&query=Herrero,+J+G" target="_blank" rel="noreferrer noopener">Joaquín Galant Herrero<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/search/cs?searchtype=author&query=Pertusa,+A" target="_blank" rel="noreferrer noopener">Antonio Pertusa<span class="sr-only"> (opens in new tab)</span></a>&nbsp;</li>
</ul>



<ul class="wp-block-list">
<li>MSR Health Futures UK: <a href="https://www.microsoft.com/en-us/research/people/hamurfet/">Hannah Richardson</a>, <a href="https://www.microsoft.com/en-us/research/people/vsalvatelli/">Valentina Salvatelli</a>, <a href="https://www.microsoft.com/en-us/research/people/harssharma/">Harshita Sharma</a>, <a href="https://www.microsoft.com/en-us/research/people/sbondtaylor/">Sam Bond-Taylor</a>, <a href="https://www.microsoft.com/en-us/research/people/maxilse/">Max Ilse</a>, <a href="https://www.microsoft.com/en-us/research/people/fperezgarcia/">Fernando Perez-Garcia</a>, <a href="https://www.microsoft.com/en-us/research/people/antonsc/">Anton Schwaighofer</a>, <a href="https://www.microsoft.com/en-us/research/people/carlson/">Jonathan Carlson</a> </li>
</ul>



<ul class="wp-block-list">
<li>MSR Flow: <a href="https://www.microsoft.com/en-us/research/people/kenjitak/">Kenji Takeda</a>, <a href="https://www.microsoft.com/en-us/research/people/evelynev/">Evelyn Viegas</a>, <a href="https://www.microsoft.com/en-us/research/people/allorens/">Ashley Llorens</a></li>
</ul>



<ul class="wp-block-list">
<li>HLS: <a href="https://www.microsoft.com/en-us/research/people/mlungren/">Matthew Lungren</a>, <a href="https://www.microsoft.com/en-us/research/people/naiteeks/">Naiteek Sangani</a>, <a href="https://www.microsoft.com/en-us/research/people/shreyjain/">Shrey Jain</a>, <a href="https://www.microsoft.com/en-us/research/people/itarapov/">Ivan Tarapov</a>, <a href="https://www.microsoft.com/en-us/research/people/wguyman/">Will Guyman</a>, Mert Oez, Chris Burt, David Ardman</li>
</ul>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/padchest-gr-a-bilingual-grounded-radiology-reporting-benchmark-for-chest-x-rays/">PadChest-GR: A bilingual grounded radiology reporting benchmark for chest X-rays</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
