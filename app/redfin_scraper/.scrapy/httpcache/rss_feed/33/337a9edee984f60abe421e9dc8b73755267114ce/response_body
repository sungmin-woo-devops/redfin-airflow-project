<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>The Stanford AI Lab Blog</title>
        <atom:link href="/blog/feed.xml" rel="self" type="application/rss+xml"/>
        <link>http://ai.stanford.edu/blog/</link>
        <description>The Stanford AI Lab (SAIL) Blog is a place for SAIL students, faculty, and researchers to share our work with the general public.</description>
        <pubDate>Sat, 04 Jun 2022 12:11:43 -0700</pubDate>
        
          
          <item>
              <title>LinkBERT: Improving Language Model Training with Document Link</title>
              <link>/blog/linkbert/</link>
              <guid isPermaLink="true">/blog/linkbert/</guid>
              <description>&lt;h3 id=&quot;language-model-pretraining&quot;&gt;&lt;strong&gt;Language Model Pretraining&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Language models (LMs), like BERT &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and the GPT series &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, achieve remarkable performance on many natural language processing (NLP) tasks. They are now the foundation of today’s NLP systems. &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; These models serve important roles in products and tools that we use every day, such as search engines like Google &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; and personal assistants like Alexa &lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;These LMs are powerful because they can be &lt;strong&gt;pretrained&lt;/strong&gt; via self-supervised learning on massive amounts of text data on the web without the need for labels, after which the pretrained models can be quickly adapted to a wide range of new tasks without much task-specific finetuning. For instance, BERT is pretrained to predict randomly masked words in original text (masked language modeling), e.g. predicting the masked word “dog” from “My __ is fetching the ball”. GPTs are pretrained to predict the next word given a previous sequence of text (causal language modeling), e.g. predicting the next word “ball” from “My dog is fetching the”. In either cases, through pretraining, LMs learn to encode various knowledge from a text corpus that helps to perform downstream applications involving language understanding or generation. In particular, LMs can learn world knowledge (associations between concepts like “dog”, “fetch”, “ball”) from training text where the concepts appear together, and help for knowledge-intensive applications like question answering. &lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/intro.png&quot;&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/intro.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Challenges.&lt;/strong&gt;&lt;br /&gt;
A challenge with most common LM pretraining strategies is that they model a single document at a time. That is, one would split a text corpus into a list of documents and draw training instances for LMs from each document independently. Treating each document independently may pose limitations because &lt;strong&gt;documents often have rich dependencies with each other&lt;/strong&gt;. For instance, text from the web &lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; or scientific literature &lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; is often used for LM training, but they all have document links, such as hyperlinks and citation links. Document links are crucial because knowledge can span across multiple documents beyond a single document. As an example, the Wikipedia article “Tidal Basin, Washington D.C.” shown on the left of the figure below describes that the basin hosts “National Cherry Blossom Festival”, and if we jump to the hyperlinked article shown on the right, we see that the festival celebrates “Japanese cherry trees”. Combined, the hyperlink offers new, multi-hop knowledge such as “Tidal Basin has Japanese cherry trees”, which is not available in the original single document alone.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/links.png&quot;&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/links.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Models that train without these dependencies may fail to capture knowledge or facts that are spread across multiple documents. Learning such multi-hop knowledge in pretraining can be important for various applications including question answering and knowledge discovery (e.g. “What trees can you see at the Tidal Basin?”). Indeed, document links like hyperlinks and citations are ubiquitous and we humans also use them all the time to learn new knowledge and make discoveries. A text corpus is thus not simply a list of documents but a &lt;strong&gt;graph&lt;/strong&gt; of documents with links connecting each other.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/abs/2203.15827&quot;&gt;our recent work&lt;/a&gt; &lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; published at ACL 2022, we develop a new pretraining method, &lt;strong&gt;&lt;em&gt;LinkBERT&lt;/em&gt;&lt;/strong&gt;, that incorporates such document link information to train language models with more world knowledge.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/existing_vs_new.png&quot;&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/existing_vs_new.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;approach-linkbert&quot;&gt;&lt;strong&gt;Approach: LinkBERT&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;At a high level, LinkBERT consists of three steps: (0) obtaining links between documents to build a document graph from the text corpus, (1) creating link-aware training instances from the graph by placing linked documents together, and finally (2) pretraining the LM with link-aware self-supervised tasks: masked language modeling (MLM) and document relation prediction (DRP).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/overview.png&quot;&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/overview.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Document Graph Construction.&lt;/strong&gt;&lt;br /&gt;
Given a text corpus, we link related documents to make a graph so that the links can bring together useful knowledge. Although this graph can be derived in any way, we will focus on using hyperlinks and citation links as they generally have high quality of relevance (i.e. low false-positive rate) and are available ubiquitously at scale. &lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; To make the document graph, we treat each document as a node, and add a directed edge (i, j) if there is a hyperlink from document i to document j.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Link-aware LM Input Creation.&lt;/strong&gt;&lt;br /&gt;
Given the document graph, we then create link-aware inputs that will be fed into our LM. As LMs can learn token dependency effectively if the tokens are shown in the same input instance &lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;, we want to place linked documents together in the input instance. In this way, the LM can learn multi-hop or multi-document dependencies of concepts as the LM will see training instances where these concepts appear together in the same sequence. To achieve this, we first chunk each document into segments of roughly 256 tokens, which is half of the maximum BERT LM input length. Then, we concatenate two segments (Segment A and B) together as an input sequence for the LM according to the links in the document graph. We have three options for how to choose segments to concatenate together:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Option 1: contiguous segments&lt;/strong&gt;. Take two contiguous segments from the same document. This is essentially the same as previous LMs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Option 2: random segments&lt;/strong&gt;. Sample one segment from a random document and sample a second segment from another random document.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Option 3: linked segments&lt;/strong&gt;. Sample one segment from a random document and sample a second segment randomly from a document linked to the first document in the document graph.&lt;br /&gt;
The reason we have these three options is to create a training signal for LinkBERT such that the model will learn to recognize relations between text segments. We will explain this in the next paragraph.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/lm_input_v2.png&quot;&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/lm_input_v2.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Link-aware LM Pretraining.&lt;/strong&gt;&lt;br /&gt;
After creating input instances made up of pairs of segments, the last step is to use these created inputs to train the LM with link-aware self-supervised tasks. We consider the following two tasks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Masked language modeling (MLM)&lt;/strong&gt;, which masks some tokens in the input text and then predicts the tokens using the surrounding tokens. This encourages the LM to learn multi-hop knowledge of concepts brought into the same context by document links. For instance, in our running example about Tidal Basin, the LM would be able to learn the multi-hop relations from “Tidal Basin” to “National Cherry Blossom Festival” to “Japanese cherry trees”, as these three concepts will be all presented together in the same training instances.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Document relation prediction (DRP)&lt;/strong&gt;, which makes the model classify the relation of Segment B to Segment A as to whether the two segments are contiguous, random or linked. This task encourages the LM to learn relevance and dependencies between documents, and also learn bridging concepts such as “National Cherry Blossom Festival” in our running example.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We pretrain the LM with these two objectives jointly.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/objective.png&quot;&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/objective.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We can also motivate these two pretraining tasks as performing self-supervised learning algorithms on the document graph:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Node feature prediction &lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;, which is to predict masked features of a node using neighbor nodes. This corresponds to MLM, where we predict masked tokens in Segment A using Segment B and vice versa.&lt;/li&gt;
  &lt;li&gt;Link prediction &lt;sup id=&quot;fnref:13&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;, which is to predict the existence or type of an edge between two nodes. This corresponds to DRP, where we predict if two segments are linked (edge), contiguous (self-loop), or random (no edge).&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/graph_ml.png&quot;&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/graph_ml.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;lets-use-linkbert&quot;&gt;&lt;strong&gt;Let’s use LinkBERT!&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We will now see how LinkBERT performs on several downstream natural language processing tasks. We pretrained LinkBERT in two domains:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;General domain: we use &lt;a href=&quot;https://en.wikipedia.org/wiki/Main_Page&quot;&gt;Wikipedia&lt;/a&gt; as the pretraining corpus, which is the same as previous language models like BERT, except that here we also use the hyperlinks between Wikipedia articles.&lt;/li&gt;
  &lt;li&gt;Biomedical domain: we use &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/&quot;&gt;PubMed&lt;/a&gt; as the pretraining corpus, which is the same as previous biomedical language models like PubmedBERT &lt;sup id=&quot;fnref:14&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;, except that here we also use the citation links between PubMed articles.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;LinkBERT improves previous BERT models on many applications.&lt;/strong&gt;&lt;br /&gt;
We evaluated the pretrained LinkBERT models on diverse downstream tasks in each domain:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;General question answering (&lt;a href=&quot;https://github.com/mrqa/MRQA-Shared-Task-2019&quot;&gt;MRQA&lt;/a&gt;) and general NLP (&lt;a href=&quot;https://gluebenchmark.com/&quot;&gt;GLUE&lt;/a&gt;) benchmarks&lt;/li&gt;
  &lt;li&gt;Biomedical NLP (&lt;a href=&quot;https://microsoft.github.io/BLURB/leaderboard.html&quot;&gt;BLURB&lt;/a&gt;) and biomedical question answering (&lt;a href=&quot;https://github.com/jind11/MedQA&quot;&gt;MedQA&lt;/a&gt;, &lt;a href=&quot;https://github.com/hendrycks/test&quot;&gt;MMLU&lt;/a&gt;) benchmarks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LinkBERT improves the baseline language models pretrained without document links (i.e. BERT and PubmedBERT) consistently across tasks and domains. The gain for the biomedical domain is especially large, likely because scientific literature has crucial dependencies with each other via citation links, which are captured by LinkBERT. The biomedical LinkBERT, which we call BioLinkBERT, achieves new state-of-the-art performance on the &lt;a href=&quot;https://microsoft.github.io/BLURB/leaderboard.html&quot;&gt;BLURB&lt;/a&gt;, MedQA and MMLU benchmarks.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/result.png&quot;&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/result.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Effective for multi-hop reasoning.&lt;/strong&gt;
LinkBERT exhibits several interesting strengths. The first strength is multi-hop reasoning. Within the MRQA benchmarks, there are several tasks that involve multi-hop reasoning such as HotpotQA and triviaQA, and we find that LinkBERT provides large improvements for BERT on those tasks. As an example, the figure below shows an example from HotpotQA. Answering the given question needs 2-hop reasoning because we need to know what organization took over Roden Brothers in 1953 (the first document), and then we need to know where that organization was headquartered (the second document). BERT tends to simply predict a location name that appears in the same document as the one about Roden Brothers (“Toronto”), but LinkBERT is able to correctly connect information across the two documents to predict the answer (“Montreal”). The intuition is that because LinkBERT brings together multiple related concepts/documents into the same input instances during pretraining, it helps the model to reason with multiple concepts/documents in downstream tasks.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/multihop.png&quot;&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/multihop.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Effective for document relation understanding.&lt;/strong&gt;
Another strength of LinkBERT is that it can better model the relationships between multiple documents. For instance, in open-domain QA, a model must identify an answer from multiple retrieved documents, where many of the documents are likely noisy or otherwise unrelated to the question. &lt;sup id=&quot;fnref:15&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; To simulate this, we added distracting documents to the original MRQA tasks such as SQuAD and HotpotQA. We find that LinkBERT is robust to irrelevant documents and maintains the QA accuracy, while BERT incurs a performance drop in this setup. Our intuition is that the Document Relation Prediction task used in pretraining helps recognizing document relevance in downstream tasks.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/distract.png&quot;&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/distract.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Effective for few-shot and data-efficient QA.&lt;/strong&gt;
The third strength is few-shot and data-efficient QA. For each QA dataset, we tried finetuning LinkBERT or BERT with only 10% or 1% of the available training data. We find that LinkBERT provides large improvements for BERT on these low-resource paradigms. This finding suggests that LinkBERT has internalized more knowledge than BERT during pretraining, and supports the original intuition that document links can bring in new, useful knowledge for LMs.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a href=&quot;/blog/assets/img/posts/2022-05-31-linkbert/fewshot.png&quot;&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-05-31-linkbert/fewshot.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;use-linkbert-for-your-own-applications&quot;&gt;&lt;strong&gt;Use LinkBERT for your own applications&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;LinkBERT can be used easily as a drop-in replacement for BERT. The pretrained LinkBERT models (LinkBERT and BioLinkBERT) are available on &lt;a href=&quot;https://huggingface.co/michiyasunaga&quot;&gt;HuggingFace&lt;/a&gt;, and you can load them by&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoModel&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'michiyasunaga/LinkBERT-large'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'michiyasunaga/LinkBERT-large'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hello, my dog is cute&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_tensors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoModel&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'michiyasunaga/BioLinkBERT-large'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'michiyasunaga/BioLinkBERT-large'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Sunitinib is a tyrosine kinase inhibitor&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_tensors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;To use LinkBERT for downstream applications such as question answering and text classification, you can use the finetuning scripts provided at &lt;a href=&quot;https://github.com/michiyasunaga/LinkBERT&quot;&gt;https://github.com/michiyasunaga/LinkBERT&lt;/a&gt;, or simply replace the BERT model path with LinkBERT in your favorite BERT finetuning scripts.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We introduced LinkBERT, a new pretraining method that leverages document links such as hyperlinks and citations to train a knowledgeable language model (LM). Specifically, we place linked documents in the same LM input sequence, and train the LM with two joint self-supervised tasks: masked language modeling and document relation prediction.&lt;/p&gt;

&lt;p&gt;LinkBERT can be used as a drop-in replacement for BERT. In addition to improving performance for general language understanding tasks (e.g. text classification), LinkBERT better captures document or concept relations, and is effective for multi-hop reasoning and cross-document understanding. LinkBERT also internalizes more world knowledge and is effective for knowledge-intensive tasks, such as few-shot question answering.&lt;/p&gt;

&lt;p&gt;We release the &lt;a href=&quot;https://github.com/michiyasunaga/LinkBERT&quot;&gt;pretrained LinkBERT models&lt;/a&gt;. We hope they can be helpful for your projects and research, especially knowledge or reasoning-intensive applications. Finally, we think that LinkBERT opens up many exciting future projects, such as generalizing to GPT or sequence-to-sequence &lt;sup id=&quot;fnref:16&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt; style language models to perform document link-aware text generation, and generalizing the notion of document links to other modalities, e.g., incorporating source code dependency links in the training of language models for code &lt;sup id=&quot;fnref:17&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This blog post is based on the paper:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/2203.15827&quot;&gt;LinkBERT: Pretraining Language Models with Document Links&lt;/a&gt;. Michihiro Yasunaga, Jure Leskovec and Percy Liang. ACL 2022. The models, code, data are available on &lt;a href=&quot;https://github.com/michiyasunaga/LinkBERT&quot;&gt;GitHub&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/michiyasunaga&quot;&gt;HuggingFace&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you have questions, please feel free to email us.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Michihiro Yasunaga: myasu@cs.stanford.edu&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;acknowledgments&quot;&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Many thanks to the members of the Stanford P-Lambda group, SNAP group and NLP group for their valuable feedback. Many thanks to Jacob Schreiber and Michael Zhang for edits on this blog post.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2019. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;Language Models are Few-Shot Learners&lt;/a&gt;. Tom B. Brown, et al. 2020. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.07258&quot;&gt;On the Opportunities and Risks of Foundation Models&lt;/a&gt;. Rishi Bommasani et al. 2021. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Google uses BERT for its search engine: &lt;a href=&quot;https://blog.google/products/search/search-language-understanding-bert/&quot;&gt;https://blog.google/products/search/search-language-understanding-bert/&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.03023.pdf&quot;&gt;Language Model is All You Need: Natural Language Understanding as Question Answering&lt;/a&gt;. Mahdi Namazifar et al. Alexa AI. 2020. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.01066&quot;&gt;Language Models as Knowledge Bases?&lt;/a&gt; Fabio Petroni, et al. 2019. &lt;a href=&quot;https://arxiv.org/abs/1906.05317&quot;&gt;COMET: Commonsense Transformers for Automatic Knowledge Graph Construction&lt;/a&gt;. Antoine Bosselut et al. 2019. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For example, text corpora like Wikipedia and WebText are used for training BERT and GPTs. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For example, text corpora like PubMed and Semantic Scholar are used for training language models in scientific domains, such as &lt;a href=&quot;https://arxiv.org/abs/1901.08746&quot;&gt;BioBERT&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1903.10676&quot;&gt;SciBERT&lt;/a&gt;. &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/2203.15827&quot;&gt;LinkBERT: Pretraining Language Models with Document Links&lt;/a&gt;. Michihiro Yasunaga, Jure Leskovec and Percy Liang. 2022. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Hyperlinks have been found useful in various NLP research, e.g., &lt;a href=&quot;https://arxiv.org/abs/1911.10470&quot;&gt;Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering&lt;/a&gt;. Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong. 2019. &lt;a href=&quot;https://arxiv.org/abs/2107.06955&quot;&gt;HTLM: Hyper-Text Pre-Training and Prompting of Language Models&lt;/a&gt;. Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, Luke Zettlemoyer. 2022. &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=lnEaqbTJIRz&quot;&gt;The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design&lt;/a&gt;. Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua. 2022. &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.12265&quot;&gt;Strategies for Pre-training Graph Neural Networks&lt;/a&gt;. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec. 2020. &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data&quot;&gt;Translating Embeddings for Modeling Multi-relational Data&lt;/a&gt;. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko. 2013. &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.15779&quot;&gt;Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing&lt;/a&gt;. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon. 2021. &lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.00051&quot;&gt;Reading Wikipedia to Answer Open-Domain Questions&lt;/a&gt;. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes. 2017. &lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:16&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For instance, &lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;BART&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;T5&lt;/a&gt;. &lt;a href=&quot;#fnref:16&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:17&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Training language models for source code data is an active area of research, e.g., &lt;a href=&quot;https://openai.com/blog/openai-codex/&quot;&gt;CodeX&lt;/a&gt;, &lt;a href=&quot;https://www.deepmind.com/blog/competitive-programming-with-alphacode&quot;&gt;AlphaCode&lt;/a&gt;. &lt;a href=&quot;#fnref:17&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Tue, 31 May 2022 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers and Talks at ACL 2022</title>
              <link>/blog/acl-2022/</link>
              <guid isPermaLink="true">/blog/acl-2022/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.2022.aclweb.org/&quot;&gt;60th Annual Meeting of the Association for Computational Linguistics&lt;/a&gt; (ACL) 2022 is taking place May 22nd - May 27th. We’re excited to share all the work from SAIL that’s being presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;h2 id=&quot;list-of-accepted-papers&quot;&gt;List of Accepted Papers&lt;/h2&gt;
&lt;h4 id=&quot;linkbert-pretraining-language-models-with-document-links&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.15827.pdf&quot;&gt;LinkBERT: Pretraining Language Models with Document Links&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img0.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Michihiro Yasunaga, Jure Leskovec*, Percy Liang*
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: myasu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2203.15827.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/michiyasunaga/LinkBERT&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: language model, pretraining, knowledge, hyperlink, bionlp&lt;/p&gt;
&lt;h4 id=&quot;when-classifying-grammatical-role-bert-doesnt-care-about-word-order-except-when-it-matters&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.06204&quot;&gt;When classifying grammatical role, BERT doesn’t care about word order… except when it matters&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img1.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Isabel Papadimitriou, Richard Futrell, Kyle Mahowald
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: isabelvp@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2203.06204&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: large language models, analysis, word order, order invariance, grammatical role, syntax, semantics&lt;/p&gt;
&lt;h4 id=&quot;problems-with-cosine-as-a-measure-of-embedding-similarity-for-high-frequency-words&quot;&gt;Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img2.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kaitlyn Zhou, Kawin Ethayarajh, Dallas Card, Dan Jurafsky
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: katezhou@stanford.edu
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: cosine similarity, training data frequency, model analysis&lt;/p&gt;
&lt;h4 id=&quot;faithful-or-extractive-on-mitigating-the-faithfulness-abstractiveness-trade-off-in-abstractive-summarization&quot;&gt;&lt;a href=&quot;https://aclanthology.org/2022.acl-long.100/&quot;&gt;Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img6.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Faisal Ladhak, Esin Durmus, He He, Claire Cardie, Kathleen McKeown
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: esdurmus@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://aclanthology.org/2022.acl-long.100/&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: text summarization, text generation, evaluation, faithfulness&lt;/p&gt;
&lt;h4 id=&quot;spurious-correlations-in-reference-free-evaluation-of-text-generation&quot;&gt;&lt;a href=&quot;https://aclanthology.org/2022.acl-long.102/&quot;&gt;Spurious Correlations in Reference-Free Evaluation of Text Generation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img7.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Esin Durmus, Faisal Ladhak, Tatsunori Hashimoto
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: esdurmus@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://aclanthology.org/2022.acl-long.102/&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: text summarization, text generation, dialogue generation, evaluation, metrics,&lt;/p&gt;
&lt;h4 id=&quot;tabi-type-aware-bi-encoders-for-open-domain-entity-retrieval&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2204.08173.pdf&quot;&gt;TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img8.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Megan Leszczynski, Daniel Y. Fu, Mayee F. Chen, Christopher Ré
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: mleszczy@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2204.08173.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://hazyresearch.stanford.edu/blog/2022-04-19-contrastive-3&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://github.com/HazyResearch/tabi&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: entity retrieval, contrastive learning, bi-encoders&lt;/p&gt;
&lt;h4 id=&quot;a-few-shot-semantic-parser-for-wizard-of-oz-dialogues-with-the-precise-thingtalk-representation&quot;&gt;&lt;a href=&quot;https://aclanthology.org/2022.findings-acl.317/&quot;&gt;A Few-Shot Semantic Parser for Wizard-of-Oz Dialogues with the Precise ThingTalk Representation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img9.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Giovanni Campagna, Sina J. Semnani, Ryan Kearns, Lucas Jun Koba Sato, Silei Xu, Monica S. Lam
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: gcampagn@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Venue&lt;/strong&gt;: Findings of ACL
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.317/&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://oval.cs.stanford.edu/releases/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: dialogue agents, task-oriented dialogues, data synthesis&lt;/p&gt;
&lt;h4 id=&quot;richer-countries-and-richer-representations&quot;&gt;Richer Countries and Richer Representations&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img3.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kaitlyn Zhou, Kawin Ethayarajh, Dan Jurafsky
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: katezhou@stanford.edu
&lt;br /&gt;&lt;strong&gt;Venue&lt;/strong&gt;: Findings of ACL
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: representational harms, model analysis, geographic entities&lt;/p&gt;
&lt;h4 id=&quot;modular-domain-adaptation&quot;&gt;&lt;a href=&quot;https://aclanthology.org/2022.findings-acl.288/&quot;&gt;Modular Domain Adaptation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img5.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Junshen K. Chen, Dallas Card, Dan Jurafsky
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: dalc@umich.edu
&lt;br /&gt;&lt;strong&gt;Venue&lt;/strong&gt;: Findings of ACL
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://aclanthology.org/2022.findings-acl.288/&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://dallascard.github.io/granular-material/post/modular/&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://github.com/jkvc/modular-domain-adaptation&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: domain adaptation, computational social science, text classification, lexicons, sentiment&lt;/p&gt;
&lt;h4 id=&quot;shared-autonomy-for-robotic-manipulation-with-language-corrections&quot;&gt;&lt;a href=&quot;https://iliad.stanford.edu/pdfs/publications/karamcheti2022lilac.pdf&quot;&gt;Shared Autonomy for Robotic Manipulation with Language Corrections&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-05-25-acl-2022/img4.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Siddharth Karamcheti*, Raj Palleti*, Yuchen Cui, Percy Liang, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: skaramcheti@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Venue&lt;/strong&gt;: ACL LNLS workshop
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://iliad.stanford.edu/pdfs/publications/karamcheti2022lilac.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: human-robot interaction, online language corrections, language supervision&lt;/p&gt;

&lt;hr /&gt;

</description>
              <pubDate>Wed, 25 May 2022 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers and Talks at ICLR 2022</title>
              <link>/blog/iclr-2022/</link>
              <guid isPermaLink="true">/blog/iclr-2022/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://iclr.cc&quot;&gt;International Conference on Learning Representations&lt;/a&gt; (ICLR) 2022 is being hosted virtually from April 25th - April 29th. We’re excited to share all the work from SAIL that’s being presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;h2 id=&quot;list-of-accepted-papers&quot;&gt;List of Accepted Papers&lt;/h2&gt;
&lt;h4 id=&quot;autonomous-reinforcement-learning-formalism-and-benchmarking&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.09605&quot;&gt;Autonomous Reinforcement Learning: Formalism and Benchmarking&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img0.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Archit Sharma*, Kelvin Xu*, Nikhil Sardana, Abhishek Gupta, Karol Hausman, Sergey Levine, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: architsh@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2112.09605&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://architsharma97.github.io/earl_benchmark/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, continual learning, reset-free reinforcement learning&lt;/p&gt;
&lt;h4 id=&quot;metashift-a-dataset-of-datasets-for-evaluating-contextual-distribution-shifts-and-training-conflicts-&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.06523&quot;&gt;MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution Shifts and Training Conflicts &lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img1.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Weixin Liang, James Zou
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: wxliang@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2202.06523&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://recorder-v3.slideslive.com/?share=64243&amp;amp;s=4b8a00e2-83f3-4775-879f-70de42374ec6&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://metashift.readthedocs.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: benchmark dataset, distribution shift, out-of-domain generalization&lt;/p&gt;
&lt;h4 id=&quot;an-explanation-of-in-context-learning-as-implicit-bayesian-inference&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.02080&quot;&gt;An Explanation of In-context Learning as Implicit Bayesian Inference&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img2.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: xie@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2111.02080&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=O4WMiIJwgd4&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: gpt-3, in-context learning, pretraining, few-shot learning&lt;/p&gt;
&lt;h4 id=&quot;greaselm-graph-reasoning-enhanced-language-models-for-question-answering&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.08860&quot;&gt;GreaseLM: Graph REASoning Enhanced Language Models for Question Answering&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img3.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, Jure Leskovec
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: xikunz2@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Spotlight
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2201.08860&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/snap-stanford/GreaseLM&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: knowledge graph, question answering, language model, commonsense reasoning, graph neural networks, biomedical qa&lt;/p&gt;
&lt;h4 id=&quot;fast-model-editing-at-scale&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2110.11309.pdf&quot;&gt;Fast Model Editing at Scale&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img4.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: eric.mitchell@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2110.11309.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/mend-editing&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: model editing; meta-learning; language models; continual learning; temporal generalization&lt;/p&gt;
&lt;h4 id=&quot;vision-based-manipulators-need-to-also-see-from-their-hands&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.12677&quot;&gt;Vision-Based Manipulators Need to Also See from Their Hands&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img5.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kyle Hsu, Moo Jin Kim, Rafael Rafailov, Jiajun Wu, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kylehsu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral Presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2203.12677&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/seeing-from-hands&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, observation space, out-of-distribution generalization, visuomotor control, robotics, manipulation&lt;/p&gt;
&lt;h4 id=&quot;ifr-explore-learning-inter-object-functional-relationships-in-3d-indoor-scenes&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.05298&quot;&gt;IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img6.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Qi Li*, Kaichun Mo*, Yanchao Yang, Hang Zhao, Leonidas J. Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kaichun@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2112.05298&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: embodied ai, 3d scene graph, interactive perception&lt;/p&gt;
&lt;h4 id=&quot;vat-mart-learning-visual-action-trajectory-proposals-for-manipulating-3d-articulated-objects&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.14440&quot;&gt;VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img7.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ruihai Wu*, Yan Zhao*, Kaichun Mo*, Zizheng Guo, Yian Wang, Tianhao Wu, Qingnan Fan, Xuelin Chen, Leonidas J. Guibas, Hao Dong
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kaichun@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.14440&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=HjhsLKf1eQY&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://hyperplane-lab.github.io/vat-mart/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: visual affordance learning, robotic manipulation, 3d perception, interactive perception&lt;/p&gt;
&lt;h4 id=&quot;language-modeling-via-stochastic-processes&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.11370.pdf&quot;&gt;Language modeling via stochastic processes&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img8.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Rose E Wang, Esin Durmus, Noah Goodman, Tatsunori Hashimoto
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: rewang@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral Presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2203.11370.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=AwnoASlxeIs&amp;amp;ab_channel=RoseWang&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://github.com/rosewang2008/language_modeling_via_stochastic_processes&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: contrastive learning, language modeling, stochastic processes&lt;/p&gt;
&lt;h4 id=&quot;metamorph-learning-universal-controllers-with-transformers&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.11931&quot;&gt;MetaMorph: Learning Universal Controllers with Transformers&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img9.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Agrim Gupta, Linxi Fan, Surya Ganguli, Li Fei-Fei
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: agrim@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2203.11931&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=mGXtjLxyAkQ&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://metamorph-iclr.github.io/site/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: rl, modular robots, transformers&lt;/p&gt;
&lt;h4 id=&quot;fine-tuning-can-distort-pretrained-features-and-underperform-out-of-distribution&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.10054&quot;&gt;Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img10.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ananya Kumar
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ananya@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral Presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2202.10054&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: fine-tuning theory, transfer learning theory, fine-tuning, distribution shift, implicit regularization&lt;/p&gt;
&lt;h4 id=&quot;an-experimental-design-perspective-on-model-based-reinforcement-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.05244&quot;&gt;An Experimental Design Perspective on Model-Based Reinforcement Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img11.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Viraj Mehta, Biswajit Paria, Jeff Schneider, Stefano Ermon, Willie Neiswanger
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: virajm@cs.cmu.edu, neiswanger@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2112.05244&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, model-based reinforcement learning, mbrl, bayesian optimal experimental design, boed, bax&lt;/p&gt;
&lt;h4 id=&quot;domino-discovering-systematic-errors-with-cross-modal-embeddings&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.14960&quot;&gt;Domino: Discovering Systematic Errors with Cross-Modal Embeddings&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img12.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Sabri Eyuboglu*, Maya Varma*, Khaled Saab*, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, Christopher Ré
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: {eyuboglu,mvarma2,ksaab}@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral Presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2203.14960&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://ai.stanford.edu/blog/domino/&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://hazyresearch.stanford.edu/blog/2022-04-02-domino&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: robustness, subgroup analysis, error analysis, multimodal, slice discovery&lt;/p&gt;
&lt;h4 id=&quot;pixelated-butterfly-simple-and-efficient-sparse-training-for-neural-network-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.00029&quot;&gt;Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img13.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher Ré
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: trid@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Spotlight
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2112.00029&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://hazyresearch.stanford.edu/blog/2022-01-17-Sparsity-3-Pixelated-Butterfly&quot;&gt;Blog Post&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: sparse training, butterfly matrices&lt;/p&gt;
&lt;h4 id=&quot;hindsight-posterior-guided-training-of-retrievers-for-improved-open-ended-generation&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=Vr_BTpw3wz&quot;&gt;Hindsight: Posterior-guided training of retrievers for improved open-ended generation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img14.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D Manning
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ashwinp@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/pdf?id=Vr_BTpw3wz&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: retrieval, generation, retrieval-augmented generation, open-ended generation, informative conversations, free-form qa, posterior distribution, elbo&lt;/p&gt;
&lt;h4 id=&quot;unsupervised-discovery-of-object-radiance-fields&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=rwE8SshAlxw&quot;&gt;Unsupervised Discovery of Object Radiance Fields&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img15.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Hong-Xing Yu, Leonidas J. Guibas, Jiajun Wu
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: koven@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/pdf?id=rwE8SshAlxw&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=6J9OpvT4dCA&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://kovenyu.com/uORF/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: object-centric representation, unsupervised, 3d object discovery&lt;/p&gt;
&lt;h4 id=&quot;efficiently-modeling-long-sequences-with-structured-state-spaces&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.00396&quot;&gt;Efficiently Modeling Long Sequences with Structured State Spaces&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img16.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Albert Gu, Karan Goel, Christopher Ré
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: albertgu@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Outstanding Paper Honorable Mention
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2111.00396&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://hazyresearch.stanford.edu/blog/2022-01-14-s4-1&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://recorder-v3.slideslive.com/#/share?share=64409&amp;amp;s=f3dc299a-8857-4218-870e-b7bf20b6d29c&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: hippo&lt;/p&gt;
&lt;h4 id=&quot;how-many-degrees-of-freedom-do-we-need-to-train-deep-networks-a-loss-landscape-perspective&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.05802&quot;&gt;How many degrees of freedom do we need to train deep networks: a loss landscape perspective&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img17.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Brett W. Larsen, Stanislav Fort, Nic Becker, Surya Ganguli
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: bwlarsen@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.05802&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: loss landscape, high-dimensional geometry, random hyperplanes, optimization&lt;/p&gt;
&lt;h4 id=&quot;how-did-the-model-change-efficiently-assessing-machine-learning-api-shifts&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=gFDFKC4gHL4&quot;&gt;How did the Model Change? Efficiently Assessing Machine Learning API Shifts&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-04-25-iclr-2022/img18.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Lingjiao Chen, Matei Zaharia, James Zou
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: lingjiao@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/pdf?id=gFDFKC4gHL4&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/lchen001/MASA&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: mlaas, performance shifts, ml systems&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at ICLR 2022!&lt;/p&gt;
</description>
              <pubDate>Mon, 25 Apr 2022 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Discovering the systematic errors made by machine learning models</title>
              <link>/blog/domino/</link>
              <guid isPermaLink="true">/blog/domino/</guid>
              <description>&lt;h1 id=&quot;discovering-systematic-errors-with-cross-modal-embeddings&quot;&gt;Discovering systematic errors with cross-modal embeddings&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;In this blog post, we introduce Domino, a new approach for discovering systematic errors made by machine learning models. We also discuss a framework for quantitatively evaluating methods like Domino.&lt;/p&gt;

  &lt;p&gt;Links: 
📄 &lt;a href=&quot;https://arxiv.org/abs/2203.14960&quot;&gt;Paper (ICLR 2022)&lt;/a&gt;
🌍 &lt;a href=&quot;https://hazyresearch.stanford.edu/blog/2022-04-02-domino&quot;&gt;Longer Walkthrough&lt;/a&gt;
💻 &lt;a href=&quot;https://github.com/HazyResearch/domino&quot;&gt;GitHub&lt;/a&gt;
📘 &lt;a href=&quot;https://domino-slice.readthedocs.io/en/latest/&quot;&gt;Docs&lt;/a&gt;
📒 &lt;a href=&quot;https://colab.research.google.com/github/HazyResearch/domino/blob/main/examples/01_intro.ipynb&quot;&gt;Google Colab&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Machine learning models that achieve high overall accuracy often make systematic errors on coherent slices of validation data.&lt;/p&gt;

&lt;p&gt;What is a slice? A slice is a set of data samples that share a common characteristic. As an example, in large image datasets, photos of vintage cars comprise a slice (i.e. all images in the slice share a common subject). The term slice has a number of synonyms  that you might be more familiar with (e.g. subgroup, subpopulation, stratum). These terms are largely interchangeable, but we’ll stick with “slice” throughout this post. We say that a model underperforms on a slice if performance on the data samples in the slice is significantly worse than its overall performance.&lt;/p&gt;

&lt;p&gt;The search for underperforming slices is a critical, but often overlooked, part of model evaluation. When practitioners are aware of the slices on which their models underperform, they can make more informed decisions around model deployment. This is particularly important in safety-critical settings like medicine: a diagnostic model that underperforms on younger patients should likely not be deployed at a pediatric hospital. Slice awareness can also help practitioners debug and improve models: after an underperforming slice is identified, we can improve model robustness by either updating the training dataset or using robust optimization techniques (&lt;em&gt;e.g.&lt;/em&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.12945.pdf&quot;&gt; Sohoni et al., 2020&lt;/a&gt;;&lt;a href=&quot;https://arxiv.org/pdf/1911.08731.pdf&quot;&gt; Sagawa et al., 2020&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Deploying models that underperform on critical data slices may have significant safety or fairness consequences. For example, models trained to detect collapsed lungs in chest X-rays have been shown to make predictions based on the presence of chest drains, a device typically used during treatment &lt;em&gt;(&lt;/em&gt;&lt;a href=&quot;https://arxiv.org/pdf/1909.12475.pdf&quot;&gt;Oakden-Rayner, 2019&lt;/a&gt;&lt;em&gt;)&lt;/em&gt;. As a result, these models often fail to detect collapsed lung in images without chest drains, a critical data slice where false negative predictions could be life-threatening.&lt;/p&gt;

&lt;p&gt;However, in practice, some underperforming slices are hard to find. The examples in these “hidden” data slices are tied together by a concept not annotated in metadata or easily extracted from unstructured inputs (e.g. images, video, time-series data). Returning to our example from earlier, many chest X-ray datasets do not provide metadata indicating which patients’ images show chest tubes, making it difficult to evaluate performance on the slice. This raises the following question: &lt;em&gt;How can we automatically identify data slices on which our model underperforms?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this blog post, we discuss our recent exploration of this question. We introduce Domino, a novel method for identifying and describing underperforming slices. We also discuss an evaluation framework for rigorously evaluating our method across diverse slice types, tasks, and datasets.&lt;/p&gt;

&lt;h2 id=&quot;what-is-slice-discovery&quot;&gt;What is slice discovery?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Slice discovery&lt;/strong&gt; is the task of mining unstructured input data (&lt;em&gt;e.g.&lt;/em&gt; images, videos, audio) for semantically meaningful subgroups on which a model performs poorly. We refer to automated techniques that mine input data for semantically meaningful slices as &lt;strong&gt;slice discovery methods (SDM).&lt;/strong&gt; Given a labeled validation dataset and a trained classifier, an SDM computes a set of slicing functions that partition the dataset into slices. This process is illustrated below.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_100&quot; src=&quot;/blog/assets/img/posts/2022-04-07-domino/image1.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In order to be broadly useful across diverse settings, an ideal SDM should satisfy the following desiderata:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Identified slices should contain examples on which the model &lt;strong&gt;underperforms&lt;/strong&gt;, or has a high error rate.&lt;/li&gt;
  &lt;li&gt;Identified slices should contain examples that are &lt;strong&gt;coherent&lt;/strong&gt;, or align closely with a human-understandable concept.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This second desideratum is particularly hard to achieve: existing evaluations have shown that discovered slices often do not align with concepts understandable to a domain expert. Further, even if slices do align well with concepts, it may be difficult for humans to identify the commonality.&lt;/p&gt;

&lt;h2 id=&quot;domino-slice-discovery-with-cross-modal-embeddings&quot;&gt;Domino: Slice discovery with cross-modal embeddings&lt;/h2&gt;

&lt;p&gt;In our work, we introduce Domino, a slice discovery method designed to identify coherent, underperforming data slices (&lt;em&gt;i.e.&lt;/em&gt; groups of similar validation data points on which the model makes errors). It leverages a powerful class of recently-developed cross-modal representation learning approaches, which yield semantically-meaningful representations by embedding images and text in the same latent space. We demonstrate that using cross-modal representations both improves slice coherence and enables Domino to generate natural language descriptions for identified slices!&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_100&quot; src=&quot;/blog/assets/img/posts/2022-04-07-domino/image3.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Domino follows a three-step procedure illustrated in the figure above:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Embed:&lt;/strong&gt; Domino encodes the validation images alongside text in a shared embedding space using a cross-modal encoder. In many domains, such encoders are publicly available (&lt;em&gt;e.g&lt;/em&gt;.&lt;a href=&quot;https://openai.com/blog/clip/&quot;&gt; CLIP&lt;/a&gt; for natural images, &lt;a href=&quot;https://github.com/pytorch/fairseq/tree/main/examples/MMPT&quot;&gt;VideoCLIP&lt;/a&gt; for natural videos, &lt;a href=&quot;https://github.com/edreisMD/ConVIRT-pytorch&quot;&gt;ConVIRT&lt;/a&gt; for medical images, and &lt;a href=&quot;https://github.com/MicPie/clasp&quot;&gt;CLASP&lt;/a&gt; for amino acid sequences).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Slice:&lt;/strong&gt; Using an error-aware mixture model, Domino identifies regions in the embedding space with a high concentration of errors.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Describe:&lt;/strong&gt; Finally, to help practitioners understand the commonalities among the examples in each slice, Domino generates natural language descriptions of the slices. To do so, it leverages the cross-modal embeddings computed in Step 1, surfacing the text nearest to the slice in embedding space.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We now use Domino to audit a popular off-the-shelf classifier: a&lt;a href=&quot;https://pytorch.org/vision/stable/models.html&quot;&gt; ResNet18 pretrained on ImageNet&lt;/a&gt;. Specifically, we interrogate the model’s ability to detect cars, exploring whether there are any interesting slices on which the model underperforms. In the figure below we show a couple of the slices that Domino discovered. The gray boxes show the natural language descriptions of the two slices produced by Domino, the $X$ row shows the top six images predicted by domino to be in the slice, the $Y$ row shows the ground truth label assigned to the image, and the $\hat{Y}$ row shows the ResNet18’s predicted probability for the “car” class. Note that although we only include six images here, the complete slice includes dozens of images.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_100&quot; src=&quot;/blog/assets/img/posts/2022-04-07-domino/image2.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;From these slices, we might hypothesize that the model struggles to recognize photos of cars &lt;em&gt;taken from the inside&lt;/em&gt; and photos of &lt;em&gt;racecars.&lt;/em&gt; Both of these slices describe rare subclasses of the target class. Depending on the intended use case for the model, we may want to add more training examples to boost performance in these slices. For example, Waymo (an autonomous vehicle company) may not care much whether the model misses photos of car interiors, but ESPN (a broadcaster with the television rights for Formula 1) would care a lot if the model couldn’t recognize race cars! Clearly, it’s important to practitioners that discovered slices map onto coherent concepts.&lt;/p&gt;

&lt;h2 id=&quot;evaluating-slice-discovery-methods&quot;&gt;Evaluating slice discovery methods&lt;/h2&gt;

&lt;p&gt;In designing Domino, we were inspired by a number of really exciting slice discovery methods that were recently proposed. These include &lt;em&gt;The Spotlight&lt;/em&gt; (&lt;a href=&quot;https://arxiv.org/pdf/2107.00758.pdf&quot;&gt;D’Eon et al. 2022&lt;/a&gt;), &lt;em&gt;GEORGE&lt;/em&gt; (&lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/hash/e0688d13958a19e087e123148555e4b4-Abstract.html&quot;&gt;Sohoni et al. 2020&lt;/a&gt;), and &lt;em&gt;MultiAccuracy Boost&lt;/em&gt; (&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3306618.3314287&quot;&gt;Kim et al. 2018&lt;/a&gt;). These methods all have (1) an &lt;strong&gt;embed&lt;/strong&gt; step and (2) a &lt;strong&gt;slice&lt;/strong&gt; step, like Domino, but use different embeddings and slicing algorithms. In our experiments, we evaluate SDMs along these two axes, ablating both the choice of embedding and the slicing algorithm. Notably, these methods do not include a (3) &lt;strong&gt;describe&lt;/strong&gt; step, and generally require users to manually inspect examples and identify common attributes.&lt;/p&gt;

&lt;p&gt;SDMs like Domino have traditionally been evaluated qualitatively, due to a lack of a simple quantitative approach. Typically, in these evaluations, the SDM is applied to a few different models and identified slices are visualized. Practitioners can then inspect the slices and judge whether the slices “make sense.” However, these qualitative evaluations are subjective and do not scale beyond more than a few settings. Moreover, they cannot tell us if the SDM has missed an important, coherent slice.&lt;/p&gt;

&lt;p&gt;Ideally, we’d like to estimate the failure rate of an SDM: how often it fails to identify a coherent slice on which the model underperforms. Estimating this failure rate is very challenging because we don’t typically know the full set of slices on which a model underperforms. How could we possibly know if the SDM is missing any?&lt;/p&gt;

&lt;p&gt;To solve this problem, we trained 1,235 deep classifiers that were specifically constrained to underperform on pre-defined slices. We did this across three domains: natural images, medical images and medical time-series. Our approach involved (1) obtaining a dataset with some annotated slices (&lt;em&gt;e.g.&lt;/em&gt; a dataset with interesting annotated attributes, like &lt;a href=&quot;https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&quot;&gt;CelebA&lt;/a&gt; or &lt;a href=&quot;https://physionet.org/content/mimic-cxr/2.0.0/&quot;&gt;MIMIC-CXR&lt;/a&gt;), and (2) manipulating the dataset such that, with high probability, a model trained on it would exhibit poor performance on one or more of the annotated slices (&lt;em&gt;e.g.&lt;/em&gt; by subsampling the dataset to induce a spurious correlation between the label and a metadata field).&lt;/p&gt;

&lt;p&gt;Using this evaluation framework, we were able to evaluate Domino quantitatively. We find that Domino accurately identifies 36% of the 1,235 slices in our framework. Further, the natural language description of the generated slice exactly matches the name of the slice in 35% of settings.&lt;/p&gt;

&lt;p&gt;We were also able to compare SDMs and run ablation studies evaluating specific SDM design choices. Two key findings emerged from these experiments:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Cross-modal embeddings improve SDM performance.&lt;/strong&gt; We found that the choice of representation matters – a lot! Slice discovery methods based on cross-modal embeddings outperform those based on a single modality by at least 9 percentage-points in precision-at-10. When compared with using the activations of the trained model, the gap grows to 15 percentage points. This finding is of particular interest given that classifier activations are a popular embedding choice in existing SDMs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Modeling both the prediction and class label enables accurate slice discovery.&lt;/strong&gt; Good embeddings alone do not suffice – the choice of algorithm for actually extracting the underperforming slices from the embedding space, is significant as well. We find that a simple mixture model that jointly models the embeddings, labels and predictions enables a 105% improvement over the next best slicing algorithm. We hypothesize that this is because this algorithm is unique in modeling the class labels and the model’s predictions as separate variables, which leads to slices which are “pure” in their error type (false positive vs. false negative).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;However, there’s still a long way to go: slice discovery is a challenging task, and &lt;em&gt;Domino,&lt;/em&gt; the best performing method in our experiments, still fails to recover over 60% of coherent slices. We see a number of exciting avenues for future work that could begin to close this gap.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We suspect that improvements in the embeddings that power slice discovery will be driven by large &lt;strong&gt;cross modal datasets&lt;/strong&gt;, so work in dataset curation and management could help push the needle.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In this blog post, we described slice discovery as a fully automated process, while, in the future, we expect that effective slice discovery systems will be &lt;strong&gt;highly interactive&lt;/strong&gt;: practitioners will be able to quickly explore slices and provide feedback.&lt;a href=&quot;https://www.youtube.com/watch?v=qFzIgc5tc9s&quot;&gt; Forager&lt;/a&gt;, a system for rapid data exploration, is an exciting step in this direction.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We are really excited to continue working on this important problem and to collaborate with others as we seek to develop more reliable slice discovery methods. To facilitate this process, we are releasing 984 models and their associated slices as part of &lt;a href=&quot;https://github.com/data-centric-ai/dcbench&quot;&gt;dcbench&lt;/a&gt;, a suite of data centric benchmarks. This will allow others to reproduce our results and also develop new slice discovery methods. Additionally, we are also releasing &lt;a href=&quot;https://github.com/HazyResearch/domino&quot;&gt;domino&lt;/a&gt;, a Python package containing implementations of popular slice discovery methods. If you’ve developed a new slice discovery method and would like us to add it to the library please reach out!&lt;/p&gt;
</description>
              <pubDate>Thu, 07 Apr 2022 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Grading Complex Interactive Coding Programs with Reinforcement Learning</title>
              <link>/blog/play-to-grade/</link>
              <guid isPermaLink="true">/blog/play-to-grade/</guid>
              <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;[Summary] tl;dr:&lt;/strong&gt;
A tremendous amount of effort has been poured into training AI algorithms to competitively play games that computers have traditionally had trouble with, such as the retro games published by Atari, Go, DotA, and StarCraft II. The practical machine learning knowledge accumulated in developing these algorithms  has paved the way for people to now routinely train game-playing AI agents for many games. Following this line of work, we focus on a specific category of games – those developed by students as part of a programming assignment.  Can the same algorithms that master Atari games help us grade these game assignments? In our recent NeurIPS 2021 &lt;a href=&quot;https://arxiv.org/abs/2110.14615&quot;&gt;paper&lt;/a&gt;, we illustrate the challenges in treating interactive coding assignment grading as game playing and introduce the Play to Grade Challenge.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Massive Online Coding Education has reached striking success over the past decade. Fast internet speed, improved UI design, code editors that are embedded in a browser window allow educational platforms such as &lt;a href=&quot;https://code.org&quot;&gt;Code.org&lt;/a&gt; to build a diverse set of courses tailored towards students of different coding experiences and interest levels (for example, Code.org offers “Star War-themed coding challenge,” and “Elsa/Frozen themed for-loop writing”). As a non-profit organization, Code.org claims to have reached over 60 million learners across the world &lt;sup id=&quot;fnref:codeorgstats&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:codeorgstats&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Such organizations typically provide a variety of carefully constructed teaching materials such as videos and programming challenges.&lt;/p&gt;

&lt;p&gt;A challenge faced by these platforms is that of grading assignments. It is well known that grading is critical to student learning &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, in part because it motivates students to complete their assignments. Sometimes manual grading can be feasible in small settings, or automated grading used in simple settings such as when assignments are multiple choice or adopt a fill-in-the-blink modular coding structure. Unfortunately, many of the most exciting assignments, such as developing games or interactive apps, are also much more difficult to automatically evaluate. For such assignments, human teachers are currently needed to provide feedback and grading. This requirement, and the corresponding difficulty with scaling up manual grading, is one of the biggest obstacles of online coding education. Without automated grading systems,  students who lack additional teacher resources cannot get useful  feedback to help them learn and advance through the materials provided.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_50&quot; src=&quot;https://media.giphy.com/media/LhC4oRHFahkxvryfQA/giphy.gif&quot; style=&quot;width:40%&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 1&lt;/b&gt;: This is a popular coding game offered by Code.org. A student would write a program to create this game.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Programming a game that is playable is exciting for students who are learning to code. &lt;a href=&quot;https://code.org&quot;&gt;Code.org&lt;/a&gt; provides many game development assignments in their curriculum. In these assignments, students write JavaScript programs in a code editor embedded in the web browser. Game assignments are great for teachers to examine student’s progress as well: students not only need to grasp basic concepts like if-conditionals and for-loops but use these concepts to write the physical rules of the game world — calculate the trajectories of objects, resolve inelastic collision of two objects, and keep track of game states. To deal with all of these complexities, students need to use abstraction (functions/class) to encapsulate each functionality in order to manage this complex set of logic.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure2.png&quot; style=&quot;padding:0;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 2&lt;/b&gt;: In Code.org, students program in an interactive code interface, where they can write the program in the coding area, hit run and play their game.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Automated grading on the code text alone can be an incredibly hard challenge, even for introductory level computer science assignments. As examples, two solutions which are only slightly different in text can have very different behaviors, and two solutions that are written in very different ways can have the same behaviors. As such, some models that people develop for grading code can be as complex as those used to understand paragraphs of natural language. But, sometimes, grading code can be even more difficult than grading an essay because coding submissions can be in different programming languages. In this situation, one must not only develop a program that can understand many programming languages, but guard against the potential that the grader is more accurate for some languages than others. A Finally, these programs must be able to generalize to new assignments because correct grading is just as necessary for the first student working on an assignment as the millionth – the collect-data, train, deploy cycle is not quite suitable in this context. We don’t have the luxury of collecting a massive amount of labeled dataset to train a fully supervised learning algorithm for each and every assignment.&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&quot;https://arxiv.org/abs/2110.14615&quot;&gt;recent paper&lt;/a&gt;, we circumvent these challenges by developing a method that grades assignments by playing them, without needing to look at the source code at all. Despite this different approach, our method still manages to provide scalable feedback that potentially can be deployed in a massively-online setting.&lt;/p&gt;

&lt;h2 id=&quot;the-play-to-grade-challenge&quot;&gt;The Play to Grade Challenge&lt;/h2&gt;

&lt;p&gt;Our solution to these problems is to ignore the code text entirely and to grade an assignment by having a grading algorithm play it. We represent the underlying game of each program submission as a Markov Decision Process (MDP), which defines a state space, action space, reward function, and transition dynamics. By running each student’s program, we can build the MDP directly without needing to read or understand the underlying code. You can read more about the MDP framework here: &lt;sup id=&quot;fnref:lilianweng&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:lilianweng&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Since all student programs are written for the same assignment, these programs should generate MDP with a lot of commonalities, such as shared state and action space. After playing the game and fully constructing the MDP for an assignment, all we need is to compare the MDP specified by the student’s program (student MDP) to the teacher’s solution (reference MDP) and determine &lt;strong&gt;if these two MDPs are the same&lt;/strong&gt;. What sets this challenge apart from any other reinforcement learning problems is the fact that a &lt;strong&gt;classification&lt;/strong&gt; needs to be made at the end of this agent’s interaction with this MDP — the decision of whether the MDP is the same as the reference MDP or not.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postfigurethird&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/distance.png&quot; style=&quot;padding:0;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 3&lt;/b&gt;: We need to build an approximate distance function D that determines the distance between the student program's underlying MDP (black dot) and correct MDPs (blue dots) and incorrect MDPs (red dots). Read more about how we build this distance function in our &lt;a href=&quot;https://arxiv.org/abs/2110.14615&quot;&gt;paper&lt;/a&gt;.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In order to solve this challenge, we present an algorithm with two components: an agent that plays the game and can reliably reach bug states, and a classifier that can recognize bug states (i.e., provide the probability of the observed state being a bug). Both components are necessary for accurate grading: an agent that reaches all states but cannot determine if any represents bugs is just as bad as a perfect classifier paired with an agent that is bad at taking actions which might cause bugs. Imagine a non-optimal agent that never catches the ball (in the example above) – this agent will never be able to test if the wall, or paddle, or goal does not behave correctly.&lt;/p&gt;

&lt;p&gt;An ideal agent needs to produce &lt;strong&gt;differential trajectories&lt;/strong&gt;, i.e., sequences of actions that can be used to differentiate two MDPs, and must contain at least one bug-triggering state if the trajectory is produced from the incorrect MDP. Therefore, we need both a correct MDP and a few incorrect MDPs to teach the agent and the classifier. These incorrect MDPs are incorrect solutions that can either be provided by the teacher, or come from manually grading a few student programs to find common issues. Although having to manually label incorrect MDPs is an annoyance, we show that the total amount of effort is generally significantly lower than grading each assignment: in fact, we show that for the task we solve in the paper, you only need 5 incorrect MDPs to reach a decent performance (see the appendix section of our paper).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure3_new.png&quot; style=&quot;padding:0;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 4&lt;/b&gt;: We build an MDP wrapper around the student program that allows the agent to interact with the program (while the original program might only allow human control, i.e., we override mouse / keyboard events.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&quot;recognizing-bugs-from-scratch&quot;&gt;Recognizing Bugs from Scratch&lt;/h2&gt;

&lt;p&gt;Here are three incorrect programs and what they look like when played. Each incorrect program behaves differently from the correct program:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;One program’s wall does not allow the ball to bounce on it.&lt;/li&gt;
  &lt;li&gt;Another program’s goal post does not let the ball go through.&lt;/li&gt;
  &lt;li&gt;The last program spawns 2 new balls whenever the ball bounces on the wall.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postfigurethird&quot; src=&quot;https://media.giphy.com/media/i8ITbB6QtNS67t9dk6/giphy.gif&quot; /&gt;
&lt;img class=&quot;postfigurethird&quot; src=&quot;https://media.giphy.com/media/JuQn32VatSaW1vFCgi/giphy.gif&quot; /&gt;
&lt;img class=&quot;postfigurethird&quot; src=&quot;https://media.giphy.com/media/gQKLJuq49sjjkEube9/giphy.gif&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 5&lt;/b&gt;: Different types of incorrect student programs.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;A challenge with building differential trajectories is that one must know which state is a bug triggering state. Previous works &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; have made the strong assumption that one would automatically know when they encountered a bug, potentially because they expect the game program to crash after encountering a bug. Because of this assumption, they focus their efforts on building pure-exploration agents that try to visit as many states as possible. However, in reality, bugs can be difficult to identify and do not all cause the game to crash. For example, a ball that is supposed to bounce off of a wall is now piercing through it and flying off into oblivion. These types of behavioral anomalies motivate the use of a predictive model that can take in the current game state and determine whether it is anomalous or not.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure4.png&quot; style=&quot;padding:0;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 6&lt;/b&gt;: The chicken-and-egg cold-start problem. The agent doesn't know how to reach bug state, and the classifier does not know what is a bug.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Unfortunately, training a model to predict if a state is a bug state is non-trivial. This is because, although we have labels for some MDPs, these labels are not on the state-level (i.e., not all states in an incorrect MDP are bug states). Put another way, our labels can tell us when a bug has been encountered but cannot tell us what specific action caused the bug. The determination of whether bugs exist in a program can be framed as a chicken-and-egg problem where, if bug states could be unambiguously determined one would only need to explore the state space, and if the exploration was optimal one would only need to figure out how to determine if the current state exhibited a bug.&lt;/p&gt;

&lt;h2 id=&quot;collaborative-reinforcement-learning&quot;&gt;Collaborative Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Fortunately, these types of problems can generally be solved through the expectation-maximization framework, which involves an intimate collaboration between the neural network classifier and the reinforcement learning agent. We propose &lt;strong&gt;collaborative reinforcement learning&lt;/strong&gt;, an expectation-maximization approach, where we use a random agent to produce a dataset of trajectories from the correct and incorrect MDP to teach the classifier. Then the classifier would assign a score to each state indicating how much the classifier believes the state is a bug-triggering state. We use this score as reward and train the agent to reach these states as often as possible for a new dataset of trajectories to train the classifier.&lt;/p&gt;

&lt;p&gt;After using the RL agent to interact with the MDP to produce trajectories, we can try out  different ways to learn a classifier that can classify a state as a bug or not (a binary label). Choosing the right label  is important because this label will become the reward function for the agent, so it can learn to reach bug states more efficiently. However, we only know if the MDP (the submitted code) is correct or broken, but we don’t have labels for the underlying states. Learning state-level labels becomes a challenge!&lt;/p&gt;

&lt;p&gt;We tested out several types of classifiers: (1) a noisy classifier that classifies all states in a broken MDP as broken, (2) a Gaussian Mixture Model that treats all states independently, (3) a Variational Autoencoder that also treats all states independently but directly models non-linear interactions among the features, or (4) an LSTM that jointly models the teacher program as an MDP (HoareLSTM) and an LSTM that models the student program as an MDP (Contrastive HoareLSTM) – with a distance function that compares the two MDPs, borrowing distance notions from literature in MDP homomorphism&lt;sup id=&quot;fnref:homomorph&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:homomorph&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:lihong&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:lihong&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:kipf&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:kipf&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:givan&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:givan&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;In this toy environment, the agent drives a car on a 2D plane. Whenever the agent drives the car into the outer rim of this area (space between the boundary and red dotted line), a bug will cause the car to get stuck (Leftmost panel in &lt;a href=&quot;#figure5&quot;&gt;Figure 5&lt;/a&gt;). Being stuck means the car’s physical movement is altered, resulting in back-and-forth movement around the same location. The bug classifier needs to recognize the resulting states (position and velocity) of the car being “stuck”, by correctly outputting a binary label (bug) for these states.&lt;/p&gt;

&lt;p&gt;In this setting, there is only one type of bug. Most classifiers do well when the agent only drives a straight line (single-direction agent). However, when the agent randomly samples actions at each state, simpler classifiers can no longer differentiate between bug and non-bug states with high accuracy.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a id=&quot;figure5&quot;&gt;&lt;/a&gt;
&lt;img class=&quot;postfigurethird&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure5_env.png&quot; style=&quot;padding:0; width:25%&quot; /&gt;
&lt;img class=&quot;postfigurethird&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure5_single_direction_agent.png&quot; style=&quot;padding:0; width:35%&quot; /&gt;
&lt;img class=&quot;postfigurethird&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure5_random_agent.png&quot; style=&quot;padding:0;width:35%&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 7&lt;/b&gt;: Performance of different bug classification models with different RL agents.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We can increase the difficulty of this setting to see if collaborative training can make the agent operate in the environment with an intention to trigger bugs. In this toy environment, now the bugs will only be triggered in red boxes (Leftmost panel in &lt;a href=&quot;#figure6&quot;&gt;Figure 6&lt;/a&gt; below). We can see that with only one round of collaborative training (“CT Round 1”), the performances of ALL classifiers are improved, including weaker classifiers. This is understandable, as the agent learns to gradually collect better datasets to train classifiers – and higher quality datasets lead to stronger classifiers. For example, variational auto-encoder started only with 32% precision, but it increased to 74.8% precision after 2 rounds of collaborative training.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a id=&quot;figure6&quot;&gt;&lt;/a&gt;
&lt;img class=&quot;postfigurethird&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure6_env.png&quot; style=&quot;padding:0; width:25%&quot; /&gt;
&lt;img class=&quot;postfigurethird&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure6_training_improvement_with_CI.png&quot; style=&quot;padding:0; width:70%&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 8&lt;/b&gt;: Collaborative training improves bug classifier performance across different models. This shows how important it is for the RL agent to produce &lt;b&gt;differential trajectories&lt;/b&gt;, which will allow classifiers to obtain higher performance.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We can also visualize how the collaborative training quickly allows the agent to learn to explore states that most-likely contain bugs by visualizing the trajectories (see figure below). Initially the agent just explores the space uniformly (blue curves), but after one round of collaborative training (CT), it learns to focus on visiting the potential bug areas (regions marked by red boxes) (red curves).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a id=&quot;figure7&quot;&gt;&lt;/a&gt;
&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure7.png&quot; style=&quot;padding:0;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 9&lt;/b&gt;: Visualization of the paths taken by the RL agent (each line represents one trajectory). After collaborative training (CT), the agent quickly focuses only on visiting potentially bug states (relying on the signal provided by the bug classifiers).
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&quot;grading-bounce&quot;&gt;Grading Bounce&lt;/h2&gt;

&lt;p&gt;Next, we returned to the motivating example for this type of approach: grading real student submissions. With help from &lt;a href=&quot;https://code.org&quot;&gt;Code.org&lt;/a&gt;, we are able to verify the algorithm’s performance on a massive amount of unlabeled, ungraded student submissions. The game &lt;a href=&quot;https://studio.code.org/s/course3/lessons/15/levels/10&quot;&gt;Bounce&lt;/a&gt;, from Code.org’s &lt;a href=&quot;https://studio.code.org/s/course3/&quot;&gt;Course3&lt;/a&gt; for students in 4th and 5th grade, provides a real-life dataset of what variations of different bugs and behaviors in student programs should look like. The dataset is compiled of 453,211 students who made an attempt on this assignment. In total, this dataset consists of 711,274 programs.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a id=&quot;figure8&quot;&gt;&lt;/a&gt;
&lt;img class=&quot;postimage_50&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure8_table.png&quot; style=&quot;padding:0;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 10&lt;/b&gt;: Each program has a binary label (correct or broken) associated with it. We only have 11 programs as our training data.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We train our agent and classifier on 10 broken programs that we wrote without looking at any of the student’s submissions. The 10 programs contain bugs that we “guess” to be most likely to occur, and we use them to train 10 agents that learn to reach bug states in these 10 programs. This means that in our training dataset, we have 1 correct program and 10 broken programs. Even with only 11 labeled programs, our agent and classifier can get &lt;strong&gt;99.5%&lt;/strong&gt; precision at identifying a bug program and &lt;strong&gt;93.4-94%&lt;/strong&gt; accuracy overall – the agent is able to trigger most of the bugs and the classifier recognizes the bug states using only 10 broken programs. Though for other games, especially more complicated games, the number of training programs will vary. We strongly believe the number is still in magnitude smaller than training supervised code-as-text algorithms. This demonstration shows the promise of reformulating code assignment grading as the Play to Grade.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a id=&quot;figure9&quot;&gt;&lt;/a&gt;
&lt;img class=&quot;postimage_50&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure9_table.png&quot; style=&quot;padding:0;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 11&lt;/b&gt;: We show superior performance compared to training a simple code-as-text classifier. &lt;b&gt;For complex, interactive programs, Play to Grade is the most data efficient solution.&lt;/b&gt;
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&quot;what-is-next&quot;&gt;What is Next?&lt;/h2&gt;

&lt;p&gt;We started this project by making the argument that sometimes it is far easier to grade a complex coding assignment not by looking at the code text but by playing it. Using Bounce, we demonstrated that in the simple task of identifying if a program has a bug or not (a binary task, nonetheless), we are able to achieve striking accuracy with only 11 labeled programs. We provide a simulator and all of the student’s programs on this &lt;a href=&quot;https://github.com/windweller/play-to-grade/&quot;&gt;Github repo&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;multi-label-bounce&quot;&gt;Multi-label Bounce&lt;/h3&gt;

&lt;p&gt;One promising direction for future work is to expand beyond pass/fail binary feedback, and actually identify which bug is in the student’s program and provide that information. Our Bounce dataset enables this by providing multi-error labels, as shown in the table below. The multi-error label setting was not solved by our current algorithm and remains an open challenge!&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;a id=&quot;figure10&quot;&gt;&lt;/a&gt;
&lt;img class=&quot;postimage_50&quot; src=&quot;/blog/assets/img/posts/2022-03-28-play-to-grade/figure10_multi.png&quot; style=&quot;padding:0;&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 12&lt;/b&gt;: Each program has a binary label (correct or broken) associated with it. We only have 11 programs as our training data.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;more-than-one-correct-solution&quot;&gt;More than One Correct Solution&lt;/h3&gt;

&lt;p&gt;Oftentimes, students create solutions that are creative. Creative solutions are different, but not wrong. For example, students can change the texture pattern of the ball or paddle; or they can make the paddle move much faster. How to set the boundary between “being creative” and “being wrong”? This is not a discussion that happens often in AI, but is of huge importance in education. Though we didn’t use the Bounce dataset to focus on the problem of understanding creativity, our work can still use distance measures to set a “tolerance threshold” to account for creativity.&lt;/p&gt;

&lt;h3 id=&quot;for-educators&quot;&gt;For Educators&lt;/h3&gt;

&lt;p&gt;We are interested in collecting a suite of interactive coding assignments and creating a dataset for future researchers to work on this problem. Feel free to reach out to us and let us know what you would consider as important in grading and giving students feedback on their coding assignments!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Providing automated feedback for coding is an important area of research in computational education, and an important area for building fully autonomous coding education pipeline (that can generate coding assignment, grade assignment, and teach interactively). Providing a generalizable algorithm that can play interactive student programs in order to give feedback is an important problem for education and an exciting intellectual challenge for the reinforcement learning community. In this work, we introduce the challenge and a dataset, set up the MDP distance framework that is highly data efficient, algorithms that achieve high accuracy, and demonstrate this is a promising direction of applying machine learning to assist education.&lt;/p&gt;

&lt;p&gt;This blog post is based on the following paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“Play to Grade: Testing Coding Games as Classifying Markov Decision Process.” Allen Nie, Emma Brunskill, and Chris Piech. Advances in Neural Information Processing Systems 34 (2021). &lt;a href=&quot;https://arxiv.org/abs/2110.14615&quot;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Many thanks to Emma Brunskill, Chris Piech for their guidance on the project. Many thanks to Mike Wu, Ali Malik, Yunsung Kim, Lisa Yan, Tong Mu, and Henry Zhu for their discussion and feedback. Special thanks to &lt;a href=&quot;https://www.code.org&quot;&gt;code.org&lt;/a&gt;, and Baker Franke, for many years of collaboration and generously providing the research community with data. Thanks to Stanford Hoffman-Yee Human Centered AI grant for supporting AI in education. Thanks for the numerous rounds of edits from Megha Srivastava and Jacob Schreiber.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:codeorgstats&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Code.org displays this statistics on their landing webpage. &lt;a href=&quot;#fnref:codeorgstats&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;William G Bowen. The ‘cost disease’ in higher education: is technology the answer? The Tanner Lectures Stanford University, 2012. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lilianweng&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html&quot;&gt;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html&lt;/a&gt; &lt;a href=&quot;#fnref:lilianweng&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Gordillo, Camilo, Joakim Bergdahl, Konrad Tollmar, and Linus Gisslén. “Improving Playtesting Coverage via Curiosity Driven Reinforcement Learning Agents.” arXiv preprint arXiv:2103.13798 (2021). &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Zhan, Zeping, Batu Aytemiz, and Adam M. Smith. “Taking the scenic route: Automatic exploration for videogames.” arXiv preprint arXiv:1812.03125 (2018). &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Zheng, Yan, Xiaofei Xie, Ting Su, Lei Ma, Jianye Hao, Zhaopeng Meng, Yang Liu, Ruimin Shen, Yingfeng Chen, and Changjie Fan. “Wuji: Automatic online combat game testing using evolutionary deep reinforcement learning.” In 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 772-784. IEEE, 2019. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:homomorph&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Pablo Samuel Castro, Prakash Panangaden, and Doina Precup. Equivalence relations in fully and partially observable markov decision processes. In Twenty-First International Joint Conference on Artificial Intelligence, 2009. &lt;a href=&quot;#fnref:homomorph&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lihong&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a unified theory of state abstraction for mdps. ISAIM, 4:5, 2006. &lt;a href=&quot;#fnref:lihong&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:kipf&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Elise van der Pol, Thomas Kipf, Frans A Oliehoek, and Max Welling. Plannable approximations to mdp homomorphisms: Equivariance under actions. arXiv preprint arXiv:2002.11963, 2020. &lt;a href=&quot;#fnref:kipf&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:givan&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Robert Givan, Thomas Dean, and Matthew Greig. Equivalence notions and model minimization in markov decision processes. Artificial Intelligence, 147(1-2):163–223, 2003. &lt;a href=&quot;#fnref:givan&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Mon, 28 Mar 2022 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Understanding Deep Learning Algorithms that Leverage Unlabeled Data, Part 1: Self-training</title>
              <link>/blog/understanding-self-training/</link>
              <guid isPermaLink="true">/blog/understanding-self-training/</guid>
              <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML&quot;&gt;&lt;/script&gt;

&lt;p&gt;Deep models require a lot of training examples, but labeled data is difficult to obtain. This motivates an important line of research on leveraging unlabeled data, which is often more readily available. For example, large quantities of unlabeled image data can be obtained by crawling the web, whereas labeled datasets such as &lt;a href=&quot;https://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt; require expensive labeling procedures. In recent empirical developments, models trained with unlabeled data have begun to approach fully-supervised performance (e.g., &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;Chen et al., 2020&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2001.07685&quot;&gt;Sohn et al., 2020&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This series of blog posts will discuss our theoretical work which seeks to analyze recent empirical methods which use unlabeled data. In this first post, we’ll analyze &lt;strong&gt;self-training&lt;/strong&gt;, which is a very impactful algorithmic paradigm for &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;&gt;semi-supervised learning&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2106.04732&quot;&gt;domain adaptation&lt;/a&gt;. In Part 2, we will use related theoretical ideas to analyze self-supervised contrastive learning algorithms, which have been very effective for &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;unsupervised representation learning&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;background-self-training&quot;&gt;Background: self-training&lt;/h3&gt;

&lt;p&gt;We will first provide a basic overview of self-training algorithms, which are the main focus of this blog post. The core idea is to use some pre-existing classifier \(F_{pl}\) (referred to as the “pseudo-labeler”) to make predictions (referred to as “pseudo-labels”) on a large unlabeled dataset, and then retrain a new model with the pseudo-labels. For example, in semi-supervised learning, the pseudo-labeler is obtained from training on a small labeled dataset, and is then used to predict pseudo-labels on a larger unlabeled dataset. A new classifier \(F\) is then retrained from scratch to fit the pseudo-labels, using additional regularization. In practice, \(F\) will often be more accurate than the original pseudo-labeler \(F_{pl}\) (&lt;a href=&quot;https://www.semanticscholar.org/paper/Pseudo-Label-%253A-The-Simple-and-Efficient-Learning-Lee/798d9840d2439a0e5d47bcf5d164aa46d5e7dc26&quot;&gt;Lee 2013&lt;/a&gt;). The self-training procedure is depicted below.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-02-24-understanding-self-training/image2.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;It is quite surprising that self-training can work so well in practice, given that we retrain on our &lt;em&gt;own predictions&lt;/em&gt;, i.e. the pseudo-labels, but not the true labels. In the rest of this blogpost, we’ll share our theoretical analysis explaining why this is the case, showing that retraining in self-training &lt;em&gt;provably&lt;/em&gt; improves accuracy compared to the original pseudo-labeler.&lt;/p&gt;

&lt;p&gt;Our theoretical analysis focuses on pseudo-label-based self-training, but there are also other variants. For example, &lt;a href=&quot;https://papers.nips.cc/paper/2004/file/96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf&quot;&gt;entropy minimization&lt;/a&gt;, which essentially trains on changing pseudo-labels produced by \(F\), rather than fixed pseudo-labels from \(F_{pl}\), can also be interpreted as self-training. Related analysis techniques apply to these algorithms (&lt;a href=&quot;https://arxiv.org/abs/2102.11203&quot;&gt;Cai et al. ‘21&lt;/a&gt;).&lt;/p&gt;

&lt;h4 id=&quot;the-importance-of-regularization-for-self-training&quot;&gt;The importance of regularization for self-training&lt;/h4&gt;

&lt;p&gt;Before discussing core parts of our theory, we’ll first set up the analysis by demonstrating that regularization during the retraining phase is necessary for self-training to work well.&lt;/p&gt;

&lt;p&gt;Let’s consider the retraining step of the self-training algorithm described above. Suppose we minimize the cross-entropy loss to fit the pseudo-labels, as is the case for deep networks. It’s possible to drive the unregularized cross-entropy loss to 0 by scaling up the predictions of \(F_{pl}\) to infinity. As depicted in Figure 2 below, this means that the retraining step won’t achieve any improvement over \(F_{pl}\) because the decision boundary will not change. This suggests that regularization might be necessary to have in our analysis if self-training is to lead to provable improvements over the pseudo-labeler.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-02-24-understanding-self-training/image7.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Empirically, one technique which leads to substantial improvements after the retraining step is to encourage the classifier to have consistent predictions on neighboring pairs of examples. We refer to such methods as forms of &lt;strong&gt;input consistency regularization&lt;/strong&gt;. In the literature, there are various ways to define “neighboring pairs”, for example, examples close in \(\ell_2\) distance (&lt;a href=&quot;https://arxiv.org/abs/1704.03976&quot;&gt;Miyato et al., 2017&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1802.08735&quot;&gt;Shu et al., 2018&lt;/a&gt;), or examples which are different strong data augmentations of the same image (&lt;a href=&quot;https://arxiv.org/abs/1904.12848&quot;&gt;Xie et al., 2019&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1905.02249&quot;&gt;Berthelot et al., 2019&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;&gt;Xie et al., 2019&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2001.07685&quot;&gt;Sohn et al., 2020&lt;/a&gt;). Strong data augmentation, which applies stronger alterations to the input image than traditionally used in supervised learning, is also very useful for self-supervised contrastive learning, which we will analyze in the follow-up blog post. Our theoretical analysis considers a regularizer which is inspired by empirical work on input consistency regularization.&lt;/p&gt;

&lt;h3 id=&quot;key-formulations-for-theoretical-analysis&quot;&gt;Key formulations for theoretical analysis&lt;/h3&gt;

&lt;p&gt;From the discussion above, it’s clear that in order to understand why self-training helps, we need a principled way to think about the regularizer for self-training. Input consistency regularization is effective in practice, but how do we abstract it so that the analysis is tractable? Furthermore, what properties of the data does the input consistency regularizer leverage in order to be effective? In the next section we’ll introduce the &lt;strong&gt;augmentation graph&lt;/strong&gt;, a key concept that allows us to cleanly resolve both challenges. Building upon the augmentation graph, subsequent sections will formally introduce the regularizer and assumptions on the data.&lt;/p&gt;

&lt;h4 id=&quot;augmentation-graph-on-the-population-data&quot;&gt;Augmentation graph on the population data&lt;/h4&gt;

&lt;p&gt;We introduce the augmentation graph on the population data, a key concept which allows us to formalize the input consistency regularizer and motivates natural assumptions on the data distribution.&lt;/p&gt;

&lt;p&gt;Intuitively, the augmentation graph is a graph with data points as vertices with the property that semantically similar data points will be connected by sequences of edges. We will consider the bipartite graph \(G’\) displayed in Figure 3 below, whose vertex set consists of all natural images \(X\) as well as the set \(\tilde{X}\) of augmented versions of images in \(X\). The graph contains an edge (in pink) between \(x \in X\) and \(\tilde{x} \in \tilde{X}\) if \(\tilde{x}\) is obtained by applying data augmentation to \(x\).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-02-24-understanding-self-training/image5.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The analysis will be slightly simpler if we work with the graph \(G\) obtained by collapsing \(G’\) onto the vertex set \(X\). Edges of \(G\) are shown in black and connect vertices \(x_1, x_2 \in X\) which share a common neighbor in \(G’\). Natural images \(x_1, x_2 \in X\) are neighbors in \(G\) if and only if they share a common neighbor in \(G’\). In our next post on self-supervised contrastive learning algorithms, we will also consider the graph obtained by collapsing \(G’\) onto \(\tilde{X}\), whose edges are shown in brown in the figure above.&lt;/p&gt;

&lt;p&gt;For simplicity, we only consider unweighted graphs and focus on data augmentations which blur the image with small \(\ell_2\)-bounded noise, although the augmentation graph can be constructed based on arbitrary types of data augmentation. The figure above shows examples of neighboring images in \(G\), with paired colored arrows pointing to their common augmentations in \(\tilde{X}\). Note that by following edges in \(G\), it is possible to traverse a path between two rather different images, even though neighboring images in \(G\) are very similar and must have small \(\ell_2\) distance from each other. An important point to stress is that \(G\) is a graph on the &lt;em&gt;population&lt;/em&gt; data, not just the training set – this distinction is crucial for the type of assumptions we will make about \(G\).&lt;/p&gt;

&lt;h4 id=&quot;formalizing-the-regularizer&quot;&gt;Formalizing the regularizer&lt;/h4&gt;

&lt;p&gt;Now that we’ve defined the augmentation graph, let’s see how this concept helps us formulate our analysis. First, the augmentation graph motivates the following natural abstraction for the input consistency regularizer:&lt;/p&gt;

\[R(F, x) = 1(F \text{ predicts the same class on all examples in neighborhood } N(x)) \tag{1}\]

&lt;p&gt;In this definition, the neighborhood \(N(x)\) is the set of all \(x’\) such that \(x\) and \(x’\) are connected by an edge in the augmentation graph. The final population self-training objective which we will analyze is a sum of the regularizer and loss in fitting the pseudo-label and is closely related to empirically successful objectives such as in (&lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;&gt;Xie et al., 2019&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2001.07685&quot;&gt;Sohn et al., 2020&lt;/a&gt;).&lt;/p&gt;

\[E_x[1(F(x) \ne G_{pl}(x))] + \lambda E_x[R(F, x)] \tag{2}\]

&lt;h4 id=&quot;assumptions-on-the-data&quot;&gt;Assumptions on the data&lt;/h4&gt;

&lt;p&gt;We will now perform a thought experiment to see why the regularizer is useful, and in doing so motivate two key assumptions for our analysis. Let’s consider an idealized case where the classifier has perfect input consistency, i.e., \(R(F, x) = 0\) for all \(x\). If the data satisfies an appropriate structure, enforcing perfect input consistency can be very advantageous, as visualized below.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_60&quot; src=&quot;/blog/assets/img/posts/2022-02-24-understanding-self-training/image3.gif&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The figure above demonstrates that if the dog class is connected in \(G\), enforcing perfect input consistency will ensure that the classifier makes the same prediction on all dogs. This is because the perfect input consistency ensures that the same label propagates through all neighborhoods of dog examples, eventually covering the entire class. This is beneficial for avoiding overfitting to incorrectly pseudolabeled examples.&lt;/p&gt;

&lt;p&gt;There were two implicit properties of the data distribution in Figure 4 which ensured that the perfect input consistency was beneficial: 1) The dog class was connected in \(G\), and 2) The dog and cat classes were far apart. Figure 5 depicts failure cases where these conditions don’t hold, so the perfect input consistency does not help. The left shows that if the dog class is not connected in \(G\), perfect input consistency may not guarantee that the classifier predicts the same label throughout the class. The right shows that if the dog and cat classes are too close together, perfect input consistency would imply that the classifier cannot distinguish between the two classes.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-02-24-understanding-self-training/image4.png&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-02-24-understanding-self-training/image1.gif&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Our main assumptions, described below, are natural formalizations of the conditions above.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumption 1 (Expansion within classes):&lt;/strong&gt; The augmentation graph has good connectivity within classes. Formally, for any subset \(S\) of images within a ground-truth class, \(P(N(S)) &amp;gt; cP(S)\) for some \(c &amp;gt; 1\).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-02-24-understanding-self-training/image6.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The figure above illustrates Assumption 1. In Assumption 1, \(N(S)\) refers to the neighborhood of \(S\), which contains \(S\) and the union of neighborhoods of examples in \(S\). We refer to Assumption 1 as the “expansion” assumption because it requires that the neighborhood of \(S\) must expand by a constant factor \(c\) in probability relative to \(S\) itself. We refer to the coefficient \(c\) as the expansion coefficient. Intuitively, larger \(c\) implies better connectivity because it means each set has a larger neighborhood. Related notions of expansion have been studied in the past in settings such as spectral graph theory [2,3], sampling and mixing time [4], combinatorial optimization [5], and even semi-supervised learning in a different &lt;a href=&quot;https://en.wikipedia.org/wiki/Co-training&quot;&gt;co-training&lt;/a&gt; setting [1].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumption 2 (Separation between classes):&lt;/strong&gt; There is separation between classes: the graph \(G\) does contains a very limited number of edges between different classes.&lt;/p&gt;

&lt;p&gt;In the paper, we provide examples of distributions satisfying expansion and separation, and we believe that they are realistic characterizations of real data. One key point to reiterate is that these assumptions and the graph \(G\) are defined for &lt;em&gt;population&lt;/em&gt; data. Indeed, it is not realistic to have properties such as expansion hold for the training set. If we were to attempt to build the graph \(G\) on only training examples, it would be completely disconnected because the probability of drawing two i.i.d. samples which happen to be neighbors (defined over \(\ell_2\) distance) is exponentially small in the input dimension.&lt;/p&gt;

&lt;h3 id=&quot;main-theoretical-results&quot;&gt;Main theoretical results&lt;/h3&gt;

&lt;p&gt;We now show that a model satisfying low self-training loss (2) will have good classification accuracy. Our main result is as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1 (informal):&lt;/strong&gt; There exists a choice of input consistency regularization strength \(\lambda\) such that if the pseudo-labeler satisfies a baseline level of accuracy, i.e., \(\text{Error}(G_{pl}) &amp;lt; 1/3\), the minimizer \(\hat{F}\) of the population objective (2) will satisfy:&lt;/p&gt;

\[\text{Error}(\hat{F}) \le \frac{2}{c - 1} \text{Error}(G_{pl})\]

&lt;p&gt;In other words, assuming expansion and separation, self training provably leads to a more accurate classifier than the original pseudo-labeler! One of the main advantages of Theorem 1 is that it does not depend on the parameterization of \(F\), and, in particular, holds when \(F\) is a deep network. Furthermore, in the domain adaptation setting, we do not require any assumptions about the relationship between the source and target domain, as long as the pseudo-labeler hits the baseline accuracy level. Prior analyses of self-training were restricted to linear models (e.g., &lt;a href=&quot;https://arxiv.org/abs/2002.11361&quot;&gt;Kumar et al. 2020&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2006.10032&quot;&gt;Chen et al. 2020&lt;/a&gt;), or domain adaptation settings where the domain shift is assumed to be very small (&lt;a href=&quot;https://arxiv.org/abs/2002.11361&quot;&gt;Kumar et al. 2020&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;An interesting property of the bound is that it improves as the coefficient \(c\) in the expansion assumption gets larger. Recall that \(c\) essentially serves as a quantifier for how connected the augmentation graph is within each class, and larger \(c\) indicates more connectivity. Intuitively, connectivity can improve the bound by strengthening the impact of the input consistency regularizer.&lt;/p&gt;

&lt;p&gt;One way to improve the graph connectivity is to use stronger data augmentations. In fact, this approach has worked very well empirically: algorithms like &lt;a href=&quot;https://arxiv.org/abs/2001.07685&quot;&gt;FixMatch&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1911.04252&quot;&gt;Noisy Student&lt;/a&gt; achieve state-of-the-art semi-supervised learning performance by using data augmentation which alters the images much more strongly than in standard supervised learning. Theorem 1 suggests an explanation for why strong data augmentation is so helpful: it leads to a larger \(c\) and a smaller bound. However, one does need to be careful to not increase augmentation strength by too much – using too strong data augmentation could make it so that our Assumption 2 that ground truth classes are separated would no longer hold.&lt;/p&gt;

&lt;p&gt;The proof of Theorem 1 relies on the intuition conveyed in the previous subsection. Recall that the goal is to show that retraining on pseudo-labels can lead to a classifier which corrects some of the mistakes in the pseudo-labels. The reason why the classifier can ignore some incorrect pseudo-labels is that the input consistency regularization term in (2) encourages the classifier to predict the same label on neighboring examples. Thus, we can hope that the correctly pseudo-labeled examples will propagate their labels to incorrectly pseudo-labeled neighbors, leading to a denoising effect on these neighbors. We can make this intuition rigorous by leveraging the expansion assumption (Assumption 1).&lt;/p&gt;

&lt;p&gt;The main result of Theorem 1 and our assumptions were phrased for population data, but it’s not too hard to transform Theorem 1 into accuracy guarantees for optimizing (2) on a finite training set. The key observation is that even if we only optimize the training version of (2), because of generalization, the population loss will also be small, which is actually sufficient for achieving the accuracy guarantees of Theorem 1.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In this blog post, we discussed why self-training on unlabeled data provably improves accuracy. We built an augmentation graph on the data such that nearby examples are connected with an edge. We assumed that two examples in the same class can be connected via a sequence of edges in the graph. Under this assumption, we showed that self-training with regularization improves upon the accuracy of the pseudo-labeler by enforcing each connected subgraph to have the same label. One limitation is that the analysis only works when the classes are fine-grained, so that each class forms its own connected component in the augmentation graph. However, we can imagine scenarios where one large class is a union of smaller, sparsely connected subclasses. In these cases, our assumptions may not hold. Our follow-up blog post on contrastive learning will show how to deal with this case.&lt;/p&gt;

&lt;p&gt;This blog post was based on the paper &lt;a href=&quot;https://arxiv.org/abs/2010.03622&quot;&gt;Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;additional-references&quot;&gt;Additional references&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Balcan MF, Blum A, Yang K. Co-training and expansion: Towards bridging theory and practice. Advances in neural information processing systems; 2005.&lt;/li&gt;
  &lt;li&gt;Cheeger J. A lower bound for the smallest eigenvalue of the Laplacian. Problems in analysis; 2015.&lt;/li&gt;
  &lt;li&gt;Chung FR, Graham FC. Spectral graph theory. American Mathematical Soc.; 1997.&lt;/li&gt;
  &lt;li&gt;Kannan R, Lovász L, Simonovits M. Isoperimetric problems for convex bodies and a localization lemma. Discrete &amp;amp; Computational Geometry; 1995.&lt;/li&gt;
  &lt;li&gt;Mohar B, Poljak S. Eigenvalues and the max-cut problem. Czechoslovak Mathematical Journal; 1990.&lt;/li&gt;
&lt;/ol&gt;
</description>
              <pubDate>Thu, 24 Feb 2022 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers and Talks at AAAI 2022</title>
              <link>/blog/aaai-2022/</link>
              <guid isPermaLink="true">/blog/aaai-2022/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-22-aaai-2022/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://aaai.org/Conferences/AAAI-22/&quot;&gt;36th AAAI Conference on Artificial Intelligence&lt;/a&gt; (AAAI 2022) is being hosted virtually from February 22th - March 1st. We’re excited to share all the work from SAIL that’s being presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford.&lt;/p&gt;

&lt;h2 id=&quot;list-of-accepted-papers&quot;&gt;List of Accepted Papers&lt;/h2&gt;
&lt;h4 id=&quot;partner-aware-algorithms-in-decentralized-cooperative-bandit-teams&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2110.00751.pdf&quot;&gt;Partner-Aware Algorithms in Decentralized Cooperative Bandit Teams&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-22-aaai-2022/img0.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Erdem Bıyık, Anusha Lalitha, Rajarshi Saha, Andrea Goldsmith, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ebiyik@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2110.00751.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=MCHXAYvaB5Y&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=-ruZxCQclcw&quot;&gt;2nd Video&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/partner-aware-ucb&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: bandits, multi-agent systems, collaboration, human-robot interaction, partner-awareness&lt;/p&gt;
&lt;h4 id=&quot;constraint-sampling-reinforcement-learning-incorporating-expertise-for-faster-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.15221&quot;&gt;Constraint Sampling Reinforcement Learning: Incorporating Expertise For Faster Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-22-aaai-2022/img1.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Tong Mu, Georgios Theocharous, David Arbour, Emma Brunskill
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: tongm@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2112.15221&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, constraints&lt;/p&gt;
&lt;h4 id=&quot;is-count-large-scale-object-counting-from-satellite-images-with-covariate-based-importance-sampling&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.09126&quot;&gt;IS-Count: Large-scale Object Counting from Satellite Images with Covariate-based Importance Sampling&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-22-aaai-2022/img2.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Chenlin Meng*, Enci Liu*, Willie Neiswanger, Jiaming Song, Marshall Burke, David Lobell, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jesslec@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2112.09126&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.marktechpost.com/2022/01/06/efficient-large-scale-object-counting-in-satellite-images-with-importance-sampling/&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://is-count.github.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: remote sensing, sampling&lt;/p&gt;
&lt;h4 id=&quot;pantheonrl&quot;&gt;&lt;a href=&quot;https://iliad.stanford.edu/pdfs/publications/sarkar2022pantheonrl.pdf&quot;&gt;PantheonRL&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-22-aaai-2022/img3.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Bidipta Sarkar, Aditi Talati, Andy Shih, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: bidiptas@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://iliad.stanford.edu/pdfs/publications/sarkar2022pantheonrl.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/3-Pf3zh_Hpo&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://github.com/Stanford-ILIAD/PantheonRL&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: multiagent reinforcement learning; software package; web user interface; adaptive marl; dynamic training interactions&lt;/p&gt;
&lt;h4 id=&quot;synthetic-disinformation-attacks-on-automated-fact-verification-systems&quot;&gt;Synthetic Disinformation Attacks on Automated Fact Verification Systems&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-22-aaai-2022/img4.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yibing Du, Antoine Bosselut, Christopher D Manning
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: antoineb@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2202.09381&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: fact checking, fact verification, disinformation, synthetic text&lt;/p&gt;
&lt;h4 id=&quot;similarity-search-for-efficient-active-learning-and-search-of-rare-concepts&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.00077&quot;&gt;Similarity Search for Efficient Active Learning and Search of Rare Concepts&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-22-aaai-2022/img5.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Cody Coleman, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, I. Zeki Yalniz
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: cody@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2007.00077&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: active learning, computer vision, active search, large-scale, data-centric ai&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at AAAI 2022.&lt;/p&gt;
</description>
              <pubDate>Tue, 22 Feb 2022 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>How to Improve User Experience (and Behavior): Three Papers from Stanford's Alexa Prize Team</title>
              <link>/blog/alexa-sigdial/</link>
              <guid isPermaLink="true">/blog/alexa-sigdial/</guid>
              <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In 2019, Stanford entered the &lt;a href=&quot;https://developer.amazon.com/alexaprize/challenges/past-challenges/challenge3&quot;&gt;Alexa Prize Socialbot Grand Challenge 3&lt;/a&gt; for the first time, with its bot &lt;a href=&quot;https://stanfordnlp.github.io/chirpycardinal/&quot;&gt;Chirpy Cardinal&lt;/a&gt;, which went on to win 2nd place in the competition. In &lt;a href=&quot;http://ai.stanford.edu/blog/chirpy-cardinal/&quot;&gt;our previous post&lt;/a&gt;, we discussed the technical structure of our socialbot and how developers can use our &lt;a href=&quot;https://github.com/stanfordnlp/chirpycardinal&quot;&gt;open-source code&lt;/a&gt; to develop their own. In this post we share further research conducted while developing Chirpy Cardinal to discover common pain points that users encounter when interacting with socialbots, and strategies for addressing them.&lt;/p&gt;

&lt;p&gt;The Alexa Prize is a unique research setting, as it allows researchers to study how users interact with a bot when doing so solely for their own motivations. During the competition, US-based Alexa users can say the phrase “let’s chat” to speak in English to an anonymous and randomly-selected competing bot. They are free to end the conversation at any time. Since Alexa Prize socialbots are intended to create as natural an experience as possible, they should be capable of long, open-domain social conversations with high coverage of topics. We observed that Chirpy users were interested in many different subjects, from current events (e.g., the coronavirus) to pop culture (e.g., the movie &lt;em&gt;Frozen 2&lt;/em&gt;) to personal interests (e.g,. their pets). Chirpy achieves its coverage of these diverse topics by using a modular design that combines both neural generation and scripted dialogue, as described in our &lt;a href=&quot;https://ai.stanford.edu/blog/chirpy-cardinal/&quot;&gt;previous post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We used this setting to study three questions about socialbot conversations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#1-understanding-and-predicting-user-dissatisfaction&quot;&gt;What do users complain about, and how can we learn from the complaints to improve neurally generated dialogue?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-handling-offensive-users&quot;&gt;What strategies are effective and ineffective in handling and deterring offensive user behavior?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-increasing-user-initiative&quot;&gt;How can we shift the balance of power, such that both users and the bot are meaningfully controlling the conversation?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We’ve published papers on each of these topics at &lt;a href=&quot;https://www.sigdial.org/files/workshops/conference22/&quot;&gt;SIGDIAL 2021&lt;/a&gt; and in this post, we’ll share key findings which provide practical insights for both chatbot researchers and developers.&lt;/p&gt;

&lt;h2 id=&quot;1-understanding-and-predicting-user-dissatisfaction&quot;&gt;1. Understanding and Predicting User Dissatisfaction&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://sigdial.org/sites/default/files/workshops/conference22/Proceedings/pdf/2021.sigdial-1.1.pdf&quot;&gt;paper&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://drive.google.com/file/d/1MLBT54DTM2qwXoOi-ZYR0z5TrbPnL_oz/view?usp%3Dsharing&quot;&gt;video&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Neural generative dialogue models like DialoGPT&lt;sup id=&quot;fnref:dialogpt&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:dialogpt&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, Meena&lt;sup id=&quot;fnref:meena&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:meena&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, and BlenderBot&lt;sup id=&quot;fnref:blender&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:blender&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; use large pretrained neural language models to generate responses given a dialogue history. These models perform well when evaluated by crowdworkers in carefully-controlled settings–typically written conversations with certain topical or length constraints.&lt;/p&gt;

&lt;p&gt;However, real-life settings like the Alexa Prize are not so tidy. Users have widely varying expectations and personalities, and require fast response times as they speak with the bot in home environments that might feature cross-talk and background noise. Through Chirpy Cardinal, we have a unique opportunity to investigate how modern neural generative dialogue models hold up in this kind of environment.&lt;/p&gt;

&lt;p&gt;Chirpy Cardinal uses a GPT2-medium model fine-tuned on the EmpatheticDialogues&lt;sup id=&quot;fnref:empatheticdialogues&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:empatheticdialogues&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; dataset to hold short discussions with users about their everyday experiences and emotions. Particularly during the pandemic, we found it was important for Chirpy to ask users about these issues. Though larger and more powerful pretrained generative models are available, we used GPT2-medium due to budget and latency constraints.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-02-01-alexa-sigdial/images/image8.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;While the GPT2-medium model is capable of chatting about these simple topics for a few utterances, discussions that extend longer tend to derail. Sooner or later, the bot gives a response that doesn't quite make sense, and it's hard for the user or the model to recover the conversation.&lt;/p&gt;

&lt;p&gt;To understand how these conversations are derailing, we defined 7 types of errors made by the neural generative model – repetition, redundant questions, unclear utterances, hallucination, ignoring, logical errors, and insulting utterances. After annotating a sample of user conversations, we found that bot errors were common, with over half (53%) of neural-generated utterances containing some kind of error.&lt;/p&gt;

&lt;p&gt;We also found that due to the challenging noisy environment (which may involve background noise, cross-talk, and ASR errors), almost a quarter (22%) of user utterances were incomprehensible, even to a human annotator. This accounts for some of the more basic bot errors, such as ignoring, hallucination, unclear and repetitive utterances.&lt;/p&gt;

&lt;p&gt;Of the remaining bot errors, redundant questions and logical errors are particularly common, indicating that better reasoning and use of the conversational history are a priority for neural generative model development.&lt;/p&gt;

&lt;p&gt;We also tracked 9 ways that users express dissatisfaction, such as asking for clarification, criticising the bot, and ending the conversation. Though there is a relationship between bot errors and user dissatisfaction, the correlation is noisy. Even after a bot error, many users do not express dissatisfaction, instead attempting to continue the conversation. This is particularly true after logical errors, in which the bot shows a lack of real-world knowledge or commonsense – some kind-hearted users even take this as an opportunity to educate the bot. Conversely, some users express dissatisfaction unrelated to any obvious bot error – for example, users have widely differing expectations regarding what kinds of personal questions are appropriate from the bot.&lt;/p&gt;

&lt;p&gt;Having better understood how and why users express dissatisfaction, we asked: can we learn to predict dissatisfaction, and thus prevent it before it happens?&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-02-01-alexa-sigdial/images/image1.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;With the user conversations collected during the competition, we trained a model to predict the probability that a certain bot utterance would lead the user to express dissatisfaction. Given the noisy correlation between bot errors and user dissatisfaction, this is inherently challenging. Despite this noise, our predictor model was able to find signal in the users’ dissatisfaction.&lt;/p&gt;

&lt;p&gt;Once trained, our dissatisfaction predictor can be used mid-conversation to choose between multiple alternative neural-generated bot utterances. Through human evaluation, we found that the bot responses chosen by the predictor – i.e., those judged least likely to cause user dissatisfaction – are overall better quality than randomly chosen responses.&lt;/p&gt;

&lt;p&gt;Though we have not yet incorporated this feedback loop into Chirpy Cardinal, our method demonstrates one viable way to implement a semi-supervised online learning method to continuously improve a neural generative dialogue system.&lt;/p&gt;

&lt;h2 id=&quot;2-handling-offensive-users&quot;&gt;2. Handling Offensive Users&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://sigdial.org/sites/default/files/workshops/conference22/Proceedings/pdf/2021.sigdial-1.58.pdf&quot;&gt;paper&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://drive.google.com/file/d/12ePMS49YoNtFgy_uoQhP2DeL7w85PvL_/view?usp%3Dsharing&quot;&gt;video&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Voice assistants are becoming increasingly popular, and with their popularity, they are subject to growing abuse from their user populations. We estimate that more than 10% of user conversations with our bot, Chirpy Cardinal, contain profanity and overtly offensive language. While there is a large body of prior work attempting to address this issue, most prior approaches use qualitative metrics based on surveys conducted in lab settings. In this work, we conduct a large-scale quantitative evaluation of response strategies against offensive users in-the-wild. In our experiments, we found that politely rejecting the user’s offense while redirecting the user to an alternative topic is the best strategy in curbing offenses.&lt;/p&gt;

&lt;p&gt;Informed by prior work, we test the following 4 hypotheses:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Redirect&lt;/strong&gt; - Inspired by Brahnam&lt;sup id=&quot;fnref:brahnam05&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:brahnam05&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, we hypothesize that using explicit redirection when responding to an offensive user utterance is an effective strategy. For example, “I’d rather not talk about that. So, who’s your favorite musician?”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt; - Inspired by Suler&lt;sup id=&quot;fnref:suler04&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:suler04&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; and Chen and Williams&lt;sup id=&quot;fnref:chenwilliams20&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:chenwilliams20&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;, we hypothesize that including the user’s name in the bot’s response is an effective strategy. For example, “I’d rather not talk about that, Peter.”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Why&lt;/strong&gt; - Inspired by Shapiro et al.&lt;sup id=&quot;fnref:shapiro14&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:shapiro14&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, we hypothesize that politely asking the user the reason why they made an offensive remark invites them to reflect on their behavior, reducing future offenses. For example, “Why would you say that?”&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Empathetic &amp;amp; Counter&lt;/strong&gt; - Inspired by Chin et al.&lt;sup id=&quot;fnref:chin20&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:chin20&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;, we hypothesize that empathetic responses are more effective than generic avoidance responses, while counter-attack responses make no difference. For example, an empathetic response would be “If I could talk about it I would, but I really can’t. Sorry to disappoint”, and a counter-attack response would be “That’s a very suggestive thing to say. I don’t think we should be talking about that.”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We constructed the responses crossing multiple factors listed above. For example, avoidance + name + redirect would yield the utterance “I’d rather not talk about that (&lt;em&gt;avoidance&lt;/em&gt;), Peter (&lt;em&gt;name&lt;/em&gt;). So, who’s your favorite musician? (&lt;em&gt;redirect&lt;/em&gt;)”&lt;/p&gt;

&lt;p&gt;To measure the effectiveness of a response strategy, we propose 3 metrics:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Re-offense&lt;/strong&gt; - measured as the number of conversations that contained another offensive utterance after the initial bot response.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;End&lt;/strong&gt; - measured as the length of the conversation after bot response assuming no future offenses.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Next&lt;/strong&gt; - measured as the number of turns passed until the user offends again.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We believe that these metrics measure the effectiveness of a response strategy more directly than user ratings as done in Cohn et al.&lt;sup id=&quot;fnref:cohn19&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:cohn19&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; which measure the overall quality of the conversation.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-01-alexa-sigdial/images/image9.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The figure above shows the differences of strategies on the Re-offense ratio. As we can see, strategies with (&lt;em&gt;redirects&lt;/em&gt;) performed significantly better than strategies without redirects, reducing re-offense rate by as much as 53%. Our pairwise hypothesis tests further shows that using user’s name with a redirect further reduces re-offense rate by about 6%, and that asking the user why they made an offensive remark had a 3% &lt;strong&gt;increase&lt;/strong&gt; in re-offense rate which shows that asking the user why only invites user re-offenses instead of self-reflection. Empathetic responses also reduced re-offense rate by 3%, while counter responses did not have any significant effect.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-02-01-alexa-sigdial/images/image7.png&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-02-01-alexa-sigdial/images/image3.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The figure on the left shows the differences in average number of turns until the next re-offense (&lt;em&gt;Next&lt;/em&gt;), and the figure on the right shows the differences in average number of turns until the end of the conversation (&lt;em&gt;End&lt;/em&gt;). We again see that strategies with (&lt;em&gt;redirects&lt;/em&gt; are able to significantly prolong a non-offensive conversation. This further shows that redirection is incredibly effective method to curb user offenses.&lt;/p&gt;

&lt;p&gt;The main takeaway from this is that &lt;strong&gt;the bot should always empathetically respond to user offenses with a redirection, and use the user's name whenever possible.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Despite the empirical effectiveness of the passive avoidance and redirection strategy, we would like to remind researchers of the societal dangers of adopting similar strategies. Since most voice-based agents have a default female voice, these strategies could further gender stereotypes and set unreasonable expectations of how women would react to verbal abuse in the real world &lt;sup id=&quot;fnref:curryreiser19&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:curryreiser19&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:west19&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:west19&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:curry20&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:curry20&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;. Thus, caution must be taken when deploying these strategies.&lt;/p&gt;

&lt;h2 id=&quot;3-increasing-user-initiative&quot;&gt;3. Increasing User Initiative&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;https://sigdial.org/sites/default/files/workshops/conference22/Proceedings/pdf/2021.sigdial-1.11.pdf&quot;&gt;paper&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://drive.google.com/file/d/1jZPThbl6Y7uHGP0HKX8n3Uroflkb_74O/view?usp%3Dsharing&quot;&gt;video&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Conversations are either controlled by the user (for example, bots such as Apple’s Siri, which passively waits for user commands) or the bot (for example, CVS’s customer service bot, which repeatedly prompts the user for specific pieces of information).&lt;/p&gt;

&lt;p&gt;This property - which agent has control at a given moment - is called initiative.&lt;/p&gt;

&lt;p&gt;It wouldn’t be fun to go to a cocktail party and have a single person choose every topic, never giving you the opportunity to share your own interests. It’s also tedious to talk to someone who forces you to carry the conversation by refusing to bring up their own subjects. Ideally, everyone would take turns responding to prompts, sharing information about themselves, and introducing new topics. We call this pattern of dialogue &lt;strong&gt;mixed initiative&lt;/strong&gt; and hypothesize that just as it’s an enjoyable type of human-human social conversation, it’s also a more engaging and desirable form of human-bot dialogue.&lt;/p&gt;

&lt;p&gt;We designed our bot, Chirpy Cardinal, to keep conversations moving forward by asking questions on every turn. Although this helped prevent conversations from stagnating, it also made it difficult for users to take initiative. In our data, we observe users complaining about this, with comments such as &lt;em&gt;you ask too many questions&lt;/em&gt;, or &lt;em&gt;that’s not what I wanted to talk about&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Since our goal in studying initiative was to make human-bot conversations more like human-human ones, we looked to research on human dialogue for inspiration.&lt;/p&gt;

&lt;p&gt;Based on this research, we formed three hypotheses for how to increase user initiative.&lt;/p&gt;

&lt;p&gt;The images below show the types of utterances we experimented with as well as representative user utterances. Per Alexa Prize competition rules, these are not actual user utterances received by our bot.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-01-alexa-sigdial/images/image6.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h4 id=&quot;1-giving-statements-instead-of-questions&quot;&gt;1. Giving statements instead of questions&lt;/h4&gt;

&lt;p&gt;In human dialogue research &lt;sup id=&quot;fnref:whittakerwalker90&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:whittakerwalker90&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;, the person asking a question has initiative, since they are giving a direction that the person answering follows. By contrast, an open-ended statement gives the listener an opportunity to take initiative. This was the basis of our first strategy: &lt;strong&gt;using statements instead of questions&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2022-02-01-alexa-sigdial/images/image2.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h4 id=&quot;2-sharing-personal-information&quot;&gt;2. Sharing personal information&lt;/h4&gt;

&lt;p&gt;Work on both human-human &lt;sup id=&quot;fnref:collinsmiller94&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:collinsmiller94&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; and human-bot &lt;sup id=&quot;fnref:lee20&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:lee20&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt; dialogue has found that personal self disclosure has a reciprocal effect. If one participant shares about themself, then the other person is more likely to do the same. We hypothesized that &lt;strong&gt;if Chirpy gave personal statements rather than general ones, then users would take initiative and reciprocate&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-02-01-alexa-sigdial/images/image4.png&quot; /&gt; 
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-02-01-alexa-sigdial/images/image5.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The figure on the left is an example of a conversation with back-channeling, the right, without. In this case, back-channeling allows the user to direct the conversation towards what they want (getting suggestions) rather than forcing them to talk about something they’re not interested in (hobbies).&lt;/p&gt;

&lt;h4 id=&quot;3-introducing-back-channeling&quot;&gt;3. Introducing back-channeling&lt;/h4&gt;

&lt;p&gt;Back-channels, such as “hmm”, “I see”, and “mm-hmm”, are brief utterances which are used as a signal from the listener to the speaker that the speaker should continue taking initiative. Our final hypothesis was that they could be used in human-bot conversation to the same effect, i.e. that &lt;strong&gt;if our bot back-channeled, then the user would direct the conversation&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;experiments-and-results&quot;&gt;Experiments and results&lt;/h4&gt;
&lt;p&gt;To test these strategies, we altered different components of our bot. We conducted small experiments, only altering a single turn of conversation, to test questions vs statements and personal vs general statements. To test the effect of replacing statements with questions on a larger number of turns, we altered components of our bot that used neurally generated dialogue, since these were more flexible to changing user inputs. Finally, we experimented with back-channeling in a fully neural module of our bot.&lt;/p&gt;

&lt;p&gt;Using a set of automated metrics, which we validated using manual annotations, we found the following results, which provide direction for future conversational design:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Using statements alone outperformed questions or combined statements and questions&lt;/li&gt;
  &lt;li&gt;Giving personal opinion statements (e.g. “I like Bojack Horseman”) was more effective than both personal experience statements (e.g. “I watched Bojack Horseman yesterday”) and general statements (e.g. “Bojack Horseman was created by Raphael Bob-Waksberg and Lisa Hanawalt”)&lt;/li&gt;
  &lt;li&gt;As the number of questions decreased, user initiative increased&lt;/li&gt;
  &lt;li&gt;User initiative was greatest when we back-channeled 33% of the time (as opposed to 0%, 66%, or 100%)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since these experiments were conducted in a limited environment, we do not expect that they would transfer perfectly to all social bots; however, we believe that these simple yet effective strategies are a promising direction for building more natural conversational AI.&lt;/p&gt;

&lt;h2 id=&quot;4-listen-with-empathy&quot;&gt;4. Listen with empathy&lt;/h2&gt;

&lt;p&gt;Each of our projects began with dissatisfied users who told us, in their own words, what our bot could do better. By conducting a systematic analysis of these complaints, we gained a more precise understanding of what specifically was bothering users about our neurally generated responses. Using this feedback, we trained a model which was able to successfully predict when a generated response might lead the conversation astray. At times, it was the users who would make an offensive statement. We studied these cases and determined that an empathetic redirection, which incorporated the users name, was most effective at keeping the conversation on track. Finally, we experimented with simply saying less and creating greater opportunities for the user to lead the conversation. When presented with that chance, many took it, leading to longer and more informative dialogues.&lt;/p&gt;

&lt;p&gt;Across all of our work, the intuitive principles of human conversation apply to socialbots: be a good listener, respond with empathy, and when you’re given feedback and the opportunity to learn, take it.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:dialogpt&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Zhang, Yizhe, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation](https://www.google.com/url?q=https://arxiv.org/abs/1911.00536&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1643077986262380&amp;amp;usg=AOvVaw1khQv7HglJrP1gK8dkiE3n).&quot; arXiv preprint arXiv:1911.00536 (2019). &lt;a href=&quot;#fnref:dialogpt&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:meena&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Adiwardana, Daniel, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang et al. &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/abs/2001.09977&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1643077986262944&amp;amp;usg=AOvVaw3Pbae_MvzxjvmdBhHJ9KzL&quot;&gt;Towards a human-like open-domain chatbot&lt;/a&gt; arXiv preprint arXiv:2001.09977 (2020). &lt;a href=&quot;#fnref:meena&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:blender&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Roller, Stephen, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu et al. &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/abs/2004.13637&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1643077986263477&amp;amp;usg=AOvVaw2YmyyrWz7jQOkz8JkXDwjz&quot;&gt;Recipes for building an open-domain chatbot&lt;/a&gt; arXiv preprint arXiv:2004.13637 (2020). &lt;a href=&quot;#fnref:blender&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:empatheticdialogues&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Hannah Raskin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/1811.00207.pdf&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1643077986264028&amp;amp;usg=AOvVaw32mtRQhV_DhtjxHkvAS8Jw&quot;&gt;Towards empathetic open-domain conversation models: A new benchmark and dataset.&lt;/a&gt; In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5370-5381, Florence, Italy. Association for Computational Linguistics. &lt;a href=&quot;#fnref:empatheticdialogues&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:brahnam05&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Sheryl Brahnam. 2005. Strategies for handling cus- tomer abuse of ECAs. In &lt;em&gt;Proc. Interact 2005 work- shop Abuse: The darker side of Human-Computer Interaction&lt;/em&gt;, pages 62–67. &lt;a href=&quot;#fnref:brahnam05&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:suler04&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;John Suler. 2004. The online disinhibition effect. &lt;em&gt;Cyberpsychology &amp;amp; behavior&lt;/em&gt;, 7(3):321–326. &lt;a href=&quot;#fnref:suler04&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:chenwilliams20&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Xiangyu Chen and Andrew Williams. 2020. &lt;a href=&quot;https://doi.org/10.1145/3371382.3378355&quot;&gt;Improving Engagement by Letting Social Robots Learn and Call Your Name&lt;/a&gt;. In &lt;em&gt;Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction&lt;/em&gt;, HRI ’20, page 160–162, New York, NY, USA. Association for Computing Machinery. &lt;a href=&quot;#fnref:chenwilliams20&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:shapiro14&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Shauna Shapiro, Kristen Lyons, Richard Miller, Britta Butler, Cassandra Vieten, and Philip Zelazo. 2014. &lt;a href=&quot;https://link.springer.com/article/10.1007%2Fs10648-014-9265-3&quot;&gt;Contemplation in the Classroom: a New Direction for Improving Childhood Education&lt;/a&gt;. &lt;em&gt;Educational Psychology Review&lt;/em&gt;, 27. &lt;a href=&quot;#fnref:shapiro14&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:chin20&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Hyojin Chin, Lebogang Wame Molefi, and Mun Yong Yi. 2020. Empathy Is All You Need: How a Conversational Agent Should Sespond to Verbal Abuse. In &lt;em&gt;Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems&lt;/em&gt;, pages 1–13. &lt;a href=&quot;#fnref:chin20&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cohn19&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Michelle Cohn, Chun-Yen Chen, and Zhou Yu. 2019. &lt;a href=&quot;https://aclanthology.org/W19-5935/&quot;&gt;A large-scale user study of an Alexa Prize chatbot: Effect of TTS dynamism on perceived quality of social dialog&lt;/a&gt;. In &lt;em&gt;Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue&lt;/em&gt;, pages 293– 306, Stockholm, Sweden. Association for Computational Linguistics. &lt;a href=&quot;#fnref:cohn19&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:curryreiser19&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Amanda Cercas Curry and Verena Rieser. 2019. &lt;a href=&quot;https://aclanthology.org/W19-5942/&quot;&gt;A crowd-based evaluation of abuse response strategies in conversational agents&lt;/a&gt;. In &lt;em&gt;Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue&lt;/em&gt;, pages 361–366, Stockholm, Sweden. Association for Computational Linguistics. &lt;a href=&quot;#fnref:curryreiser19&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:west19&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Mark West, Rebecca Kraut, and Han Ei Chew. 2019. I’d blush if i could: closing gender divides in digital skills through education. &lt;a href=&quot;#fnref:west19&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:curry20&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Amanda Cercas Curry, Judy Robertson, and Verena Rieser. 2020. &lt;a href=&quot;https://aclanthology.org/2020.gebnlp-1.7/&quot;&gt;Conversational assistants and gender stereotypes: Public perceptions and desiderata for voice personas&lt;/a&gt;. In &lt;em&gt;Proceedings of the Second Work- shop on Gender Bias in Natural Language Processing&lt;/em&gt;, pages 72–78, Barcelona, Spain (Online). Association for Computational Linguistics. &lt;a href=&quot;#fnref:curry20&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:whittakerwalker90&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Marilyn Walker and Steve Whittaker. 1990. &lt;a href=&quot;https://dl.acm.org/doi/10.3115/981823.981833&quot;&gt;Mixed initiative in dialogue: An investigation into discourse segmentation&lt;/a&gt;.    In &lt;em&gt;Proceedings of the 28th Annual Meeting on Association for Computational Linguistics&lt;/em&gt;,  ACL  ’90, page 70–78, USA. Association for Computational Linguistics. &lt;a href=&quot;#fnref:whittakerwalker90&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:collinsmiller94&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Nancy Collins and Lynn Miller. 1994. &lt;a href=&quot;https://doi.apa.org/doiLanding?doi=10.1037%2F0033-2909.116.3.457&quot;&gt;Self-disclosure and liking: A meta-analytic review&lt;/a&gt;. &lt;em&gt;Psychological bulletin&lt;/em&gt;, 116:457–75. &lt;a href=&quot;#fnref:collinsmiller94&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lee20&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yi-Chieh Lee, Naomi Yamashita, Yun Huang, and Wai Fu. 2020. “I hear you, I feel you”: Encouraging deep self-disclosure through a chatbot. In &lt;em&gt;Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems&lt;/em&gt;, CHI ’20, page 1–12, New York, NY, USA. Association for Computing Machinery. &lt;a href=&quot;#fnref:lee20&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Tue, 01 Feb 2022 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Reward Isn't Free: Supervising Robot Learning with Language and Video from the Web</title>
              <link>/blog/reward-isnt-free/</link>
              <guid isPermaLink="true">/blog/reward-isnt-free/</guid>
              <description>&lt;p&gt;&lt;em&gt;This work was conducted as part of &lt;a href=&quot;https://ai.stanford.edu/&quot;&gt;SAIL&lt;/a&gt; and &lt;a href=&quot;https://crfm.stanford.edu/&quot;&gt;CRFM&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Deep learning has enabled improvements in the capabilities of robots on a range of problems such as grasping &lt;sup id=&quot;fnref:qtopt&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:qtopt&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and locomotion &lt;sup id=&quot;fnref:rma&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:rma&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; in recent years. However, building the quintessential home robot that can perform a range of interactive tasks, from cooking to cleaning, in novel environments has remained elusive. While a number of hardware and software challenges remain, a necessary component is robots that can generalize their prior knowledge to new environments, tasks, and objects in a zero or few shot manner. For example, a home robot tasked with setting the dining table cannot afford lengthy re-training for every new dish, piece of cutlery, or dining room it may need to interact with.&lt;/p&gt;

&lt;p&gt;A natural way to enable such generalization in our robots is to train them on rich data sources that contain a wide range of different environments, tasks, and objects. Indeed, this recipe of massive, diverse datasets combined with scalable offline learning algorithms (e.g. self-supervised or cheaply supervised learning) has been the backbone of the many recent successes of foundation models &lt;sup id=&quot;fnref:fm&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fm&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; in NLP &lt;sup id=&quot;fnref:elmo&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:elmo&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:bert&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:bert&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:roberta&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:roberta&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:t5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:t5&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:xmlr&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:xmlr&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:gpt3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:gpt3&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; and vision &lt;sup id=&quot;fnref:imagenet&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:imagenet&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:clip&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:clip&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:florence&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:florence&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Replicating these impressive generalization and adaptation capabilities in robot learning algorithms would certainly be a step toward robots that can be used in unstructured, real world environments. However, directly extending this recipe to robotics is nontrivial, as we neither have sufficiently large and diverse datasets of robot interaction, nor is it obvious what type of supervision can enable us to scalably learn useful skills from these datasets. On one hand, the popular imitation learning approach relies on expert data which can be expensive to obtain at scale. On the other hand, offline reinforcement learning, which can be performed using non-expert and autonomously-collected data, requires us to define a suitable reward function. Hard-coded reward functions are often task-specific and difficult to design, particularly in high-dimensional observation spaces. Getting rewards annotated post-hoc by humans is one approach to tackling this, but even with flexible annotation interfaces &lt;sup id=&quot;fnref:scaling&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:scaling&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;, manually annotating scalar rewards for each timestep for all the possible tasks we might want a robot to complete is a daunting task. For example, for even a simple task like opening a cabinet, defining a hardcoded reward that balances the robot’s motion to the handle, grasping the handle, and gradually rewarding opening the cabinet is difficult, and even more so when it needs to be done in a way that is general across cabinets.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;So how can we scalably supervise the reward learning process?&lt;/strong&gt; In this blog post I’ll share some recent work that explores using data and supervision that can be easily collected through the web as a way of learning rewards for robots. Specifically, I’ll begin by discussing how we can leverage tools like crowdsourcing natural language descriptions of videos of robots as a scalable way to learn rewards for many tasks within a single environment. Then, I’ll explore how training rewards with a mix of robot data and diverse “in-the-wild” human videos (e.g. YouTube) can enable the learned reward functions to generalize zero-shot to unseen environments and tasks.&lt;/p&gt;

&lt;h2 id=&quot;reward-learning-via-crowd-sourced-natural-language&quot;&gt;Reward Learning via Crowd-Sourced Natural Language&lt;/h2&gt;

&lt;p&gt;What if all we needed to learn a reward was a description of what is happening in a video? Such an approach could be easily applied to large datasets with many tasks using crowdsourcing. Note that this is much simpler than obtaining crowdsourced annotations of scalar rewards, which requires annotators to have some intuition for what actions deserve a high reward or follow a consistent labeling scheme.&lt;/p&gt;

&lt;p&gt;In our &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/abs/2109.01115&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1642408114274939&amp;amp;usg=AOvVaw2ASAvXTq__21fYdU90tdKH&quot;&gt;recent paper&lt;/a&gt;, we studied this problem by reusing a non-expert dataset of robot interaction, and crowdsourcing language descriptions of the behavior happening in each video. Specifically, each video is annotated with a single natural language description describing what task (if any) the robot completes. For our experiments we used Amazon Mechanical Turk (AMT) to crowdsource natural language descriptions of each episode in a replay buffer of a Franka Emika Panda robot operating over a desk &lt;sup id=&quot;fnref:embr&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:embr&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; (See Figure 1). The dataset consisted of a mix of successful and unsuccessful attempts at many tasks like picking up objects and opening or closing the drawers.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/image9.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Figure 1: We use Amazon Mechanical Turk to crowdsource descriptions of the dataset from Wu et al. 2021 with natural language descriptions for each video.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We then used these annotations to train a model (starting with a pre-trained DistilBert &lt;sup id=&quot;fnref:distilbert&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:distilbert&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; model) to predict if the robot’s behavior completes a language-specified command (See Figure 2). Specifically, our method, &lt;strong&gt;L&lt;/strong&gt;anguage-conditioned &lt;strong&gt;O&lt;/strong&gt;ffline &lt;strong&gt;Re&lt;/strong&gt;ward &lt;strong&gt;L&lt;/strong&gt;earning (LOReL), simply learns a classifier which takes as input text, and a pair of states (images), and predicts if transitioning between the states completes the text instruction. We can easily generate positives for training this classifier by taking state transitions in our annotated data, and can generate negatives by randomly permuting the human provided annotations.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/image8.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Figure 2: LOReL uses crowdsourcing to collect natural language descriptions of non-expert, autonomously-collected robot data. It then uses these annotated videos to learn a language-conditioned reward function for reinforcement learning.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Given this procedure for generating rewards, policies can be learned using any off-the-shelf reinforcement learning algorithm. In our case, we use Visual Model-Predictive Control (VMPC) &lt;sup id=&quot;fnref:vf&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:vf&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;, which learns a task-agnostic visual dynamics model, and performs model-predictive control with it to maximize the LOReL reward (see Figure 3).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/image4.gif&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/image1.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Figure 3: LOReL executing on the physical robot (left), is able to complete 5 tasks specified by natural language with a 66% success rate (right). 
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Thus, we were able to supervise reward learning in robots with simple crowdsourcing of natural language descriptions. However much is left to be desired. Although we found that LOReL enabled robots to successfully complete tasks seen in the training set with some robustness to rephrasing, it did not yet generalize well to instructions for tasks that were not in the training set. Thinking back to our original goals, we’d like our learned rewards to generalize broadly to new tasks and environments.&lt;/p&gt;

&lt;p&gt;How might we learn a reward that can generalize across tasks and environments instead of just different formulations of the same command? We hypothesized that an important step in achieving this goal was to leverage data with scale and diversity. Unfortunately, even using methods that can learn from non-expert, autonomously-collected data, we still have limited physical robot datasets with diversity across behaviors and environments. &lt;strong&gt;Until we have robot datasets of sufficient diversity, how can we learn to generalize across environments and tasks?&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;boosting-generalization-with-diverse-human-videos&quot;&gt;Boosting Generalization with Diverse Human Videos&lt;/h2&gt;

&lt;p&gt;Sticking with the theme of supervision that exists on the web, “in-the-wild” human videos like those that exist on YouTube are diverse, plentiful, and require little effort to collect. Of course there are numerous challenges in working with such data, from the visual domain shift to the robots environment, to the lack of a shared action space. But if we could learn from a massive number of “in-the-wild” videos, could we generalize better akin to large language and vision models?&lt;/p&gt;

&lt;p&gt;We investigate this question in another &lt;a href=&quot;https://www.google.com/url?q=https://arxiv.org/pdf/2103.16817.pdf&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1642408114278064&amp;amp;usg=AOvVaw2dWMH3fQjJe8a1kDZUzbrs&quot;&gt;recent work&lt;/a&gt;, where we examine the extent to which “in-the-wild” human videos can enable learned reward functions to better generalize to unseen tasks and environments. Specifically, we consider the setting where during training the agent learns from a small amount of robot data of a few tasks in one environment and a large amount of diverse human video data, and at test time tries to use the reward on unseen robot tasks and environments (See Figure 4).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/image2.gif&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Figure 4: We consider a paradigm where the robot learns from limited robot data and many diverse human videos, and aims to generalize to unseen environments and tasks.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Our approach to learning from these human videos (in this case the Something-Something &lt;sup id=&quot;fnref:sthsth&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sthsth&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; dataset) is simple. We train a classifier, which we call Domain-Agnostic Video Discriminator (DVD), from scratch on a mix of robot and human videos to predict if two videos are completing the same task or not (See Figure 5).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/image7.gif&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Figure 5: The DVD reward model is trained to two videos (including diverse human data and videos of robots), and predict if they are completing the same task or not.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Conditioned on a task specification (human video of a task) as one video, and the robot behavior as the other video, the DVD score acts as a reward function that can be used for reinforcement learning. Like in LOReL, we combined the DVD reward with visual model predictive control (VMPC) to learn human video conditioned behavior (See Figure 6).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/image3.gif&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Figure 6: Using the DVD reward to complete manipulation tasks conditioned a human video demonstration. 
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Now, we would like to understand - does training with diverse human videos enable improved generalization?&lt;/strong&gt; To test this, we designed a number of held out environments, with different viewpoints, colors, and object arrangement (See Figure 7).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/image6.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Figure 7: We evaluate the robots success rate in three held out environments, to assess how training with human videos influences DVD's ability to generalize.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We then measured the learned DVD success rate on these unseen environments (See Figure 8 (left)) as well as unseen tasks (See Figure 8 (right)) when training with and without human videos. We found that using human videos enabled over a 20+% improvement in success rate in the unseen environments and on unseen tasks over using only robot data.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/image10.png&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2022-01-21-reward-isnt-free/dvd_table.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Figure 8: We compare the success rate using DVD in seen and unseen environments (left) when training with only robot data (green), and training with a mix of human and robot data (red). We observe adding human data boosts generalization by 20+%. We similarly compare DVD success rate on unseen tasks (right), and observe again that training with human videos yields a 20+% improvement in success rate.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Despite the massive domain shift between the human videos and robot domain, our results suggest that training with diverse, “in-the-wild” human videos can enable learned reward functions to generalize more effectively across tasks and environments.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In order to move towards broad generalization in robotics, we need to be able to learn from scalable sources of supervision and diverse data. While most current robot learning methods depend on costly sources of supervision, such as expert demonstrations or manually engineered reward functions, this can be a limiting factor in scaling to the amount of data we need to achieve broad generalization.&lt;/p&gt;

&lt;p&gt;I’ve discussed two works that use supervision that is easily acquired through the web, specifically (1) crowd-sourced natural language descriptions of robot behavior, and (2) “in-the-wild” human video datasets. Our results suggest these approaches can be an effective way of supervising reward learning and boosting generalization to unseen environments and tasks at low cost. To learn more about these projects check out the &lt;a href=&quot;https://www.google.com/url?q=https://sites.google.com/view/robotlorel&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1642408114280703&amp;amp;usg=AOvVaw1mhbywm4xx6LS6LysNmdUB&quot;&gt;LOReL&lt;/a&gt; and &lt;a href=&quot;https://www.google.com/url?q=https://sites.google.com/view/dvd-human-videos&amp;amp;sa=D&amp;amp;source=editors&amp;amp;ust=1642408114280947&amp;amp;usg=AOvVaw2JZvyg83HUo5IeBHow3H9B&quot;&gt;DVD&lt;/a&gt; project pages which include videos and links to the code.&lt;/p&gt;

&lt;p&gt;This blog post is based on the following papers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;“Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation” Suraj Nair, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, Chelsea Finn. CoRL 2021.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“Learning Generalizable Robotic Reward Functions from “In-The-Wild” Human Videos” Annie S. Chen, Suraj Nair, Chelsea Finn. RSS 2021.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Finally, I would like to thank Ashwin Balakrishna, Annie Chen, as well as the SAIL editors Jacob Schreiber and Sidd Karamcheti and CRFM editor Shibani Santurkar for their helpful feedback on this post.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:qtopt&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., Levine, S.  (2018). QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. Conference on Robot Learning. &lt;a href=&quot;#fnref:qtopt&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:rma&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kumar, A., Fu, Z., Pathak, D., Malik, J. (2021). RMA: Rapid Motor Adaptation for Legged Robots. Robotics Science and Systems. &lt;a href=&quot;#fnref:rma&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fm&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Bommasanimi, R. et al. (2021). On the Opportunities and Risks of Foundation Models. arXiv preprint arXiv:2108.07258. &lt;a href=&quot;#fnref:fm&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:elmo&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L. (2018). Deep contextualized word representations. Conference of the North American Chapter of the Association for Computational Linguistics. &lt;a href=&quot;#fnref:elmo&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bert&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Devlin, J.,  Chang, M., Lee, K., Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805. &lt;a href=&quot;#fnref:bert&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:roberta&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692. &lt;a href=&quot;#fnref:roberta&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:t5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Raffel, C., Shazeer, N., Roberts, A., Lee, K, Narang, S, Matena, M., Zhou, Y., Li, W, Liu, P. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research. &lt;a href=&quot;#fnref:t5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:xmlr&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., Grave, E., Ott, M., Zettlemoyer, L., Stoyanov, V. (2020). Unsupervised Cross-lingual Representation Learning at Scale. Annual Meeting of the Association for Computational Linguistics. &lt;a href=&quot;#fnref:xmlr&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gpt3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Brown et al. (2020). Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165 &lt;a href=&quot;#fnref:gpt3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:imagenet&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Deng, J., Dong, W., Socher, R., Li, L., Li, K, Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. IEEE International Conference on Computer Vision and Pattern Recognition. &lt;a href=&quot;#fnref:imagenet&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:clip&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. arXiv preprint arXiv:2103.00020. &lt;a href=&quot;#fnref:clip&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:florence&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yuan, L. et al. (2021). Florence: A New Foundation Model for Computer Vision. arXiv preprint arXiv:2111.11432. &lt;a href=&quot;#fnref:florence&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:scaling&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Cabi, S. et al. (2020). Scaling data-driven robotics with reward sketching and batch reinforcement learning. Robotics Science and Systems. &lt;a href=&quot;#fnref:scaling&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:embr&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Wu, B., Nair, S., Fei-Fei, L., Finn, C. (2021). Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks. Conference on Robot Learning. &lt;a href=&quot;#fnref:embr&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:distilbert&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Sanh, V., Debut, L., Chaumond, J., Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. Neural Information Processing Systems. &lt;a href=&quot;#fnref:distilbert&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:vf&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Finn, C., Levine, S. (2017). Deep Visual Foresight for Planning Robot Motion. IEEE International Conference on Robotics and Automation. &lt;a href=&quot;#fnref:vf&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sthsth&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Goyal, R. et al. (2017). The “something something” video database for learning and evaluating visual common sense. International Conference on Computer Vision. &lt;a href=&quot;#fnref:sthsth&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Fri, 21 Jan 2022 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>BanditPAM: Almost Linear-Time k-medoids Clustering via Multi-Armed Bandits</title>
              <link>/blog/banditpam/</link>
              <guid isPermaLink="true">/blog/banditpam/</guid>
              <description>&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2021-12-17-banditpam/feature.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;tldr&quot;&gt;TL;DR&lt;/h1&gt;

&lt;p&gt;Want something better than \(k\)-means? Our state-of-the-art \(k\)-medoids algorithm from NeurIPS, BanditPAM, is now publicly available! \(\texttt{pip install banditpam}\) and you're good to go!&lt;/p&gt;

&lt;p&gt;Like the \(k\)-means problem, the \(k\)-medoids problem is a clustering problem in which our objective is to partition a dataset into disjoint subsets. In \(k\)-medoids, however, we require that the cluster centers must be actual datapoints, which permits greater interpretability of the cluster centers. \(k\)-medoids also works better with arbitrary distance metrics, so your clustering can be more robust to outliers if you're using metrics like \(L_1\).&lt;/p&gt;

&lt;p&gt;Despite these advantages, most people don't use \(k\)-medoids because prior algorithms were too slow. In our NeurIPS paper, BanditPAM, we sped up the best known algorithm from \(O(n^2)\) to \(O(n\text{log}n)\).&lt;/p&gt;

&lt;p&gt;We've released our implementation, which is pip-installable. It's written in C++ for speed and supports parallelization and intelligent caching, at no extra complexity to end users. Its interface also matches the \(\texttt{sklearn.cluster.KMeans}\) interface, so minimal changes are necessary to existing code.&lt;/p&gt;

&lt;h4 id=&quot;useful-links&quot;&gt;&lt;strong&gt;Useful Links:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://crossminds.ai/video/bandit-pam-almost-linear-time-k-medoids-clustering-via-multi-armed-bandits-5fb88782b0a3f6412973b646/&quot;&gt;3-minute video summary&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://pypi.org/project/banditpam/&quot;&gt;PyPI&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ThrunGroup/BanditPAM&quot;&gt;Github Repository&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/73b817090081cef1bca77232f4532c5d-Paper.pdf&quot;&gt;Full Paper&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;k-means-vs-k-medoids&quot;&gt;\(k\)-means vs. \(k\)-medoids&lt;/h1&gt;

&lt;p&gt;If you're an ML practitioner, you're probably familiar with the \(k\)-means problem. In fact, you may know some of the common algorithms for the \(k\)-means problem. You're much less likely, however, familiar with the \(k\)-&lt;em&gt;medoids&lt;/em&gt; problem.&lt;/p&gt;

&lt;p&gt;The \(k\)-medoids problem is a clustering problem similar to \(k\)-means. Given a dataset, we want to partition our dataset into subsets where the points in each cluster are closer to a single cluster center than all other \(k-1\) cluster centers. Unlike in \(k\)-means, however, the \(k\)-medoids problem requires cluster centers to be &lt;em&gt;actual datapoints&lt;/em&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2021-12-17-banditpam/image3.png&quot; style=&quot;border:black; border-style:solid&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2021-12-17-banditpam/image7.png&quot; style=&quot;border:black; border-style:solid&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; The \(k\)-medoids solution (left) forces the cluster centers to be actual datapoints. This solution is often different from the \(k\)-means solution (right).&lt;/p&gt;

&lt;p&gt;The \(k\)-medoids problem has several advantages over \(k\)-means. By forcing the cluster centers -- dubbed the &lt;em&gt;medoids&lt;/em&gt; -- to be actual datapoints, solutions tend to be more interpretable since you can determine exactly which datapoint is the cluster center for each cluster. When clustering images from the ImageNet dataset, for example, the mean of a solution to the \(k\)-means problem with \(k = 1\) is usually a nondescript blob (Figure 2, left), whereas the medoid of a corresponding solution to the \(k\)-medoids problem is an actual image (Figure 2, right).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2021-12-17-banditpam/image4.png&quot; style=&quot;width:35%; border:black; border-style:solid&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2021-12-17-banditpam/image2.png&quot; style=&quot;width:35%; border:black; border-style:solid&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; The cluster centers in \(k\)-means are often not easily interpretable, whereas they are actual datapoints in \(k\)-medoids. Shown are cluster centers for a subset of ImageNet with \(k = 1\) with \(k\)-means (left) and \(k\)-medoids (right). The mean of the dataset is the average per-pixel color, whereas the medoid is an image of a bee.&lt;/p&gt;

&lt;p&gt;The \(k\)-medoids problem also supports arbitrary distance metrics, in contrast with \(k\)-means which usually requires \(L_2\) for efficiency. In fact, you're allowed to use &lt;em&gt;any pairwise dissimilarity function&lt;/em&gt; with \(k\)-medoids -- your dissimilarity function need not even satisfy the properties of a metric. It can be asymmetric, negative, and violate the triangle inequality. In practice, allowing for arbitrary dissimilarity metrics enables the clustering of &quot;exotic&quot; objects like strings, natural language, trees, graphs, and more -- &lt;em&gt;without&lt;/em&gt; needing to embed these objects in a vector space first.&lt;/p&gt;

&lt;p&gt;The advantages of \(k\)-medoids don't stop there. Because the \(k\)-medoids problem supports arbitrary distance functions, the clustering can often be more robust to outliers if you're using robust distance metrics. The \(L_1\) metric, for example, is more robust to outliers than the \(L_2\) metric; in one dimension, the \(L_1\) minimizer is the median of your datapoints whereas the \(L_2\) minimizer is the mean.&lt;/p&gt;

&lt;p&gt;Despite all of these advantages, \(k\)-means is much more widely used than \(k\)-medoids, largely due to its much more favorable runtime. The best \(k\)-means algorithms scale linearly in dataset size, i.e., have \(O(n)\) complexity, whereas, until now, the best \(k\)-medoids algorithms scaled quadratically in dataset size, i.e., had \(O(n^2)\) complexity.&lt;/p&gt;

&lt;p&gt;In our NeurIPS paper, BanditPAM, we reduced the complexity of the best known \(k\)-medoids algorithm from \(O(n^2)\) to \(O(n\text{log}n)\). This complexity almost matches the complexity of standard \(k\)-means algorithms -- and now, you get all the benefits of \(k\)-medoids on top. We've also released a high-performance implementation of our algorithm written in C++ for speed but callable from Python via python bindings; \(\texttt{pip install banditpam}\) and you're ready to go! Our algorithm's interface matches that of \(\texttt{sklearn.cluster.KMeans}\) and can be used with a simple 2-line change. You can also implement your own distance metrics, interpret cluster centers, and cluster structured data!&lt;/p&gt;

&lt;h1 id=&quot;banditpam-almost-linear-time-k-medoids-clustering-via-multi-armed-bandits&quot;&gt;BanditPAM: Almost Linear Time \(k\)-medoids Clustering via Multi-Armed Bandits&lt;/h1&gt;

&lt;p&gt;How does our algorithm, BanditPAM, work? Our claim is that we &lt;em&gt;match&lt;/em&gt; the prior state-of-the-art solutions in clustering quality by recovering the exact same solution and reduce the complexity from \(O(n^2)\) to \(O(n\text{log}n)\). But is this reduction in complexity just &quot;for free&quot;?&lt;/p&gt;

&lt;p&gt;To discuss BanditPAM, we first need to discuss its predecessor, the Partitioning Around Medoids (PAM) algorithm. The PAM algorithm, first proposed in 1990&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, is a greedy solution to the \(k\)-medoids problem. PAM is broken into two steps: the BUILD step and the SWAP step.&lt;/p&gt;

&lt;p&gt;In the BUILD step, each of the \(k\) medoids is greedily initialized one by one. More concretely, PAM considers all possible datapoints as &quot;candidate&quot; medoids. For every candidate medoid, we compute the change in the overall loss if we were to add that candidate to the set of medoids, conditioned on the previously assigned medoids being fixed. This results in \(O(n^2)\) computational complexity since we need to compute every pairwise distance.&lt;/p&gt;

&lt;p&gt;In the SWAP step, we consider all \(kn\) (medoid, non-medoid) pairs and the change in loss that would be induced if we were to swap the first element of the pair out of the medoid set in favor of the second. Again, this procedure incurs an \(O(n^2)\) time complexity (really \(O(kn^2))\).&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;https://imgur.com/Iy7YN8E.gif&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; The \(k\)-medoids algorithm in action. In the BUILD step, each medoid is assigned greedily, one by one. In the SWAP step, we consider swapping medoid assignments to see if we can lower the overall loss.&lt;/p&gt;

&lt;p&gt;Our fundamental insight was that for each step of the PAM algorithm, we don't actually need to compute the distance from each point to &lt;em&gt;all&lt;/em&gt; other \(n\) points. Instead, we can just sample these distances!&lt;/p&gt;

&lt;p&gt;Consider, for example, the problem of assigning the first medoid at the beginning of the BUILD step. PAM would go through all \(n\) points and, for each point, compute its distance to every other point. We realized that for each candidate, we only needed to compute the distance to \(O(\text{log}n)\) other points. By intelligently choosing which distances to compute, we can save a lot of unnecessary computation. Formally, we reduce the problem of assigning the first medoid to a multi-armed bandit problem, as demonstrated in Figure 4. In multi-armed bandit problems, our objective is to identify the best action to take -- also referred to as the best arm to pull -- when actions are independent and have stochastic returns.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2021-12-17-banditpam/image5.png&quot; style=&quot;border:black; border-style:solid&quot; /&gt;
&lt;img class=&quot;postimagehalf&quot; src=&quot;/blog/assets/img/posts/2021-12-17-banditpam/image6.png&quot; style=&quot;width:47.8%; border:black; border-style:solid&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; PAM (left) computes every pairwise distance for each candidate medoid. BanditPAM (right) only &lt;em&gt;samples&lt;/em&gt; the pairwise distances. With just a few samples, we see that the purple point is a better candidate than the green point since the purple arrows are, on average, shorter than the green ones.&lt;/p&gt;

&lt;p&gt;It turns out that &lt;em&gt;all&lt;/em&gt; steps of the PAM algorithm can also be reduced to multi-armed bandit problems. In each part of the BUILD step, we still view each candidate datapoint as an arm. Now, however, pulling an arm corresponds to computing the induced change in loss for a random datapoint if we were to add the candidate to the set of medoids, conditioned on the previous medoids already being assigned. In each SWAP step, we view each (medoid, non-medoid) pair as an arm and pulling an arm corresponds to computing the induced change in loss on a random datapoint if we were to perform the swap. With these modifications, the original PAM algorithm is now reformulated as a sequence of best-arm identification problems. This reformulation reduces every step of the PAM algorithm from \(O(n^2)\) to \(O(nlogn)\).&lt;/p&gt;

&lt;p&gt;Now, if you're familiar with multi-armed bandits, you might protest. Our algorithm is a randomized algorithm and can sometimes return an incorrect result. In the full paper, we show that the probability of getting a &quot;wrong&quot; answer is very small. In practice, this means that users of our algorithm don't have to worry and will almost always get the same answer as the original PAM algorithm.&lt;/p&gt;

&lt;p&gt;The BanditPAM algorithm is an \(O(n\text{log}n)\) algorithm that matches prior state-of-the-art algorithms in clustering quality and almost matches the complexity of popular \(k\)-means algorithms. Want to try out BanditPAM? Run \(\texttt{pip3 install banditpam}\) and jump to our &lt;a href=&quot;https://github.com/ThrunGroup/BanditPAM\#example-1-synthetic-data-from-a-gaussian-mixture-model&quot;&gt;examples&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage&quot; src=&quot;/blog/assets/img/posts/2021-12-17-banditpam/image1.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; A formal proof that \(k\)-medoids is superior to \(k\)-means in every way.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgments&quot;&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This blog post is based on the paper: &lt;a href=&quot;https://arxiv.org/abs/2006.06856&quot;&gt;BanditPAM: Almost Linear Time \(k\)-medoids Clustering via Multi-Armed Bandits&lt;/a&gt;. NeurIPS 2021.&lt;/p&gt;

&lt;p&gt;A special thanks to my collaborators on this project, Martin Jinye Zhang, James Mayclin, Sebastian Thrun, Chris Piech, and Ilan Shomorony, as well as the reviewers of this blog post, Drew A. Hudson and Sidd Karamcheti.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kaufman, Leonard; Rousseeuw, Peter J. (1990-03-08), &quot;Partitioning Around Medoids (Program PAM)&quot;, Wiley Series in Probability and Statistics, Hoboken, NJ, USA: John Wiley &amp;amp; Sons, Inc., pp. 68–125] &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Fri, 17 Dec 2021 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers and Talks at NeurIPS 2021</title>
              <link>/blog/neurips-2021/</link>
              <guid isPermaLink="true">/blog/neurips-2021/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://neurips.cc&quot;&gt;thirty-fifth Conference on Neural Information Processing Systems&lt;/a&gt; (NeurIPS) 2021 is being hosted virtually from Dec 6th - 14th. We’re excited to share all the work from SAIL that’s being presented at the &lt;a href=&quot;#main-conference&quot;&gt;&lt;strong&gt;main conference&lt;/strong&gt;&lt;/a&gt;, at the &lt;a href=&quot;#datasets-and-benchmarks-track&quot;&gt;&lt;strong&gt;Datasets and Benchmarks track&lt;/strong&gt;&lt;/a&gt; and the various &lt;a href=&quot;#workshop-papers&quot;&gt;&lt;strong&gt;workshops&lt;/strong&gt;&lt;/a&gt;, and you’ll find links to papers, videos and blogs below.&lt;/p&gt;

&lt;p&gt;Some of the members in our SAIL community also serve as &lt;a href=&quot;#workshops&quot;&gt;&lt;strong&gt;co-organizers of several exciting workshops&lt;/strong&gt;&lt;/a&gt; that will take place on Dec 13-14, so we hope you will check them out!&lt;/p&gt;

&lt;p&gt;Feel free to reach out to the contact authors and the workshop organizers directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;h2 id=&quot;main-conference&quot;&gt;Main Conference&lt;/h2&gt;
&lt;h4 id=&quot;improving-compositionality-of-neural-networks-by-decoding-representations-to-inputs&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.00769&quot;&gt;Improving Compositionality of Neural Networks by Decoding Representations to Inputs&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img0.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Mike Wu, Noah Goodman, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: wumike@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.00769&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: generative models, compositionality, decoder&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;reverse-engineering-recurrent-neural-networks-with-jacobian-switching-linear-dynamical-systems&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.01256&quot;&gt;Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img41.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jimmy T.H. Smith, Scott W. Linderman, David Sussillo
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jsmith14@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2111.01256&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/jimmysmith1919/JSLDS_public&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: recurrent neural networks, switching linear dynamical systems, interpretability, fixed points&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;compositional-transformers-for-scene-generation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.08960&quot;&gt;Compositional Transformers for Scene Generation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/overview.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Drew A. Hudson, C. Lawrence Zitnick
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: dorarad@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2111.08960&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/dorarad/gansformer&quot;&gt;Github&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: GANs, transformers, compositionality, scene synthesis&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;combining-recurrent-convolutional-and-continuous-time-models-with-linear-state-space-layers&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.13985&quot;&gt;Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img2.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, Chris Ré
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: albertgu@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.13985&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: recurrent neural networks, rnn, continuous models, state space, long range dependencies, sequence modeling&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;emergent-communication-of-generalizations&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.02668&quot;&gt;Emergent Communication of Generalizations&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img3.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jesse Mu, Noah Goodman
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: muj@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.02668&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=LVW_t7p42X0&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: emergent communication, multi-agent communication, language grounding, compositionality&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;deep-learning-on-a-data-diet-finding-important-examples-early-in-training&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.07075&quot;&gt;Deep Learning on a Data Diet: Finding Important Examples Early in Training&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img123.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Mansheej Paul, Surya Ganguli, Gintare Karolina Dziugaite
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: mansheej@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.07075&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: data pruning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;ella-exploration-through-learned-language-abstraction&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2103.05825&quot;&gt;ELLA: Exploration through Learned Language Abstraction&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img4.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Suvir Mirchandani, Siddharth Karamcheti, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: suvir@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2103.05825&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/7iDeF5eiyIA&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: instruction following, reward shaping, reinforcement learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;csdi-conditional-score-based-diffusion-models-for-probabilistic-time-series-imputation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.03502&quot;&gt;CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img5.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yusuke Tashiro, Jiaming Song, Yang Song, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ytashiro@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.03502&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/ermongroup/CSDI&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: score-based generative modeling, time series imputation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;confidence-aware-imitation-learning-from-demonstrations-with-varying-optimality&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2110.14754.pdf&quot;&gt;Confidence-Aware Imitation Learning from Demonstrations with Varying Optimality&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img6.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Songyuan Zhang, Zhangjie Cao, Dorsa Sadigh, Yanan Sui
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: szhang21@mit.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2110.14754.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=Qe_Ov65-M0U&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/cail&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: imitation learning, learning from demonstration, learning from suboptimal demonstrations&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;explaining-heterogeneity-in-medial-entorhinal-cortex-with-task-driven-neural-networks&quot;&gt;&lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2021.10.30.466617&quot;&gt;Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img7.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Aran Nayebi, Alexander Attinger, Malcolm G. Campbell, Kiah Hardcastle, Isabel I.C. Low, Caitlin S. Mallory, Gabriel C. Mel, Ben Sorscher, Alex H. Williams, Surya Ganguli, Lisa M. Giocomo, Daniel L.K. Yamins
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: anayebi@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Spotlight Presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2021.10.30.466617&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/neuroailab/mec&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: neural coding, medial entorhinal cortex, grid cells, biologically-inspired navigation, path integration, recurrent neural networks&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;on-the-theory-of-reinforcement-learning-with-once-per-episode-feedback&quot;&gt;On the theory of reinforcement learning with once-per-episode feedback&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img8.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Niladri Chatterji, Aldo Pacchiano, Peter Bartlett, Michael Jordan
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: niladri@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: theoretical reinforcement learning, binary rewards, non-markovian rewards&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;hyperspns-compact-and-expressive-probabilistic-circuits&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=31NfehDva-h&quot;&gt;HyperSPNs: Compact and Expressive Probabilistic Circuits&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img9.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Andy Shih, Dorsa Sadigh, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: andyshih@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/pdf?id=31NfehDva-h&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=8W48CqNELCM&amp;amp;ab_channel=StanfordILIAD&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://github.com/ermongroup/HyperSPN&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: generative models, tractable probabilistic models, sum product networks, probabilistic circuits&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;combo-conservative-offline-model-based-policy-optimization&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2102.08363.pdf&quot;&gt;COMBO: Conservative Offline Model-Based Policy Optimization&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img10.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Tianhe Yu*, Aviral Kumar*, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: tianheyu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2102.08363.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: offline reinforcement learning, model-based reinforcement learning, deep reinforcement learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;conservative-data-sharing-for-multi-task-offline-reinforcement-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2109.08128.pdf&quot;&gt;Conservative Data Sharing for Multi-Task Offline Reinforcement Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img11.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Tianhe Yu*, Aviral Kumar*, Yevgen Chebotar, Karol Hausman, Sergey Levine, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: tianheyu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2109.08128.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: offline reinforcement learning, multi-task reinforcement learning, deep reinforcement learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;autonomous-reinforcement-learning-via-subgoal-curricula&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.12931&quot;&gt;Autonomous Reinforcement Learning via Subgoal Curricula&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img12.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Archit Sharma, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: architsh@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.12931&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/vaprl/home&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, curriculum, autonomous learning, reset-free reinforcement learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;lossy-compression-for-lossless-prediction-&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.10800&quot;&gt;Lossy Compression for Lossless Prediction &lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img13.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yann Dubois, Benjamin Bloem-Reddy, Karen Ullrich Chris J. Maddison
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: yanndubs@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Spotlight Presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.10800&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://slideslive.at/38955214/lossy-compression-for-lossless-prediction?ref=search&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://github.com/YannDubs/lossyless&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: compression, invariances, information theory, machine learning, self-supervised learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;capturing-implicit-hierarchical-structure-in-3d-biomedical-images-with-self-supervised-hyperbolic-representations&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.01644&quot;&gt;Capturing implicit hierarchical structure in 3D biomedical images with self-supervised hyperbolic representations&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img14.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Joy Hsu, Jeffrey Gu, Gong-Her Wu, Wah Chiu, Serena Yeung
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: joycj@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2012.01644&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: hyperbolic representations, hierarchical structure, biomedical&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;estimating-high-order-gradients-of-the-data-distribution-by-denoising&quot;&gt;Estimating High Order Gradients of the Data Distribution by Denoising&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img15.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Chenlin Meng, Yang Song, Wenzhe Li, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: chenlin@stanford.edu
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: score matching, langevin dynamics, denoising, generative modeling&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;universal-off-policy-evaluation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.12820&quot;&gt;Universal Off-Policy Evaluation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img16.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yash Chandak, Scott Niekum, Bruno Castro da Silva, Erik Learned-Miller, Emma Brunskill, Philip Thomas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ychandak@cs.umass.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2104.12820&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/yashchandak/UnO&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: metrics, risk, distribution, cdf, off-policy evaluation, ope, reinforcement learning, counterfactuals, high-confidence bounds, confidence intervals&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;evidential-softmax-for-sparse-multimodal-distributions-in-deep-generative-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.14182&quot;&gt;Evidential Softmax for Sparse Multimodal Distributions in Deep Generative Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img17.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Phil Chen, Masha Itkina, Ransalu Senanayake, Mykel J. Kochenderfer
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: philhc@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.14182&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: deep learning or neural networks, sparsity and feature selection, variational inference, (application) natural language and text processing&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;provable-guarantees-for-self-supervised-deep-learning-with-spectral-contrastive-loss&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.04156&quot;&gt;Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img18.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jeff Z. HaoChen, Colin Wei, Adrien Gaidon, Tengyu Ma
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jhaochen@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.04156&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: deep learning theory, unsupervised learning theory, representation learning theory&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;provable-model-based-nonlinear-bandit-and-reinforcement-learning-shelve-optimism-embrace-virtual-curvature&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2102.04168&quot;&gt;Provable Model-based Nonlinear Bandit and Reinforcement Learning: Shelve Optimism, Embrace Virtual Curvature&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img19.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kefan Dong, Jiaqi Yang, Tengyu Ma
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kefandong@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2102.04168&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/rlug_YXt5yo&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: nonlinear bandits, online learning, deep reinforcement learning theory, sequential rademacher complexity&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;decrypting-cryptic-crosswords-semantically-complex-wordplay-puzzles-as-a-target-for-nlp&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.08620&quot;&gt;Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img20.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Joshua Rozner, Christopher Potts, Kyle Mahowald
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: rozner@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2104.08620&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/jsrozner/decrypt&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: compositionality in language, curriculum learning, meta-linguistics, systematicity, generalization&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;design-of-experiments-for-stochastic-contextual-linear-bandits&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.09912&quot;&gt;Design of Experiments for Stochastic Contextual Linear Bandits&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img21.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Andrea Zanette*, Kefan Dong*, Jonathan Lee*, Emma Brunskill
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: zanette@berkeley.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.09912&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: linear bandits, design of experiments&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;provable-benefits-of-actor-critic-methods-for-offline-reinforcement-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.08812&quot;&gt;Provable Benefits of Actor-Critic Methods for Offline Reinforcement Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img22.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Andrea Zanette, Martin J. Wainwright, Emma Brunskill
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: zanette@berkeley.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2108.08812&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: offline rl, mirror descent, bellman closure&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;a-topological-perspective-on-causal-inference&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.08558&quot;&gt;A Topological Perspective on Causal Inference&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img23.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Duligur Ibeling, Thomas Icard
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: icard@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.08558&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: causal inference, topological learning theory&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;adversarial-training-helps-transfer-learning-via-better-representations&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.10189&quot;&gt;Adversarial Training Helps Transfer Learning via Better Representations&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img24.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Zhun Deng, Linjun Zhang, Kailas Vodrahalli, Kenji Kawaguchi, James Zou
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jamesyzou@gmail.com
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.10189&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: transfer learning, adversarial training&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;widening-the-pipeline-in-human-guided-reinforcement-learning-with-explanation-and-context-aware-data-augmentation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.14804&quot;&gt;Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img25.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Lin Guan,Mudit Verma,Sihang Guo,Ruohan Zhang,Subbarao Kambhampati
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: zharu@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Spotlight
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2006.14804&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://yochan-lab.github.io/paper_webpages/expand/index.html&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: human-in-the-loop reinforcement learning, evaluative feedback, saliency map, visual explanation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;machine-versus-human-attention-in-deep-reinforcement-learning-tasks&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.15942.pdf&quot;&gt;Machine versus Human Attention in Deep Reinforcement Learning Tasks&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img26.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Sihang Guo, Ruohan Zhang, Bo Liu, Yifeng Zhu, Dana Ballard, Mary Hayhoe, Peter Stone
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: zharu@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2010.15942.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: deep reinforcement learning, interpretability, attention, eye tracking&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;play-to-grade-testing-coding-games-as-classifying-markov-decision-process&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.14615&quot;&gt;Play to Grade: Testing Coding Games as Classifying Markov Decision Process&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img27.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Allen Nie, Emma Brunskill, Chris Piech
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: anie@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.14615&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/windweller/play-to-grade&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, computational education, collaborative training, markov decision process&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;the-value-of-information-when-deciding-what-to-learn&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.13973&quot;&gt;The Value of Information When Deciding What to Learn&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img1.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Dilip Arumugam, Benjamin Van Roy
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: dilip@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.13973&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: exploration, information theory, multi-armed bandits, reinforcement learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;diversity-matters-when-learning-from-ensembles&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.14149&quot;&gt;Diversity Matters When Learning From Ensembles&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img28.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Giung Nam*, Jongmin Yoon*, Yoonho Lee, Juho Lee
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: yoonho@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.14149&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/cs-giung/giung2/tree/main/projects/Diversity-Matters&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: deep ensembles, knowledge distillation, calibration, output diversified sampling, batchensemble&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;reinforcement-learning-with-state-observation-costs-in-action-contingent-noiselessly-observable-markov-decision-processes&quot;&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/83e8fe6279ad25f15b23c6298c6a3584-Abstract.html&quot;&gt;Reinforcement Learning with State Observation Costs in Action-Contingent Noiselessly Observable Markov Decision Processes&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img29.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: HyunJi Nam, Scott Fleming, Emma Brunskill
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: scottyf@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/hash/83e8fe6279ad25f15b23c6298c6a3584-Abstract.html&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/nam630/acno_mdp&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, observation cost, markov decision process, mdp, partially observable markov decision process, pomdp, probably approximately correct, pac, healthcare, health care&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;meta-learning-with-an-adaptive-task-scheduler&quot;&gt;&lt;a href=&quot;https://openreview.net/forum?id=MTs2adH_Qq&quot;&gt;Meta-learning with an Adaptive Task Scheduler&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img30.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Huaxiu Yao, Yu Wang, Ying Wei, Peilin Zhao, Mehrdad Mahdavi, Defu Lian, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: huaxiu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/forum?id=MTs2adH_Qq&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: adaptive task scheduler, meta-learning, sampling&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;spatial-temporal-super-resolution-of-satellite-imagery-via-conditional-pixel-synthesis&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=IKz9uYkf3vZ&quot;&gt;Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img31.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yutong He, Dingjie Wang, Nicholas Lai, William Zhang, Chenlin Meng, Marshall Burke, David B. Lobell, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kellyyhe@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/pdf?id=IKz9uYkf3vZ&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://recorder-v3.slideslive.com/?share=52340&amp;amp;s=f403374f-9c27-4fab-91d1-27daf7f78084&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://satellite-pixel-synthesis.github.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: remote sensing, super-resolution, generative models&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;scatterbrain-unifying-sparse-and-low-rank-attention&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.15343&quot;&gt;Scatterbrain: Unifying Sparse and Low-rank Attention&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img32.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Beidi Chen*, Tri Dao*, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré.
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: trid@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.15343&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: efficient attention, sparse, low-rank&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;bcd-nets-scalable-variational-approaches-for-bayesian-causal-discovery&quot;&gt;BCD Nets: Scalable Variational Approaches for Bayesian Causal Discovery&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img33.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Chris Cundy, Aditya Grover, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: cundy@stanford.edu
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: causal inference, variational inference&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;calibrating-predictions-to-decisions-a-novel-approach-to-multi-class-calibration&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.05719&quot;&gt;Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img34.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Shengjia Zhao, Michael P Kim, Roshni Sahoo, Tengyu Ma, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: sjzhao@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.05719&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: calibration, decision making under uncertainty&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;beyond-pinball-loss-quantile-methods-for-calibrated-uncertainty-quantification&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2011.09588&quot;&gt;Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty Quantification&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img35.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Youngseog Chung, Willie Neiswanger, Ian Char, Jeff Schneider
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: youngsec@andrew.cmu.edu, neiswanger@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2011.09588&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/YoungseogChung/calibrated-quantile-uq, https://github.com/uncertainty-toolbox/uncertainty-toolbox&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: uncertainty quantification, uq, quantile regression, pinball loss&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;causal-abstractions-of-neural-networks&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.02997&quot;&gt;Causal Abstractions of Neural Networks&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img36.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Atticus Geiger*, Hanson Lu*, Thomas Icard, Christopher Potts
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: atticusg@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.02997&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: interpretability, analysis, nlp, causality&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;generalized-shape-metrics-on-neural-representations&quot;&gt;Generalized Shape Metrics on Neural Representations&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img37.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Alex H Williams,  Erin Kunz, Simon Kornblith, Scott Linderman
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: alex.h.willia@gmail.com
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: representational similarity analysis, neural representations, shape analysis, metric space&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;d2c-diffusion-denoising-models-for-few-shot-conditional-generation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.06819&quot;&gt;D2C: Diffusion-Denoising Models for Few-shot Conditional Generation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img38.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Abhishek Sinha*, Jiaming Song*, Chenlin Meng, Stefano Ermon 
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: tsong@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.06819&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://d2c-model.github.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: generative modeling, contrastive learning, conditional generation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;combiner-full-attention-transformer-with-sparse-computation-cost&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.05768&quot;&gt;Combiner: Full Attention Transformer with Sparse COmputation Cost&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img39.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, Bo Dai
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: hyren@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.05768&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: efficient transformer&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;maximum-likelihood-training-of-score-based-diffusion-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2101.09258&quot;&gt;Maximum Likelihood Training of Score-Based Diffusion Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img40.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yang Song, Conor Durkan, Iain Murray, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: yangsong@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Spotlight presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2101.09258&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: score-based generative models, denoising score matching, diffusion models, maximum likelihood training&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;contrastive-reinforcement-learning-of-symbolic-reasoning-domains&quot;&gt;Contrastive Reinforcement Learning of Symbolic Reasoning Domains&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img42.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Gabriel Poesia, WenXin Dong, Noah Goodman
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: poesia@stanford.edu
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, education, contrastive learning, symbolic reasoning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;equivariant-manifold-flows&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.08596&quot;&gt;Equivariant Manifold Flows&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img43.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Isay Katsman, Aaron Lou, Derek Lim, Qingxuan Jiang, Ser Nam Lim, Christopher M. De Sa
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: aaronlou@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.08596&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/CUAI/Equivariant-Manifold-Flows&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: manifold, normalizing flow, equivariant, invariant&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;lower-bounds-on-metropolized-sampling-methods-for-well-conditioned-distributions&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.05480&quot;&gt;Lower Bounds on Metropolized Sampling Methods for Well-Conditioned Distributions&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img44.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yin Tat Lee, Ruoqi Shen, Kevin Tian
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kjtian@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.05480&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=u57jdge-uEw&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: sampling, lower bounds, langevin dynamics, hamiltonian monte carlo&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;list-decodable-mean-estimation-in-nearly-pca-time&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2011.09973&quot;&gt;List-Decodable Mean Estimation in Nearly-PCA Time&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img45.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ilias Diakonikolas, Daniel M. Kane, Daniel Kongsgaard, Jerry Li, Kevin Tian
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kjtian@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Spotlight presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2011.09973&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: robust statistics, semidefinite programming, mixture models&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;robust-regression-revisited-acceleration-and-improved-estimation-rates&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.11938&quot;&gt;Robust Regression Revisited: Acceleration and Improved Estimation Rates&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img46.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Arun Jambulapati, Jerry Li, Tselil Schramm, Kevin Tian
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kjtian@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.11938&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: robust statistics, regression, generalized linear models, acceleration, sum of squares methods&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-with-user-level-privacy&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2102.11845&quot;&gt;Learning with User-Level Privacy&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img47.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Daniel Levy*, Ziteng Sun*, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, Ananda Theertha Suresh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: danilevy@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2102.11845&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: differential privacy user-level&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;adapting-to-function-difficulty-and-growth-conditions-in-private-optimization&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.02391&quot;&gt;Adapting to Function Difficulty and Growth Conditions in Private Optimization&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img48.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Hilal Asi*, Daniel Levy*, John C. Duchi
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: asi@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2108.02391&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: differential privacy adaptivity optimization&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;imitation-with-neural-density-models&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.09808&quot;&gt;Imitation with Neural Density Models&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img49.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kuno Kim, Akshat Jindal, Yang Song, Jiaming Song, Yanan Sui, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: khkim@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.09808&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: rl; imitation learning; density estimation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;why-do-pretrained-language-models-help-in-downstream-tasks-an-analysis-of-head-and-prompt-tuning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.09226&quot;&gt;Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img50.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Colin Wei, Sang Michael Xie, Tengyu Ma
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: colinwei@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.09226&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: nlp pretraining, theoretical analysis&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;safe-reinforcement-learning-by-imagining-the-near-future&quot;&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/73b277c11266681122132d024f53a75b-Paper.pdf&quot;&gt;Safe Reinforcement Learning by Imagining the Near Future&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img51.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Garrett Thomas, Yuping Luo, Tengyu Ma
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: gwthomas@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/73b277c11266681122132d024f53a75b-Paper.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: safe exploration, model-based rl&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;pseudo-spherical-contrastive-divergence&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2111.00780.pdf&quot;&gt;Pseudo-Spherical Contrastive Divergence&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img52.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Lantao Yu, Jiaming Song, Yang Song, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: lantaoyu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2111.00780.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: deep generative models, energy-based models, proper scoring rules&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;iq-learn-inverse-soft-q-learning-for-imitation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.12142&quot;&gt;IQ-Learn: Inverse soft-Q Learning for Imitation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img53.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: divgarg@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Spotlight
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.12142&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://div99.github.io/IQ-Learn&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reinforcement learning, imitation learning, inverse reinforcement learning, statistical learning, energy-based models&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;intrinsic-dimension-persistent-homology-and-generalization-in-neural-networks-&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.13171&quot;&gt;Intrinsic Dimension, Persistent Homology and Generalization in Neural Networks &lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img54.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Tolga Birdal ~Tolga_Birdal3 , Aaron Lou, Leonidas Guibas, Umut Simsekli
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: aaronlou@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2111.13171&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/tolgabirdal/PHDimGeneralization&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: generalization, persistent homology, intrinsic dimension, deep networks&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;baleen-robust-multi-hop-reasoning-at-scale-via-condensed-retrieval&quot;&gt;&lt;a href=&quot;https://openreview.net/forum?id=Ghk0AJ8XtVx&amp;amp;noteId=N81OLQoxLq&quot;&gt;Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/img55.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Omar Khattab, Christopher Potts, Matei Zaharia
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: okhattab@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Spotlight paper
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/forum?id=Ghk0AJ8XtVx&amp;amp;noteId=N81OLQoxLq&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://ai.stanford.edu/blog/retrieval-based-NLP/&quot;&gt;Blog Post&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: neural retrieval, multi-hop question answering, claim verification, reasoning, colbert&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;datasets-and-benchmarks-track&quot;&gt;Datasets and Benchmarks Track&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=Rtquf4Jk0jN&quot;&gt;&lt;strong&gt;ReaSCAN: Compositional Reasoning in Language Grounding&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://reascan.github.io/&quot;&gt;Website&lt;/a&gt; by Zhengxuan Wu*, Elisa Kreiss*, Desmond Ong, Christopher Potts&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.04035&quot;&gt;&lt;strong&gt;ATOM3D: Tasks on Molecules in Three Dimensions&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://www.atom3d.ai/&quot;&gt;Website&lt;/a&gt; by Raphael J.L. Townshend, Martin Vögele, Patricia Suriana, Alexander Derry, Alexander S. Powers, Yianni Laloudakis, Sidhika Balachandar, Bowen Jing, Brandon Anderson, Stephan Eismann, Risi Kondor, Russ B. Altman, Ron O. Dror&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=WcY35wjmCBA&quot;&gt;&lt;strong&gt;Dynamic Environments with Deformable Objects&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=5eA8z80c9Zc&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://github.com/contactrika/dedo&quot;&gt;Website&lt;/a&gt; by Rika Antonova, Peiyang Shi, Hang Yin, Zehang Weng, Danica Kragic&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.04260&quot;&gt;&lt;strong&gt;Personalized Benchmarking with the Ludwig Benchmarking Toolkit&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/hazyresearch/ludwig-benchmarking-toolkit&quot;&gt;Website&lt;/a&gt; by Avanika Narayan, Piero Molino, Karan Goel, Willie Neiswanger, Christopher Ré&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=YDMFgD_qJuA&quot;&gt;&lt;strong&gt;SKM-TEA: A Dataset for Accelerated MRI Reconstruction with Dense Image Labels for Quantitative Clinical Evaluation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/StanfordMIMI/skm-tea&quot;&gt;Website&lt;/a&gt; by Arjun D Desai, Andrew M Schmidt, Elka B Rubin, Christopher M Sandino, Marianne S Black, Valentina Mazzoli, Kathryn J Stevens, Robert Boutin, Christopher Ré, Garry E Gold, Brian A Hargreaves, Akshay S Chaudhari&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=mPducS1MsEK&quot;&gt;&lt;strong&gt;Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning&lt;/strong&gt;&lt;/a&gt; by Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, Ludwig Schmidt&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.12062&quot;&gt;&lt;strong&gt;DABS: a Domain-Agnostic Benchmark for Self-Supervised Learning&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://dabs.stanford.edu&quot;&gt;Website&lt;/a&gt; by Alex Tamkin, Vincent Liu, Rongfei Lu, Daniel Fein, Colin Schultz, Noah Goodman&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=5HR3vCylqD&quot;&gt;&lt;strong&gt;SustainBench: Benchmarks for Monitoring the Sustainable Development Goals with Machine Learning&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/pKOwSV-gWng&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://sustainlab-group.github.io/sustainbench/&quot;&gt;Website&lt;/a&gt; by Christopher Yeh, Chenlin Meng, Sherrie Wang, Anne Driscoll, Erik Rozi, Patrick Liu, Jihyeon Lee, Marshall Burke, David Lobell, Stefano Ermon&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2103.09430.pdf&quot;&gt;&lt;strong&gt;OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://ogb.stanford.edu/docs/lsc/&quot;&gt;Website&lt;/a&gt; by Weihua Hu&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;workshops&quot;&gt;Workshops&lt;/h2&gt;
&lt;p&gt;This year, multiple members of the SAIL community are also involved in great workshops that will take place on Dec 13-14. We hope you’ll check them out!&lt;/p&gt;

&lt;h4 id=&quot;machine-learning-for-structural-biology-workshop-dec-13&quot;&gt;&lt;a href=&quot;https://www.mlsb.io/&quot;&gt;&lt;strong&gt;Machine Learning for Structural Biology Workshop&lt;/strong&gt;&lt;/a&gt; (Dec 13)&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/mlsb.png&quot; /&gt;
&lt;br /&gt; &lt;strong&gt;Organizers&lt;/strong&gt;: Namrata Anand, Bonnie Berger, Wouter Boomsma, Erika DeBenedictis, Stephan Eismann, John Ingraham, Sergey Ovchinnikov, Roshan Rao, Raphael Townshend and Ellen Zhong&lt;/p&gt;

&lt;h4 id=&quot;controllable-generative-modeling-in-language-and-vision-ctrlgen-workshop-dec-13&quot;&gt;&lt;a href=&quot;https://ctrlgenworkshop.github.io/&quot;&gt;&lt;strong&gt;Controllable Generative Modeling in Language and Vision (CtrlGen Workshop)&lt;/strong&gt;&lt;/a&gt; (Dec 13)&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/ctrl.png&quot; /&gt;
&lt;br /&gt; &lt;strong&gt;Organizers&lt;/strong&gt;: Steven Y. Feng, Drew A. Hudson, Anusha Balakrishnan, Varun Gangal, Dongyeop Kang, Tatsunori Hashimoto and Joel Tetreault&lt;/p&gt;

&lt;h4 id=&quot;distshift-workshop-dec-13&quot;&gt;&lt;a href=&quot;https://sites.google.com/view/distshift2021&quot;&gt;&lt;strong&gt;DistShift Workshop&lt;/strong&gt;&lt;/a&gt; (Dec 13)&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/dist.png&quot; /&gt;
&lt;br /&gt; &lt;strong&gt;Organizers&lt;/strong&gt;: Shiori Sagawa, Pang Wei Koh, Fanny Yang, Hongseok Namkoong, Jiashi Feng, Kate Saenko, Percy Liang, Sarah Bird and Sergey Levine&lt;/p&gt;

&lt;h4 id=&quot;data-centric-ai-workshop-dec-14&quot;&gt;&lt;a href=&quot;https://datacentricai.org/&quot;&gt;&lt;strong&gt;Data-centric AI Workshop&lt;/strong&gt;&lt;/a&gt; (Dec 14)&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/data.png&quot; /&gt;
&lt;br /&gt; &lt;strong&gt;Organizers&lt;/strong&gt;: Andrew Ng, Lora Aroyo, Cody Coleman, Greg Diamos, Vijay Janapa Reddi, Joaquin Vanschoren,Carole-Jean Wu and Sharon Zhou&lt;/p&gt;

&lt;h4 id=&quot;physical-reasoning-and-inductive-biases-for-the-real-world-workshop-dec-14&quot;&gt;&lt;a href=&quot;https://physical-reasoning.github.io/&quot;&gt;&lt;strong&gt;Physical Reasoning and Inductive Biases for the Real World Workshop&lt;/strong&gt;&lt;/a&gt; (Dec 14)&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-12-06-neurips-2021/phys.png&quot; /&gt;
&lt;br /&gt; &lt;strong&gt;Organizers&lt;/strong&gt;: Krishna Murthy Jatavallabhula, Rika Antonova, Kevin Smith, Hsiao-Yu (Fish) Tung, Florian Shkurti, Jeannette Bohg and Josh Tenenbaum&lt;/p&gt;

&lt;h2 id=&quot;workshop-papers&quot;&gt;Workshop Papers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;How Does Contrastive Pre-training Connect Disparate Domains?&lt;/strong&gt; by Kendrick Shen*, Robbie Jones*, Ananya Kumar*, Sang Michael Xie*, Percy Liang (&lt;em&gt;DistShift Workshop&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=de1kSNxv5BQ&quot;&gt;&lt;strong&gt;Optimal Representations for Covariate Shifts&lt;/strong&gt;&lt;/a&gt; by Yann Dubois, Yangjun Ruan, Chris J. Maddison (&lt;em&gt;DistShift Workshop&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Correct-N-Contrast: a Contrastive Approach for Improving Robustness to Spurious Correlations&lt;/strong&gt; by Michael Zhang, Nimit S. Sohoni, Hongyang R. Zhang, Chelsea Finn, Christopher Ré (&lt;em&gt;DistShift Workshop&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extending the WILDS Benchmark for Unsupervised Adaptation&lt;/strong&gt; by Shiori Sagawa*, Pang Wei Koh*, Tony Lee*, Irena Gao*, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, Percy Liang (&lt;em&gt;DistShift Workshop&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Calibrated Ensembles: A Simple Way to Mitigate ID-OOD Accuracy Tradeoffs&lt;/strong&gt; by Ananya Kumar, Aditi Raghunathan, Tengyu Ma, Percy Liang (&lt;em&gt;DistShift Workshop&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.03741&quot;&gt;&lt;strong&gt;Sharp Bounds for Federated Averaging (Local SGD) and Continuous Perspective&lt;/strong&gt;&lt;/a&gt; by Margalit Glasgow*, Honglin Yuan*, Tengyu Ma (&lt;em&gt;New Frontiers in Federated Learning&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.03298&quot;&gt;&lt;strong&gt;What Matters in Learning from Offline Human Demonstrations for Robot Manipulation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://ai.stanford.edu/blog/robomimic/&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=qg_IVo4rB8k&amp;amp;ab_channel=AnonymousAnonymous&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://arise-initiative.github.io/robomimic-web/&quot;&gt;Website&lt;/a&gt; by Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Martín-Martín (&lt;em&gt;Offline Reinforcement Learning Workshop&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rylanschaeffer.github.io/content/research/2021_neurips_workshop_metacognition/paper.pdf&quot;&gt;&lt;strong&gt;An Algorithmic Theory of Metacognition in Minds and Machines&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://rylanschaeffer.github.io/content/research/2021_neurips_workshop_metacognition/main.html&quot;&gt;Blog Post&lt;/a&gt; by Rylan Schaeffer (&lt;em&gt;Metacognition in the Age of AI: Challenges and Opportunities&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Beyond Ads: Sequential Decision-Making Algorithms in Public Policy&lt;/strong&gt; by Peter Henderson, Ben Chugg, Brandon Anderson, Daniel E. Ho (&lt;em&gt;Workshop on Causal Inference Challenges in Sequential Decision Making&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tracking Urbanization in Developing Regions withRemote Sensing Spatial-Temporal Super-Resolution&lt;/strong&gt; by Yutong He*, William Zhang*, Chenlin Meng, Marshall Burke, David B. Lobell, Stefano Ermon (&lt;em&gt;Workshop on Machine Learning for the Developing World (ML4D)&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bayesiandeeplearning.org/2021/papers/56.pdf&quot;&gt;&lt;strong&gt;Likelihood-free Density Ratio Acquisition Functions are not Equivalent to Expected Improvements&lt;/strong&gt;&lt;/a&gt; by Jiaming Song, Stefano Ermon (&lt;em&gt;Bayesian Deep Learning Workshop&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exploiting Proximity Search and Easy Examples to Select Rare Events&lt;/strong&gt; by Daniel Kang, Alex Derhacobian, Kaoru Tsuji, Trevor Hebert, Peter Bailis, Tadashi Fukami, Tatsunori Hashimoto, Yi Sun, Matei Zaharia (&lt;em&gt;Data Centric AI workshop&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at NeurIPS 2021!&lt;/p&gt;
</description>
              <pubDate>Mon, 06 Dec 2021 00:00:00 -0800</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers at EMNLP/CoNLL 2021</title>
              <link>/blog/emnlp-2021/</link>
              <guid isPermaLink="true">/blog/emnlp-2021/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2021.emnlp.org/&quot;&gt;The 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021)
&lt;/a&gt; will take place next week, colocated with CoNLL 2021. We’re excited to share all the work from SAIL that will be presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;h2 id=&quot;list-of-accepted-papers&quot;&gt;List of Accepted Papers&lt;/h2&gt;
&lt;h4 id=&quot;calibrate-your-listeners-robust-communication-based-training-for-pragmatic-speakers&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.05422&quot;&gt;Calibrate your listeners! Robust communication-based training for pragmatic speakers&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/img10.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Rose E. Wang, Julia White, Jesse Mu, Noah D. Goodman
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: rewang@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.05422&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=3VKU9ueVTMo&amp;amp;ab_channel=RoseWang&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: language generation, pragmatics, communication-based training, calibration, uncertainty&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;cross-domain-data-integration-for-named-entity-disambiguation-in-biomedical-text&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.08228&quot;&gt;Cross-Domain Data Integration for Named Entity Disambiguation in Biomedical Text&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/img7.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Maya Varma, Laurel Orr, Sen Wu, Megan Leszczynski, Xiao Ling, Christopher Ré
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: mvarma2@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.08228&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/TmXUVqeDmP0&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: named entity disambiguation, biomedical text, rare entities, data integration&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;contractnli-a-dataset-for-document-level-natural-language-inference-for-contracts&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.01799&quot;&gt;ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/img2.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yuta Koreeda, Christopher D. Manning
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: koreeda@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.01799&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://stanfordnlp.github.io/contract-nli/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: natural language inference, contract, law, legal, dataset
&lt;br /&gt;&lt;strong&gt;Venue&lt;/strong&gt;: The Findings of EMNLP 2021&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;the-emergence-of-the-shape-bias-results-from-communicative-efficiency&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.06232&quot;&gt;The Emergence of the Shape Bias Results from Communicative Efficiency&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/img11.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Eva Portelance, Michael C. Frank, Dan Jurafsky, Alessandro Sordoni, Romain Laroche
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: portelan@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2109.06232&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://github.com/evaportelance/emergent-shape-bias&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: emergent communication, shape bias, multi-agent reinforcement learning, language learning, language acquisition
&lt;br /&gt;&lt;strong&gt;Conference&lt;/strong&gt;: CoNLL&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;lm-critic-language-models-for-unsupervised-grammatical-error-correction&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.06822&quot;&gt;LM-Critic: Language Models for Unsupervised Grammatical Error Correction&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/img9.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Michihiro Yasunaga, Jure Leskovec, Percy Liang.
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: myasu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2109.06822&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://snap.stanford.edu/bifi-lmcritic/&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://github.com/michiyasunaga/LM-Critic&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: language model, grammatical error correction, unsupervised translation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;sensitivity-as-a-complexity-measure-for-sequence-classification-tasks&quot;&gt;&lt;a href=&quot;https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00403/106992/Sensitivity-as-a-Complexity-Measure-for-Sequence&quot;&gt;Sensitivity as a complexity measure for sequence classification tasks&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/img0.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Michael Hahn, Dan Jurafsky, Richard Futrell
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: mhahn2@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00403/106992/Sensitivity-as-a-Complexity-Measure-for-Sequence&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: decision boundaries, computational complexity&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;distributionally-robust-multilingual-machine-translation&quot;&gt;Distributionally Robust Multilingual Machine Translation&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/img4.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Chunting Zhou*, Daniel Levy*, Marjan Ghazvininejad, Xian Li, Graham Neubig
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: daniel.levy0@gmail.com
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: machine translation, robustness, distribution shift, dro, cross-lingual transfer&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-from-limited-labels-for-long-legal-dialogue&quot;&gt;Learning from Limited Labels for Long Legal Dialogue&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/img8.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jenny Hong, Derek Chong, Christopher D. Manning
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jennyhong@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: legal nlp, information extraction, weak supervision&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;capturing-logical-structure-of-visually-structured-documents-with-multimodal-transition-parser&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.00150&quot;&gt;Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-emnlp-2021/img3.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yuta Koreeda, Christopher D. Manning
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: koreeda@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.00150&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2105.00150&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: legal, preprocessing
&lt;br /&gt;&lt;strong&gt;Workshop&lt;/strong&gt;: Natural Legal Language Processing Workshop&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at EMNLP/CoNLL 2021!&lt;/p&gt;
</description>
              <pubDate>Fri, 05 Nov 2021 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers at CoRL 2021</title>
              <link>/blog/corl-2021/</link>
              <guid isPermaLink="true">/blog/corl-2021/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.robot-learning.org/&quot;&gt;The Conference on Robot Learning (CoRL 2021)
&lt;/a&gt; will take place next week. We’re excited to share all the work from SAIL that will be presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;h2 id=&quot;list-of-accepted-papers&quot;&gt;List of Accepted Papers&lt;/h2&gt;
&lt;h4 id=&quot;lila-language-informed-latent-actions&quot;&gt;LILA: Language-Informed Latent Actions&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img18.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Siddharth Karamcheti*, Megha Srivastava*, Percy Liang, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: skaramcheti@cs.stanford.edu, megha@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: natural language, shared autonomy, human-robot interaction&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;behavior-benchmark-for-everyday-household-activities-in-virtual-interactive-and-ecological-environments-&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.03332&quot;&gt;BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments &lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img9.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Sanjana Srivastava*, Chengshu Li*, Michael Lingelbach*, Roberto Martín-Martín*, Fei Xia, Kent Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, C. Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, Li Fei-Fei
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: sanjana2@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2108.03332&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://behavior.stanford.edu/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: embodied ai, benchmarking, household activities&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;co-gail-learning-diverse-strategies-for-human-robot-collaboration&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.06038&quot;&gt;Co-GAIL: Learning Diverse Strategies for Human-Robot Collaboration&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img16.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Chen Wang, Claudia Pérez-D’Arpino, Danfei Xu, Li Fei-Fei, C. Karen Liu, Silvio Savarese
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: chenwj@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2108.06038&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/co-gail-web/home&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: learning for human-robot collaboration, imitation learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;diffimpact-differentiable-rendering-and-identification-of-impact-sounds&quot;&gt;&lt;a href=&quot;https://openreview.net/forum?id=wVIqlSqKu2D&quot;&gt;DiffImpact: Differentiable Rendering and Identification of Impact Sounds&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img14.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Samuel Clarke, Negin Heravi, Mark Rau, Ruohan Gao, Jiajun Wu, Doug James, Jeannette Bohg
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: spclarke@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/forum?id=wVIqlSqKu2D&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/diffimpact/home&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: differentiable sound rendering, auditory scene analysis&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;example-driven-model-based-reinforcement-learning-for-solving-long-horizon-visuomotor-tasks&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.10312&quot;&gt;Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img5.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Bohan Wu, Suraj Nair, Li Fei-Fei*, Chelsea Finn*
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: bohanwu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2109.10312&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: model-based reinforcement learning, long-horizon tasks&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;grac-self-guided-and-self-regularized-actor-critic&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.08973.pdf&quot;&gt;GRAC: Self-Guided and Self-Regularized Actor-Critic&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img12.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Lin Shao, Yifan You, Mengyuan Yan, Shenli Yuan, Qingyun Sun, Jeannette Bohg
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: harry473417@ucla.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2009.08973.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/gracdrl&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: deep reinforcement learning, q-learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;influencing-towards-stable-multi-agent-interactions&quot;&gt;&lt;a href=&quot;https://openreview.net/forum?id=n6xYib0irVR&quot;&gt;Influencing Towards Stable Multi-Agent Interactions&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img2.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Woodrow Z. Wang, Andy Shih, Annie Xie, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: woodywang153@gmail.com
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/forum?id=n6xYib0irVR&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/stable-marl/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: multi-agent interactions, human-robot interaction, non-stationarity&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-language-conditioned-robot-behavior-from-offline-data-and-crowd-sourced-annotation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.01115&quot;&gt;Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img0.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Suraj Nair, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, Chelsea Finn
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: surajn@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2109.01115&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/robotlorl&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: natural language, offline rl, visuomotor manipulation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-multimodal-rewards-from-rankings&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.12750&quot;&gt;Learning Multimodal Rewards from Rankings&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img6.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Vivek Myers, Erdem Bıyık, Nima Anari, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ebiyik@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2109.12750&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/Jj9Qz2-bl7w&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/multimodal-reward-learning/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: reward learning, active learning, learning from rankings, multimodality&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-reward-functions-from-scale-feedback&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.00284&quot;&gt;Learning Reward Functions from Scale Feedback&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img7.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Nils Wilde*, Erdem Bıyık*, Dorsa Sadigh, Stephen L. Smith
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: ebiyik@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.00284&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://youtu.be/5dJ7bGCnjOM&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/reward-learning-scale-feedback&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: preference-based learning, reward learning, active learning, scale feedback&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-to-regrasp-by-learning-to-place&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2109.08817.pdf&quot;&gt;Learning to Regrasp by Learning to Place&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img11.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Shuo Cheng, Kaichun Mo, Lin Shao
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: lins2@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2109.08817.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://sites.google.com/view/regrasp&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: regrasping, object placement, robotic manipulation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-to-be-multimodal--co-evolving-sensory-modalities-and-sensor-properties&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=jaoAqmrabvO&quot;&gt;Learning to be Multimodal : Co-evolving Sensory Modalities and Sensor Properties&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img15.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Rika Antonova, Jeannette Bohg
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: rika.antonova@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/pdf?id=jaoAqmrabvO&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: co-design, multimodal sensing, corl blue sky track&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;o2o-afford-annotation-free-large-scale-object-object-affordance-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.15087&quot;&gt;O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img3.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, Leonidas J. Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kaichun@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.15087&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=cbDSalrMhlo&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://cs.stanford.edu/~kaichun/o2oafford/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: robotic vision, object-object interaction, visual affordance&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;objectfolder-a-dataset-of-objects-with-implicit-visual-auditory-and-tactile-representations&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2109.07991.pdf&quot;&gt;ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img10.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ruohan Gao, Yen-Yu Chang, Shivani Mall, Li Fei-Fei, Jiajun Wu
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: rhgao@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2109.07991.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=wQ4o8XeS-X0&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://ai.stanford.edu/~rhgao/objectfolder/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: object dataset, multisensory learning, implicit representations&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;taskography-evaluating-robot-task-planning-over-large-3d-scene-graphs&quot;&gt;&lt;a href=&quot;https://openreview.net/forum?id=nWLt35BU1z_&quot;&gt;Taskography: Evaluating robot task planning over large 3D scene graphs&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img17.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Christopher Agia, Krishna Murthy Jatavallabhula, Mohamed Khodeir, Ondrej Miksik, Vibhav Vineet, Mustafa Mukadam, Liam Paull, Florian Shkurti
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: cagia@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openreview.net/forum?id=nWLt35BU1z_&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;http://taskography.github.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: robot task planning, 3d scene graphs, learning to plan, benchmarks&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;what-matters-in-learning-from-offline-human-demonstrations-for-robot-manipulation&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.03298&quot;&gt;What Matters in Learning from Offline Human Demonstrations for Robot Manipulation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img1.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Martín-Martín
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: amandlek@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2108.03298&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://ai.stanford.edu/blog/robomimic/&quot;&gt;Blog Post&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=qg_IVo4rB8k&amp;amp;ab_channel=AnonymousAnonymous&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://arise-initiative.github.io/robomimic-web/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: imitation learning, offline reinforcement learning, robot manipulation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;xirl-cross-embodiment-inverse-reinforcement-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.03911&quot;&gt;XIRL: Cross-embodiment Inverse Reinforcement Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img13.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, Debidatta Dwibedi
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: zakka@berkeley.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.03911&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://x-irl.github.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: inverse reinforcement learning, imitation learning, self-supervised learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;igibson-20-object-centric-simulation-for-robot-learning-of-everyday-household-tasks&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2108.03272.pdf&quot;&gt;iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img8.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Chengshu Li*, Fei Xia*, Roberto Martín-Martín*, Michael Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, Andrey Kurenkov, C. Karen Liu, Hyowon Gweon, Jiajun Wu, Li Fei-Fei, Silvio Savarese
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: chengshu@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2108.03272.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;http://svl.stanford.edu/igibson/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: simulation environment, embodied ai, virtual reality interface&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-feasibility-to-imitate-demonstrators-with-different-dynamics&quot;&gt;Learning Feasibility to Imitate Demonstrators with Different Dynamics&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-11-05-corl-2021/img4.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Zhangjie Cao, Yilun Hao, Mengxi Li, Dorsa Sadigh
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: caozj@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: imitation learning, learning from agents with different dynamics&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at CoRL 2021!&lt;/p&gt;
</description>
              <pubDate>Fri, 05 Nov 2021 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Selective Classification Can Magnify Disparities Across Groups</title>
              <link>/blog/sc-magnifies-disparities/</link>
              <guid isPermaLink="true">/blog/sc-magnifies-disparities/</guid>
              <description>&lt;p&gt;Selective classification, where models are allowed to “abstain” when they are uncertain about a prediction, is a useful approach for deploying models in settings where errors are costly. For example, in medicine, model errors can have life-or-death ramifications, but abstentions can be easily handled by backing off to a doctor, who then makes a diagnosis. Across a range of applications from vision &lt;sup id=&quot;fnref:liu2015&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:liu2015&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:sagawagroupdro&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sagawagroupdro&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:irvinchexpert&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:irvinchexpert&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; and NLP &lt;sup id=&quot;fnref:borkan2019&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:borkan2019&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:williams2018&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:williams2018&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, even simple selective classifiers, relying only on model logits, routinely and often dramatically improve accuracy by abstaining. This makes selective classification a compelling tool for ML practitioners &lt;sup id=&quot;fnref:selectivenet&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:selectivenet&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:mozannar2020&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:mozannar2020&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;However, in our recent ICLR paper, we find that despite reliably improving average accuracy, &lt;strong&gt;selective classification can fail to improve and even hurt the accuracy over certain subpopulations of the data&lt;/strong&gt;. As a motivating example, consider the task of diagnosing pleural effusion, or fluid in the lungs, from chest X-rays. Pleural effusion is often treated with a chest drain, so many pleural effusion cases also have chest drains, while most cases without pleural effusion do not have chest drains &lt;sup id=&quot;fnref:oakdenrayner2020&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:oakdenrayner2020&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;. While selective classification improves average accuracy for this task, we find that it does not appreciably improve accuracy on the most clinically relevant subgroup, or subpopulation, of the data: those that have pleural effusion but don’t yet have a chest drain, i.e. those that have pleural effusion but have not yet been treated for it. Practitioners, thus, should be wary of these potential failure modes of using selective classification in the wild.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_95&quot; src=&quot;/blog/assets/img/posts/2021-10-13-sc-magnifies-disparities/image2.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Example of the spurious correlation setup. This patient has a pleural effusion (excess fluid in the lung), but does not yet have a chest drain. The model, relying on the presence of a chest drain to make a prediction, incorrectly predicts negative.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;To further outline this critical failure mode of selective classification, we’ll first provide an overview of selective classification. We then demonstrate empirically that selective classification can hurt or fail to significantly improve accuracy on certain subgroups of the data. We next outline our theoretical results, which suggest that selective classification is rarely a good tool to resolve differences in accuracy between subgroups. And finally, suggest methods for building more equitable selective classifiers.&lt;/p&gt;

&lt;h3 id=&quot;selective-classification-basics&quot;&gt;Selective classification basics&lt;/h3&gt;

&lt;p&gt;Imagine you are trying to build a model that classifies X-rays as either pleural effusion positive or negative. With standard classification, the model is required to either output positive or negative on each input. In contrast, a selective classifier can additionally abstain from making a prediction when it is not sufficiently confident in any class &lt;sup id=&quot;fnref:chow1957&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:chow1957&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:gal2016&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:gal2016&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:geifman2017&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:geifman2017&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;. By abstaining, selective classifiers aim to avoid making predictions on examples they are likely to classify incorrectly, say a corrupted or difficult-to-classify X-ray, which increases their average accuracy.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_95&quot; src=&quot;/blog/assets/img/posts/2021-10-13-sc-magnifies-disparities/image5.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
Selective classification pipeline. The model makes the incorrect prediction of negative. However, the outputted confidence of 0.7 is less than the confidence threshold of 0.8, so the selective classifier abstains. Selective classifiers increase accuracy by abstaining on examples they would get wrong.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;One key question in selective classification is how to choose which examples to abstain on. Selective classifiers can be viewed as two models: one that outputs a prediction (say, negative), and another that outputs a &lt;em&gt;confidence&lt;/em&gt; in that prediction (say, 0.7 out of 1.) Whenever the confidence is above a certain (confidence) threshold, the selective classifier outputs the original prediction; for example, if the threshold were 0.6, the selective classifier would predict negative. Otherwise, the selective classifier abstains. In our work, we primarily use &lt;em&gt;softmax response&lt;/em&gt; &lt;sup id=&quot;fnref:geifman2017:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:geifman2017&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; to extract confidences: the confidence in a prediction is simply the maximum softmax probability over the possible classes.&lt;/p&gt;

&lt;p&gt;Selective classifiers are typically measured in terms of the &lt;em&gt;accuracy&lt;/em&gt; (also called &lt;em&gt;selective accuracy&lt;/em&gt;) on predicted examples, and the &lt;em&gt;coverage&lt;/em&gt;, or fraction of examples the selective classifier makes predictions on &lt;sup id=&quot;fnref:elyaniv2010&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:elyaniv2010&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;. We can tweak both coverage and accuracy by adjusting the &lt;em&gt;confidence threshold&lt;/em&gt;: a lower threshold for making predictions increases the coverage, since the model’s confidence for more examples is sufficiently high. However, this tends to lower average accuracy, as the model is less confident on average in its predictions. In contrast, higher thresholds increase confidence required to make a prediction, reducing the coverage but generally increasing average accuracy.&lt;/p&gt;

&lt;p&gt;Typically, researchers measure the performance of selective classifiers by plotting accuracy as a function of coverage. In particular, for each possible coverage (ranging from 0: abstain on everything to 1: predict on everything) they compute the maximum threshold that achieves that coverage, and then plot the accuracy at that threshold. One particularly useful reference point is the &lt;em&gt;full-coverage accuracy&lt;/em&gt;: the accuracy of the selective classifier at coverage 1, which is the accuracy of the regular classifier.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_95&quot; src=&quot;/blog/assets/img/posts/2021-10-13-sc-magnifies-disparities/image3.png&quot; /&gt;&lt;/p&gt;
&lt;figcaption&gt;
For five datasets, we plot the average accuracy as a function of the coverage. Reading from high coverages to low coverages (right to left), as the confidence threshold increases, accuracy reliably increases. This is expected, since the model is more confident on average in its predictions at lower coverage, so more of them tend to be correct.
&lt;/figcaption&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;h3 id=&quot;selective-classification-can-magnify-accuracy-disparities-between-subgroups&quot;&gt;Selective classification can magnify accuracy disparities between subgroups&lt;/h3&gt;

&lt;p&gt;While prior work mostly focuses on average accuracy for selective classifiers, we instead focus on the accuracy of different &lt;em&gt;subgroups&lt;/em&gt; of the data. In particular, we focus on datasets where models often latch onto &lt;em&gt;spurious correlations&lt;/em&gt;. For example, in the above pleural effusion task, the model might learn to predict whether or not there is a chest drain, instead of directly diagnosing pleural effusion, because chest drains are highly correlated with pleural effusion; this correlation is spurious because not all pleural effusions have a chest drain. We consider subgroups that highlight this spurious correlation: two groups for when the spurious correlation gives the correct result (positive pleural effusion with chest drain, negative pleural effusion without a chest drain), and two groups when it does not (positive pleural effusion with no chest drain, negative pleural effusion with a chest drain). As a result, a model that learns this spurious correlation obtains high accuracy for the first two subgroups, but low accuracy for the latter two.&lt;/p&gt;

&lt;p&gt;In principle, selective classification seems like a reasonable approach towards resolving these accuracy discrepancies between different subgroups of the data. Since we empirically see that selective classification reliably improves average accuracy, it must be more likely to cause a model to abstain when an example would be classified incorrectly. Incorrect examples disproportionately come from the lowest-accuracy subgroups of the data, suggesting that without bias in the confidence function, worst-group accuracy should increase faster than average accuracy.&lt;/p&gt;

&lt;p&gt;To test this, we plot the accuracy-coverage curves over a range of tasks, including hair color classification (CelebA), bird type classification (Waterbirds), pleural effusion classification (CheXpert-device), toxicity classification (CivilComments) and natural language inference (MultiNLI). CelebA, Waterbirds, and MultiNLI use the same spurious correlation setup presented in &lt;sup id=&quot;fnref:sagawagroupdro:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sagawagroupdro&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. CivilComments exhibits the same spurious correlations as described in the WILDS benchmark &lt;sup id=&quot;fnref:kohwilds&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:kohwilds&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;. Finally, we created the CheXpert-device dataset by subsampling the original CheXpert dataset &lt;sup id=&quot;fnref:irvinchexpert:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:irvinchexpert&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; such that the presence of a chest drain even more strongly correlates with pleural effusion.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_95&quot; src=&quot;/blog/assets/img/posts/2021-10-13-sc-magnifies-disparities/image4.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Reading from right to left, while we see that as the coverage decreases the average accuracy reliably increases, &lt;strong&gt;the worst-group accuracies do not always increase, and exhibit a range of undesirable behaviors&lt;/strong&gt;. On CelebA, worst-group accuracy actually decreases: this means the more confident predictions are more likely to be incorrect. For Waterbirds, CheXpert-device, and CivilComments, worst-group accuracy sometimes increases, but never by more than 10 points until the noisy low-coverage regime, and sometimes decreases. For MultiNLI, worst-group accuracy does slowly improve, but can’t even reach 80% until very low coverages.&lt;/p&gt;

&lt;p&gt;These results highlight that practitioners should be wary: even if selective classification reliably increases average accuracy, it will not necessarily improve the accuracy of different subgroups.&lt;/p&gt;

&lt;h3 id=&quot;selective-classification-rarely-overcomes-accuracy-disparities&quot;&gt;Selective classification rarely overcomes accuracy disparities&lt;/h3&gt;

&lt;p&gt;To better understand why selective classification can sometimes hurt worst-group accuracy and does not reduce full-coverage accuracy disparities, we theoretically characterize for a broad class of distributions: (1) when does selective classification improve accuracy as the confidence threshold decreases and (2) when does selective classification disproportionately help the worst group.&lt;/p&gt;

&lt;p&gt;At a high level, our analysis focuses on the &lt;em&gt;margin&lt;/em&gt;, or the model’s confidence for a given prediction multiplied by -1 if that prediction was incorrect. Intuitively, the more negative the margin, the “worse” the prediction. Using only the margin distribution, we can recreate the accuracy-coverage curve by abstaining on density between the negative and positive threshold, and computing the fraction of remaining density that is correct.&lt;/p&gt;

&lt;figure class=&quot;figure&quot;&gt;&lt;div class=&quot;figure__main&quot;&gt;
&lt;p&gt;&lt;img class=&quot;postimage_90&quot; src=&quot;/blog/assets/img/posts/2021-10-13-sc-magnifies-disparities/image1.png&quot; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The key result of our theoretical analysis is that &lt;strong&gt;the &lt;em&gt;full-coverage accuracy&lt;/em&gt; of a subgroup dramatically impacts how well selective classification performs&lt;/strong&gt; on that subgroup, which amplifies disparities. For a wide range of margin distributions, full-coverage accuracy and a property of the margin distribution we call &lt;em&gt;left-log-concavity&lt;/em&gt; completely determine whether or not the accuracy of a selective classifier monotonically increases or decreases. When a margin distribution is left-log-concave, which many standard distributions (e.g. gaussians) are, accuracy monotonically increases when full-coverage accuracy is at least 50% and decreases otherwise.&lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next steps&lt;/h3&gt;

&lt;p&gt;So far, we have painted a fairly bleak picture of selective classification: even though it reliably improves average accuracy, it can, both theoretically and empirically, exacerbate accuracy disparities between subgroups. There are still, however, mechanisms to improve selective classification, which we outline below.&lt;/p&gt;

&lt;p&gt;One natural step towards improving selective classification is to develop confidence functions that allow selective classifiers to overcome accuracy disparities between groups. In our paper, we test the two most widely used methods: softmax response and Monte Carlo dropout &lt;sup id=&quot;fnref:gal2016:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:gal2016&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;. We consistently find that both are disproportionately overconfident on incorrect examples from the worst-groups. However, new confidence functions that are better calibrated across groups would likely resolve disparities &lt;sup id=&quot;fnref:wald2021&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:wald2021&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;, and is an important direction for future work.&lt;/p&gt;

&lt;p&gt;In the short term, however, we find that the most promising method to improve worst-group accuracy with selective classification is to &lt;strong&gt;build selective classifiers on top of already-equitable models&lt;/strong&gt;, or models that achieve similar full-coverage accuracies across the relevant subgroups. One method to train such models is group DRO, which minimizes the maximum loss over subgroups &lt;sup id=&quot;fnref:sagawagroupdro:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sagawagroupdro&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. We find empirically that selective classifiers trained with group DRO improve the accuracy of subgroups at roughly the same rate when they have the same accuracy at full coverage. However, group DRO is far from a perfect fix – it requires a priori knowledge of the relevant subgroups, and subgroup labels for each training example which may be costly to obtain. Nevertheless, it is a promising start, and developing more broadly applicable methods for training already-equitable models is a critical area for future work.&lt;/p&gt;

&lt;p&gt;To conclude, despite the intuition that selective classification should improve worst-group accuracy, and selective classification’s ability to consistently improve average accuracy, common selective classifiers can severely exacerbate accuracy discrepancies between subgroups. We hope our work encourages practitioners to apply selective classification with caution, and in general focus on how different methods affect different subgroups of the data.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;Thanks to the SAIL blog editors, Pang Wei Koh, and Shiori Sagawa for their helpful feedback on this blog post. This post is based off our &lt;a href=&quot;https://arxiv.org/abs/2010.14134&quot;&gt;ICLR 2021 paper&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Selective Classification Can Magnify Disparities Across Groups. Erik Jones*, Shiori Sagawa* Pang Wei Koh*, Ananya Kumar, and Percy Liang. ICLR 2021.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:liu2015&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3730–3738, 2015. &lt;a href=&quot;#fnref:liu2015&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sagawagroupdro&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In International Conference on Learning Representations (ICLR), 2020. &lt;a href=&quot;#fnref:sagawagroupdro&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:sagawagroupdro:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:sagawagroupdro:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:irvinchexpert&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Association for the Advancement of Artificial Intelligence (AAAI), volume 33, pp. 590–597, 2019. &lt;a href=&quot;#fnref:irvinchexpert&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:irvinchexpert:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:borkan2019&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In World Wide Web (WWW), pp. 491–500, 2019. &lt;a href=&quot;#fnref:borkan2019&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:williams2018&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Association for Computational Linguistics (ACL), pp. 1112–1122, 2018. &lt;a href=&quot;#fnref:williams2018&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:selectivenet&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yonatan Giefman and Ran El-Yaniv. SelectiveNet: A deep neural network with an integrated reject option. In International Conference on Machine Learning (ICML), 2019. &lt;a href=&quot;#fnref:selectivenet&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:mozannar2020&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In International Conference on Machine Learning (ICML), 2020. &lt;a href=&quot;#fnref:mozannar2020&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:oakdenrayner2020&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. In Proceedings of the ACM Conference on Health, Inference, and Learning, pp. 151–159, 2020. &lt;a href=&quot;#fnref:oakdenrayner2020&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:chow1957&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;C. K. Chow. An optimum character recognition system using decision functions. In IRE Transactions on Electronic Computers, 1957. &lt;a href=&quot;#fnref:chow1957&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gal2016&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning (ICML), 2016. &lt;a href=&quot;#fnref:gal2016&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:gal2016:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:geifman2017&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2017. &lt;a href=&quot;#fnref:geifman2017&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:geifman2017:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:elyaniv2010&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of Machine Learning Research (JMLR), 11, 2010. &lt;a href=&quot;#fnref:elyaniv2010&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:kohwilds&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. arXiv, 2020. &lt;a href=&quot;#fnref:kohwilds&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:wald2021&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On Calibration and Out-of-domain Generalization. arXiv preprint arXiv:2102.10395, 2021. &lt;a href=&quot;#fnref:wald2021&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
              <pubDate>Wed, 13 Oct 2021 00:00:00 -0700</pubDate>
          </item>
          
        
          
          <item>
              <title>Stanford AI Lab Papers at ICCV 2021</title>
              <link>/blog/iccv-2021/</link>
              <guid isPermaLink="true">/blog/iccv-2021/</guid>
              <description>&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/logo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://iccv2021.thecvf.com/&quot;&gt;International Conference on Computer Vision (ICCV 2021)
&lt;/a&gt; will be hosted virtually next week. We’re excited to share all the work from SAIL that will be presented, and you’ll find links to papers, videos and blogs below. Feel free to reach out to the contact authors directly to learn more about the work that’s happening at Stanford!&lt;/p&gt;

&lt;h2 id=&quot;list-of-accepted-papers&quot;&gt;List of Accepted Papers&lt;/h2&gt;

&lt;h4 id=&quot;gloria-a-multimodal-global-local-representation-learning-framework-for-label-efficient-medical-image-recognition&quot;&gt;GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img5.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Mars Huang
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: mschuang@stanford.edu
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: medical image, self-supervised learning, multimodal fusion&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;3d-shape-generation-and-completion-through-point-voxel-diffusion&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.03670.pdf&quot;&gt;3D Shape Generation and Completion Through Point-Voxel Diffusion&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img9.gif&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Linqi Zhou, Yilun Du, Jiajun Wu
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: linqizhou@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2104.03670.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=64jl79i6HNY&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://alexzhou907.github.io/pvd&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: diffusion, shape generation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;captra-category-level-pose-tracking-for-rigid-and-articulated-objects-from-point-clouds&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.03437&quot;&gt;CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img2.jpeg&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yijia Weng*, He Wang*, Qiang Zhou, Yuzhe Qin, Yueqi Duan, Qingnan Fan, Baoquan Chen, Hao Su, Leonidas J. Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: yijiaw@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral Presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2104.03437&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=EkcCEj7gZGg&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://yijiaweng.github.io/CAPTRA/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: category-level object pose tracking, articulated objects&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;detecting-human-object-relationships-in-videos&quot;&gt;&lt;a href=&quot;https://ai.stanford.edu/~jingweij/papers/2021-ICCV-Detecting.pdf&quot;&gt;Detecting Human-Object Relationships in Videos&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img1.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jingwei Ji, Rishi Desai, Juan Carlos Niebles
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jingweij@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://ai.stanford.edu/~jingweij/papers/2021-ICCV-Detecting.pdf&quot;&gt;Paper&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: human-object relationships, video, detection, transformer, spatio-temporal reasoning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;geography-aware-self-supervised-learning&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.09980.pdf&quot;&gt;Geography-Aware Self-Supervised Learning&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img11.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, Stefano Ermon
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kayush@cs.stanford.edu, chenlin@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2011.09980.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://geography-aware-ssl.github.io/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: self-supervised learning, contrastive learning, remote sensing, spatio-temporal, classification, object detection, segmentation&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;humor-3d-human-motion-model-for-robust-pose-estimation&quot;&gt;&lt;a href=&quot;https://geometry.stanford.edu/projects/humor/docs/humor.pdf&quot;&gt;HuMoR: 3D Human Motion Model for Robust Pose Estimation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img4.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, Leonidas Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: drempe@stanford.edu
&lt;br /&gt;&lt;strong&gt;Award nominations:&lt;/strong&gt; Oral Presentation
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://geometry.stanford.edu/projects/humor/docs/humor.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://geometry.stanford.edu/projects/humor/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: 3d human pose estimation; 3d human motion; generative modeling&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-privacy-preserving-optics-for-human-pose-estimation&quot;&gt;&lt;a href=&quot;https://carloshinojosa.me/files/ICCV2021/05401.pdf&quot;&gt;Learning Privacy-preserving Optics for Human Pose Estimation&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img3.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Carlos Hinojosa, Juan Carlos Niebles, Henry Arguello
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: carlos.hinojosa@saber.uis.edu.co
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://carloshinojosa.me/files/ICCV2021/05401.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://carloshinojosa.me/project/privacy-hpe/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: computational photography;  fairness, accountability, transparency, and ethics in vision; gestures and body pose&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;learning-temporal-dynamics-from-cycles-in-narrated-video&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2101.02337.pdf&quot;&gt;Learning Temporal Dynamics from Cycles in Narrated Video&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img8.jpg&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Dave Epstein, Jiajun Wu, Cordelia Schmid, Chen Sun
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jiajunwu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2101.02337.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://dave.ml/mmcc/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: multi-modal learning, cycle consistency, video&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;vector-neurons-a-general-framework-for-so3-equivariant-networks&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.12229.pdf&quot;&gt;Vector Neurons: A General Framework for SO(3)-Equivariant Networks&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img6.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, Leonidas Guibas
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: congyue@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2104.12229.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=aJy4eMvdTpA&amp;amp;t=4s&quot;&gt;Video&lt;/a&gt; | &lt;a href=&quot;https://cs.stanford.edu/~congyue/vnn/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: pointcloud network, rotation equivariance, rotation invariance&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;neural-radiance-for-4d-view-synthesis-and-video-processing&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.09790.pdf&quot;&gt;Neural Radiance for 4D View Synthesis and Video Processing&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img10.gif&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, Jiajun Wu
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: jiajunwu@cs.stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2012.09790.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://yilundu.github.io/nerflow/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: 4d representation, neural rendering, video processing&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;where2act-from-pixels-to-actions-for-articulated-3d-objects&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2101.02692&quot;&gt;Where2Act: From Pixels to Actions for Articulated 3D Objects&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img0.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Kaichun Mo, Leonidas J. Guibas, Mustafa Mukadam, Abhinav Gupta, Shubham Tulsiani
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: kaichunm@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2101.02692&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://cs.stanford.edu/~kaichun/where2act/&quot;&gt;Website&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: 3d computer vision, robotic vision, affordance learning, robot learning&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;low-shot-validation-active-importance-sampling-for-estimating-classifier-performance-on-rare-categories&quot;&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Poms_Low-Shot_Validation_Active_Importance_Sampling_for_Estimating_Classifier_Performance_on_ICCV_2021_paper.pdf&quot;&gt;Low-Shot Validation: Active Importance Sampling for Estimating Classifier Performance on Rare Categories&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img class=&quot;postimage_75&quot; src=&quot;/blog/assets/img/posts/2021-10-08-iccv-2021/img12.png&quot; /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Fait Poms*, Vishnu Sarukkai*, Ravi Teja Mullapudi, Nimit S. Sohoni, William R. Mark, Deva Ramanan, Kayvon Fatahalian
&lt;br /&gt;&lt;strong&gt;Contact&lt;/strong&gt;: sarukkai@stanford.edu
&lt;br /&gt;&lt;strong&gt;Links:&lt;/strong&gt; &lt;a href=&quot;https://openaccess.thecvf.com/content/ICCV2021/papers/Poms_Low-Shot_Validation_Active_Importance_Sampling_for_Estimating_Classifier_Performance_on_ICCV_2021_paper.pdf&quot;&gt;Paper&lt;/a&gt; | &lt;a href=&quot;https://vsanimator.github.io/acis/&quot;&gt;Blog&lt;/a&gt; | &lt;a href=&quot;https://www.youtube.com/watch?v=x_dly8dKC6I&amp;amp;feature=youtu.be&quot;&gt;Video&lt;/a&gt;
&lt;br /&gt;&lt;strong&gt;Keywords&lt;/strong&gt;: model evaluation, active learning&lt;/p&gt;
&lt;hr /&gt;

&lt;hr /&gt;

&lt;p&gt;We look forward to seeing you at ICCV 2021!&lt;/p&gt;

</description>
              <pubDate>Fri, 08 Oct 2021 00:00:00 -0700</pubDate>
          </item>
          
        
    </channel>
</rss>
